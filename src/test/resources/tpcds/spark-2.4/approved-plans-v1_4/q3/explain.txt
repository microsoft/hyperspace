== Physical Plan ==
TakeOrderedAndProject(limit=100, orderBy=[d_year#1 ASC NULLS FIRST,sum_agg#2 DESC NULLS LAST,brand_id#3 ASC NULLS FIRST], output=[d_year#1,brand_id#3,brand#4,sum_agg#2])
+- *(4) HashAggregate(keys=[d_year#1, i_brand#5, i_brand_id#6], functions=[sum(UnscaledValue(ss_ext_sales_price#7))])
   +- Exchange hashpartitioning(d_year#1, i_brand#5, i_brand_id#6, 5)
      +- *(3) HashAggregate(keys=[d_year#1, i_brand#5, i_brand_id#6], functions=[partial_sum(UnscaledValue(ss_ext_sales_price#7))])
         +- *(3) Project [d_year#1, ss_ext_sales_price#7, i_brand_id#6, i_brand#5]
            +- *(3) BroadcastHashJoin [ss_item_sk#8], [i_item_sk#9], Inner, BuildRight
               :- *(3) Project [d_year#1, ss_item_sk#8, ss_ext_sales_price#7]
               :  +- *(3) BroadcastHashJoin [d_date_sk#10], [ss_sold_date_sk#11], Inner, BuildRight
               :     :- *(3) Project [d_date_sk#10, d_year#1]
               :     :  +- *(3) Filter ((isnotnull(d_moy#12) && (d_moy#12 = 11)) && isnotnull(d_date_sk#10))
               :     :     +- *(3) FileScan parquet default.date_dim[d_date_sk#10,d_year#1,d_moy#12] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_moy), EqualTo(d_moy,11), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_year:int,d_moy:int>
               :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :        +- *(1) Project [ss_sold_date_sk#11, ss_item_sk#8, ss_ext_sales_price#7]
               :           +- *(1) Filter (isnotnull(ss_sold_date_sk#11) && isnotnull(ss_item_sk#8))
               :              +- *(1) FileScan parquet default.store_sales[ss_sold_date_sk#11,ss_item_sk#8,ss_ext_sales_price#7] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_sold_date_sk), IsNotNull(ss_item_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_item_sk:int,ss_ext_sales_price:decimal(7,2)>
               +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                  +- *(2) Project [i_item_sk#9, i_brand_id#6, i_brand#5]
                     +- *(2) Filter ((isnotnull(i_manufact_id#13) && (i_manufact_id#13 = 128)) && isnotnull(i_item_sk#9))
                        +- *(2) FileScan parquet default.item[i_item_sk#9,i_brand_id#6,i_brand#5,i_manufact_id#13] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_manufact_id), EqualTo(i_manufact_id,128), IsNotNull(i_item_sk)], ReadSchema: struct<i_item_sk:int,i_brand_id:int,i_brand:string,i_manufact_id:int>