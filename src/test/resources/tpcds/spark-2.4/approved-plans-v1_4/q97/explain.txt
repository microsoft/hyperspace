== Physical Plan ==
CollectLimit 100
+- *(10) HashAggregate(keys=[], functions=[sum(cast(CASE WHEN (isnotnull(customer_sk#1) && isnull(customer_sk#2)) THEN 1 ELSE 0 END as bigint)), sum(cast(CASE WHEN (isnull(customer_sk#1) && isnotnull(customer_sk#2)) THEN 1 ELSE 0 END as bigint)), sum(cast(CASE WHEN (isnotnull(customer_sk#1) && isnotnull(customer_sk#2)) THEN 1 ELSE 0 END as bigint))])
   +- Exchange SinglePartition
      +- *(9) HashAggregate(keys=[], functions=[partial_sum(cast(CASE WHEN (isnotnull(customer_sk#1) && isnull(customer_sk#2)) THEN 1 ELSE 0 END as bigint)), partial_sum(cast(CASE WHEN (isnull(customer_sk#1) && isnotnull(customer_sk#2)) THEN 1 ELSE 0 END as bigint)), partial_sum(cast(CASE WHEN (isnotnull(customer_sk#1) && isnotnull(customer_sk#2)) THEN 1 ELSE 0 END as bigint))])
         +- *(9) Project [customer_sk#1, customer_sk#2]
            +- SortMergeJoin [customer_sk#1, item_sk#3], [customer_sk#2, item_sk#4], FullOuter
               :- *(4) Sort [customer_sk#1 ASC NULLS FIRST, item_sk#3 ASC NULLS FIRST], false, 0
               :  +- Exchange hashpartitioning(customer_sk#1, item_sk#3, 5)
               :     +- *(3) HashAggregate(keys=[ss_customer_sk#5, ss_item_sk#6], functions=[])
               :        +- Exchange hashpartitioning(ss_customer_sk#5, ss_item_sk#6, 5)
               :           +- *(2) HashAggregate(keys=[ss_customer_sk#5, ss_item_sk#6], functions=[])
               :              +- *(2) Project [ss_item_sk#6, ss_customer_sk#5]
               :                 +- *(2) BroadcastHashJoin [ss_sold_date_sk#7], [d_date_sk#8], Inner, BuildRight
               :                    :- *(2) Project [ss_sold_date_sk#7, ss_item_sk#6, ss_customer_sk#5]
               :                    :  +- *(2) Filter isnotnull(ss_sold_date_sk#7)
               :                    :     +- *(2) FileScan parquet default.store_sales[ss_sold_date_sk#7,ss_item_sk#6,ss_customer_sk#5] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_sold_date_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_item_sk:int,ss_customer_sk:int>
               :                    +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :                       +- *(1) Project [d_date_sk#8]
               :                          +- *(1) Filter (((isnotnull(d_month_seq#9) && (d_month_seq#9 >= 1200)) && (d_month_seq#9 <= 1211)) && isnotnull(d_date_sk#8))
               :                             +- *(1) FileScan parquet default.date_dim[d_date_sk#8,d_month_seq#9] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_month_seq), GreaterThanOrEqual(d_month_seq,1200), LessThanOrEqual(d_month_seq,1211),..., ReadSchema: struct<d_date_sk:int,d_month_seq:int>
               +- *(8) Sort [customer_sk#2 ASC NULLS FIRST, item_sk#4 ASC NULLS FIRST], false, 0
                  +- Exchange hashpartitioning(customer_sk#2, item_sk#4, 5)
                     +- *(7) HashAggregate(keys=[cs_bill_customer_sk#10, cs_item_sk#11], functions=[])
                        +- Exchange hashpartitioning(cs_bill_customer_sk#10, cs_item_sk#11, 5)
                           +- *(6) HashAggregate(keys=[cs_bill_customer_sk#10, cs_item_sk#11], functions=[])
                              +- *(6) Project [cs_bill_customer_sk#10, cs_item_sk#11]
                                 +- *(6) BroadcastHashJoin [cs_sold_date_sk#12], [d_date_sk#8], Inner, BuildRight
                                    :- *(6) Project [cs_sold_date_sk#12, cs_bill_customer_sk#10, cs_item_sk#11]
                                    :  +- *(6) Filter isnotnull(cs_sold_date_sk#12)
                                    :     +- *(6) FileScan parquet default.catalog_sales[cs_sold_date_sk#12,cs_bill_customer_sk#10,cs_item_sk#11] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/catalog_sales], PartitionFilters: [], PushedFilters: [IsNotNull(cs_sold_date_sk)], ReadSchema: struct<cs_sold_date_sk:int,cs_bill_customer_sk:int,cs_item_sk:int>
                                    +- ReusedExchange [d_date_sk#8], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))