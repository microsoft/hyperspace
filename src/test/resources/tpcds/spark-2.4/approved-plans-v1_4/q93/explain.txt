== Physical Plan ==
TakeOrderedAndProject(limit=100, orderBy=[sumsales#1 ASC NULLS FIRST,ss_customer_sk#2 ASC NULLS FIRST], output=[ss_customer_sk#2,sumsales#1])
+- *(4) HashAggregate(keys=[ss_customer_sk#2], functions=[sum(act_sales#3)])
   +- Exchange hashpartitioning(ss_customer_sk#2, 5)
      +- *(3) HashAggregate(keys=[ss_customer_sk#2], functions=[partial_sum(act_sales#3)])
         +- *(3) Project [ss_customer_sk#2, CASE WHEN isnotnull(sr_return_quantity#4) THEN CheckOverflow((promote_precision(cast(cast((ss_quantity#5 - sr_return_quantity#4) as decimal(10,0)) as decimal(12,2))) * promote_precision(cast(ss_sales_price#6 as decimal(12,2)))), DecimalType(18,2)) ELSE CheckOverflow((promote_precision(cast(cast(ss_quantity#5 as decimal(10,0)) as decimal(12,2))) * promote_precision(cast(ss_sales_price#6 as decimal(12,2)))), DecimalType(18,2)) END AS act_sales#3]
            +- *(3) BroadcastHashJoin [sr_reason_sk#7], [cast(r_reason_sk#8 as bigint)], Inner, BuildRight
               :- *(3) Project [ss_customer_sk#2, ss_quantity#5, ss_sales_price#6, sr_reason_sk#7, sr_return_quantity#4]
               :  +- *(3) BroadcastHashJoin [cast(ss_item_sk#9 as bigint), cast(ss_ticket_number#10 as bigint)], [sr_item_sk#11, sr_ticket_number#12], Inner, BuildRight
               :     :- *(3) FileScan parquet default.store_sales[ss_item_sk#9,ss_customer_sk#2,ss_ticket_number#10,ss_quantity#5,ss_sales_price#6] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ss_item_sk:int,ss_customer_sk:int,ss_ticket_number:int,ss_quantity:int,ss_sales_price:deci...
               :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true], input[2, bigint, true]))
               :        +- *(1) Project [sr_item_sk#11, sr_reason_sk#7, sr_ticket_number#12, sr_return_quantity#4]
               :           +- *(1) Filter ((isnotnull(sr_item_sk#11) && isnotnull(sr_ticket_number#12)) && isnotnull(sr_reason_sk#7))
               :              +- *(1) FileScan parquet default.store_returns[sr_item_sk#11,sr_reason_sk#7,sr_ticket_number#12,sr_return_quantity#4] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_returns], PartitionFilters: [], PushedFilters: [IsNotNull(sr_item_sk), IsNotNull(sr_ticket_number), IsNotNull(sr_reason_sk)], ReadSchema: struct<sr_item_sk:bigint,sr_reason_sk:bigint,sr_ticket_number:bigint,sr_return_quantity:int>
               +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                  +- *(2) Project [r_reason_sk#8]
                     +- *(2) Filter ((isnotnull(r_reason_desc#13) && (r_reason_desc#13 = reason 28)) && isnotnull(r_reason_sk#8))
                        +- *(2) FileScan parquet default.reason[r_reason_sk#8,r_reason_desc#13] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/reason], PartitionFilters: [], PushedFilters: [IsNotNull(r_reason_desc), EqualTo(r_reason_desc,reason 28), IsNotNull(r_reason_sk)], ReadSchema: struct<r_reason_sk:int,r_reason_desc:string>