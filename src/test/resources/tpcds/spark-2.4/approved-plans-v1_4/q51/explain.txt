== Physical Plan ==
TakeOrderedAndProject(limit=100, orderBy=[item_sk#1 ASC NULLS FIRST,d_date#2 ASC NULLS FIRST], output=[item_sk#1,d_date#2,web_sales#3,store_sales#4,web_cumulative#5,store_cumulative#6])
+- *(15) Filter ((isnotnull(web_cumulative#5) && isnotnull(store_cumulative#6)) && (web_cumulative#5 > store_cumulative#6))
   +- Window [max(web_sales#3) windowspecdefinition(item_sk#1, d_date#2 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS web_cumulative#5, max(store_sales#4) windowspecdefinition(item_sk#1, d_date#2 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS store_cumulative#6], [item_sk#1], [d_date#2 ASC NULLS FIRST]
      +- *(14) Sort [item_sk#1 ASC NULLS FIRST, d_date#2 ASC NULLS FIRST], false, 0
         +- Exchange hashpartitioning(item_sk#1, 5)
            +- *(13) Project [CASE WHEN isnotnull(item_sk#7) THEN item_sk#7 ELSE item_sk#8 END AS item_sk#1, CASE WHEN isnotnull(d_date#9) THEN d_date#9 ELSE d_date#10 END AS d_date#2, cume_sales#11 AS web_sales#3, cume_sales#12 AS store_sales#4]
               +- SortMergeJoin [item_sk#7, d_date#9], [item_sk#8, d_date#10], FullOuter
                  :- *(6) Sort [item_sk#7 ASC NULLS FIRST, d_date#9 ASC NULLS FIRST], false, 0
                  :  +- Exchange hashpartitioning(item_sk#7, d_date#9, 5)
                  :     +- *(5) Project [item_sk#7, d_date#9, cume_sales#11]
                  :        +- Window [sum(_w0#13) windowspecdefinition(ws_item_sk#14, d_date#9 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS cume_sales#11], [ws_item_sk#14], [d_date#9 ASC NULLS FIRST]
                  :           +- *(4) Sort [ws_item_sk#14 ASC NULLS FIRST, d_date#9 ASC NULLS FIRST], false, 0
                  :              +- Exchange hashpartitioning(ws_item_sk#14, 5)
                  :                 +- *(3) HashAggregate(keys=[ws_item_sk#14, d_date#9], functions=[sum(UnscaledValue(ws_sales_price#15))])
                  :                    +- Exchange hashpartitioning(ws_item_sk#14, d_date#9, 5)
                  :                       +- *(2) HashAggregate(keys=[ws_item_sk#14, d_date#9], functions=[partial_sum(UnscaledValue(ws_sales_price#15))])
                  :                          +- *(2) Project [ws_item_sk#14, ws_sales_price#15, d_date#9]
                  :                             +- *(2) BroadcastHashJoin [ws_sold_date_sk#16], [d_date_sk#17], Inner, BuildRight
                  :                                :- *(2) Project [ws_sold_date_sk#16, ws_item_sk#14, ws_sales_price#15]
                  :                                :  +- *(2) Filter (isnotnull(ws_item_sk#14) && isnotnull(ws_sold_date_sk#16))
                  :                                :     +- *(2) FileScan parquet default.web_sales[ws_sold_date_sk#16,ws_item_sk#14,ws_sales_price#15] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/web_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ws_item_sk), IsNotNull(ws_sold_date_sk)], ReadSchema: struct<ws_sold_date_sk:int,ws_item_sk:int,ws_sales_price:decimal(7,2)>
                  :                                +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                  :                                   +- *(1) Project [d_date_sk#17, d_date#9]
                  :                                      +- *(1) Filter (((isnotnull(d_month_seq#18) && (d_month_seq#18 >= 1200)) && (d_month_seq#18 <= 1211)) && isnotnull(d_date_sk#17))
                  :                                         +- *(1) FileScan parquet default.date_dim[d_date_sk#17,d_date#9,d_month_seq#18] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_month_seq), GreaterThanOrEqual(d_month_seq,1200), LessThanOrEqual(d_month_seq,1211),..., ReadSchema: struct<d_date_sk:int,d_date:date,d_month_seq:int>
                  +- *(12) Sort [item_sk#8 ASC NULLS FIRST, d_date#10 ASC NULLS FIRST], false, 0
                     +- Exchange hashpartitioning(item_sk#8, d_date#10, 5)
                        +- *(11) Project [item_sk#8, d_date#10, cume_sales#12]
                           +- Window [sum(_w0#19) windowspecdefinition(ss_item_sk#20, d_date#10 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS cume_sales#12], [ss_item_sk#20], [d_date#10 ASC NULLS FIRST]
                              +- *(10) Sort [ss_item_sk#20 ASC NULLS FIRST, d_date#10 ASC NULLS FIRST], false, 0
                                 +- Exchange hashpartitioning(ss_item_sk#20, 5)
                                    +- *(9) HashAggregate(keys=[ss_item_sk#20, d_date#10], functions=[sum(UnscaledValue(ss_sales_price#21))])
                                       +- Exchange hashpartitioning(ss_item_sk#20, d_date#10, 5)
                                          +- *(8) HashAggregate(keys=[ss_item_sk#20, d_date#10], functions=[partial_sum(UnscaledValue(ss_sales_price#21))])
                                             +- *(8) Project [ss_item_sk#20, ss_sales_price#21, d_date#10]
                                                +- *(8) BroadcastHashJoin [ss_sold_date_sk#22], [d_date_sk#23], Inner, BuildRight
                                                   :- *(8) Project [ss_sold_date_sk#22, ss_item_sk#20, ss_sales_price#21]
                                                   :  +- *(8) Filter (isnotnull(ss_item_sk#20) && isnotnull(ss_sold_date_sk#22))
                                                   :     +- *(8) FileScan parquet default.store_sales[ss_sold_date_sk#22,ss_item_sk#20,ss_sales_price#21] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_item_sk), IsNotNull(ss_sold_date_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_item_sk:int,ss_sales_price:decimal(7,2)>
                                                   +- ReusedExchange [d_date_sk#23, d_date#10], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))