== Physical Plan ==
*(13) Sort [d_week_seq1#1 ASC NULLS FIRST], true, 0
+- Exchange rangepartitioning(d_week_seq1#1 ASC NULLS FIRST, 5)
   +- *(12) Project [d_week_seq1#1, round(CheckOverflow((promote_precision(sun_sales1#2) / promote_precision(sun_sales2#3)), DecimalType(37,20)), 2) AS round((sun_sales1 / sun_sales2), 2)#4, round(CheckOverflow((promote_precision(mon_sales1#5) / promote_precision(mon_sales2#6)), DecimalType(37,20)), 2) AS round((mon_sales1 / mon_sales2), 2)#7, round(CheckOverflow((promote_precision(tue_sales1#8) / promote_precision(tue_sales2#9)), DecimalType(37,20)), 2) AS round((tue_sales1 / tue_sales2), 2)#10, round(CheckOverflow((promote_precision(wed_sales1#11) / promote_precision(wed_sales2#12)), DecimalType(37,20)), 2) AS round((wed_sales1 / wed_sales2), 2)#13, round(CheckOverflow((promote_precision(thu_sales1#14) / promote_precision(thu_sales2#15)), DecimalType(37,20)), 2) AS round((thu_sales1 / thu_sales2), 2)#16, round(CheckOverflow((promote_precision(fri_sales1#17) / promote_precision(fri_sales2#18)), DecimalType(37,20)), 2) AS round((fri_sales1 / fri_sales2), 2)#19, round(CheckOverflow((promote_precision(sat_sales1#20) / promote_precision(sat_sales2#21)), DecimalType(37,20)), 2) AS round((sat_sales1 / sat_sales2), 2)#22]
      +- *(12) BroadcastHashJoin [d_week_seq1#1], [(d_week_seq2#23 - 53)], Inner, BuildRight
         :- *(12) Project [d_week_seq#24 AS d_week_seq1#1, sun_sales#25 AS sun_sales1#2, mon_sales#26 AS mon_sales1#5, tue_sales#27 AS tue_sales1#8, wed_sales#28 AS wed_sales1#11, thu_sales#29 AS thu_sales1#14, fri_sales#30 AS fri_sales1#17, sat_sales#31 AS sat_sales1#20]
         :  +- *(12) BroadcastHashJoin [d_week_seq#24], [d_week_seq#32], Inner, BuildRight
         :     :- *(12) HashAggregate(keys=[d_week_seq#24], functions=[sum(UnscaledValue(CASE WHEN (d_day_name#33 = Sunday) THEN sales_price#34 ELSE null END)), sum(UnscaledValue(CASE WHEN (d_day_name#33 = Monday) THEN sales_price#34 ELSE null END)), sum(UnscaledValue(CASE WHEN (d_day_name#33 = Tuesday) THEN sales_price#34 ELSE null END)), sum(UnscaledValue(CASE WHEN (d_day_name#33 = Wednesday) THEN sales_price#34 ELSE null END)), sum(UnscaledValue(CASE WHEN (d_day_name#33 = Thursday) THEN sales_price#34 ELSE null END)), sum(UnscaledValue(CASE WHEN (d_day_name#33 = Friday) THEN sales_price#34 ELSE null END)), sum(UnscaledValue(CASE WHEN (d_day_name#33 = Saturday) THEN sales_price#34 ELSE null END))])
         :     :  +- Exchange hashpartitioning(d_week_seq#24, 5)
         :     :     +- *(4) HashAggregate(keys=[d_week_seq#24], functions=[partial_sum(UnscaledValue(CASE WHEN (d_day_name#33 = Sunday) THEN sales_price#34 ELSE null END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#33 = Monday) THEN sales_price#34 ELSE null END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#33 = Tuesday) THEN sales_price#34 ELSE null END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#33 = Wednesday) THEN sales_price#34 ELSE null END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#33 = Thursday) THEN sales_price#34 ELSE null END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#33 = Friday) THEN sales_price#34 ELSE null END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#33 = Saturday) THEN sales_price#34 ELSE null END))])
         :     :        +- *(4) Project [sales_price#34, d_week_seq#24, d_day_name#33]
         :     :           +- *(4) BroadcastHashJoin [sold_date_sk#35], [d_date_sk#36], Inner, BuildRight
         :     :              :- Union
         :     :              :  :- *(1) Project [ws_sold_date_sk#37 AS sold_date_sk#35, ws_ext_sales_price#38 AS sales_price#34]
         :     :              :  :  +- *(1) Filter isnotnull(ws_sold_date_sk#37)
         :     :              :  :     +- *(1) FileScan parquet default.web_sales[ws_sold_date_sk#37,ws_ext_sales_price#38] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/web_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ws_sold_date_sk)], ReadSchema: struct<ws_sold_date_sk:int,ws_ext_sales_price:decimal(7,2)>
         :     :              :  +- *(2) Project [cs_sold_date_sk#39 AS sold_date_sk#40, cs_ext_sales_price#41 AS sales_price#42]
         :     :              :     +- *(2) Filter isnotnull(cs_sold_date_sk#39)
         :     :              :        +- *(2) FileScan parquet default.catalog_sales[cs_sold_date_sk#39,cs_ext_sales_price#41] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/catalog_sales], PartitionFilters: [], PushedFilters: [IsNotNull(cs_sold_date_sk)], ReadSchema: struct<cs_sold_date_sk:int,cs_ext_sales_price:decimal(7,2)>
         :     :              +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
         :     :                 +- *(3) Project [d_date_sk#36, d_week_seq#24, d_day_name#33]
         :     :                    +- *(3) Filter (isnotnull(d_date_sk#36) && isnotnull(d_week_seq#24))
         :     :                       +- *(3) FileScan parquet default.date_dim[d_date_sk#36,d_week_seq#24,d_day_name#33] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_date_sk), IsNotNull(d_week_seq)], ReadSchema: struct<d_date_sk:int,d_week_seq:int,d_day_name:string>
         :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
         :        +- *(5) Project [d_week_seq#32]
         :           +- *(5) Filter ((isnotnull(d_year#43) && (d_year#43 = 2001)) && isnotnull(d_week_seq#32))
         :              +- *(5) FileScan parquet default.date_dim[d_week_seq#32,d_year#43] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2001), IsNotNull(d_week_seq)], ReadSchema: struct<d_week_seq:int,d_year:int>
         +- BroadcastExchange HashedRelationBroadcastMode(List(cast((input[0, int, true] - 53) as bigint)))
            +- *(11) Project [d_week_seq#24 AS d_week_seq2#23, sun_sales#25 AS sun_sales2#3, mon_sales#26 AS mon_sales2#6, tue_sales#27 AS tue_sales2#9, wed_sales#28 AS wed_sales2#12, thu_sales#29 AS thu_sales2#15, fri_sales#30 AS fri_sales2#18, sat_sales#31 AS sat_sales2#21]
               +- *(11) BroadcastHashJoin [d_week_seq#24], [d_week_seq#44], Inner, BuildRight
                  :- *(11) HashAggregate(keys=[d_week_seq#24], functions=[sum(UnscaledValue(CASE WHEN (d_day_name#33 = Sunday) THEN sales_price#34 ELSE null END)), sum(UnscaledValue(CASE WHEN (d_day_name#33 = Monday) THEN sales_price#34 ELSE null END)), sum(UnscaledValue(CASE WHEN (d_day_name#33 = Tuesday) THEN sales_price#34 ELSE null END)), sum(UnscaledValue(CASE WHEN (d_day_name#33 = Wednesday) THEN sales_price#34 ELSE null END)), sum(UnscaledValue(CASE WHEN (d_day_name#33 = Thursday) THEN sales_price#34 ELSE null END)), sum(UnscaledValue(CASE WHEN (d_day_name#33 = Friday) THEN sales_price#34 ELSE null END)), sum(UnscaledValue(CASE WHEN (d_day_name#33 = Saturday) THEN sales_price#34 ELSE null END))])
                  :  +- ReusedExchange [d_week_seq#24, sum#45, sum#46, sum#47, sum#48, sum#49, sum#50, sum#51], Exchange hashpartitioning(d_week_seq#24, 5)
                  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                     +- *(10) Project [d_week_seq#44]
                        +- *(10) Filter ((isnotnull(d_year#52) && (d_year#52 = 2002)) && isnotnull(d_week_seq#44))
                           +- *(10) FileScan parquet default.date_dim[d_week_seq#44,d_year#52] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2002), IsNotNull(d_week_seq)], ReadSchema: struct<d_week_seq:int,d_year:int>