== Physical Plan ==
TakeOrderedAndProject(limit=100, orderBy=[c_customer_id#1 ASC NULLS FIRST,c_salutation#2 ASC NULLS FIRST,c_first_name#3 ASC NULLS FIRST,c_last_name#4 ASC NULLS FIRST,c_preferred_cust_flag#5 ASC NULLS FIRST,c_birth_day#6 ASC NULLS FIRST,c_birth_month#7 ASC NULLS FIRST,c_birth_year#8 ASC NULLS FIRST,c_birth_country#9 ASC NULLS FIRST,c_login#10 ASC NULLS FIRST,c_email_address#11 ASC NULLS FIRST,c_last_review_date#12 ASC NULLS FIRST,ctr_total_return#13 ASC NULLS FIRST], output=[c_customer_id#1,c_salutation#2,c_first_name#3,c_last_name#4,c_preferred_cust_flag#5,c_birth_day#6,c_birth_month#7,c_birth_year#8,c_birth_country#9,c_login#10,c_email_address#11,c_last_review_date#12,ctr_total_return#13])
+- *(11) Project [c_customer_id#1, c_salutation#2, c_first_name#3, c_last_name#4, c_preferred_cust_flag#5, c_birth_day#6, c_birth_month#7, c_birth_year#8, c_birth_country#9, c_login#10, c_email_address#11, c_last_review_date#12, ctr_total_return#13]
   +- *(11) BroadcastHashJoin [c_current_addr_sk#14], [ca_address_sk#15], Inner, BuildRight
      :- *(11) Project [ctr_total_return#13, c_customer_id#1, c_current_addr_sk#14, c_salutation#2, c_first_name#3, c_last_name#4, c_preferred_cust_flag#5, c_birth_day#6, c_birth_month#7, c_birth_year#8, c_birth_country#9, c_login#10, c_email_address#11, c_last_review_date#12]
      :  +- *(11) BroadcastHashJoin [ctr_customer_sk#16], [cast(c_customer_sk#17 as bigint)], Inner, BuildRight
      :     :- *(11) Project [ctr_customer_sk#16, ctr_total_return#13]
      :     :  +- *(11) BroadcastHashJoin [ctr_state#18], [ctr_state#18#19], Inner, BuildRight, (cast(ctr_total_return#13 as decimal(24,7)) > (CAST(avg(ctr_total_return) AS DECIMAL(21,6)) * CAST(1.2 AS DECIMAL(21,6)))#20)
      :     :     :- *(11) Filter isnotnull(ctr_total_return#13)
      :     :     :  +- *(11) HashAggregate(keys=[wr_returning_customer_sk#21, ca_state#22], functions=[sum(UnscaledValue(wr_return_amt#23))])
      :     :     :     +- Exchange hashpartitioning(wr_returning_customer_sk#21, ca_state#22, 5)
      :     :     :        +- *(3) HashAggregate(keys=[wr_returning_customer_sk#21, ca_state#22], functions=[partial_sum(UnscaledValue(wr_return_amt#23))])
      :     :     :           +- *(3) Project [wr_returning_customer_sk#21, wr_return_amt#23, ca_state#22]
      :     :     :              +- *(3) BroadcastHashJoin [wr_returning_addr_sk#24], [cast(ca_address_sk#15 as bigint)], Inner, BuildRight
      :     :     :                 :- *(3) Project [wr_returning_customer_sk#21, wr_returning_addr_sk#24, wr_return_amt#23]
      :     :     :                 :  +- *(3) BroadcastHashJoin [wr_returned_date_sk#25], [cast(d_date_sk#26 as bigint)], Inner, BuildRight
      :     :     :                 :     :- *(3) Project [wr_returned_date_sk#25, wr_returning_customer_sk#21, wr_returning_addr_sk#24, wr_return_amt#23]
      :     :     :                 :     :  +- *(3) Filter ((isnotnull(wr_returned_date_sk#25) && isnotnull(wr_returning_addr_sk#24)) && isnotnull(wr_returning_customer_sk#21))
      :     :     :                 :     :     +- *(3) FileScan parquet default.web_returns[wr_returned_date_sk#25,wr_returning_customer_sk#21,wr_returning_addr_sk#24,wr_return_amt#23] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/web_returns], PartitionFilters: [], PushedFilters: [IsNotNull(wr_returned_date_sk), IsNotNull(wr_returning_addr_sk), IsNotNull(wr_returning_customer..., ReadSchema: struct<wr_returned_date_sk:bigint,wr_returning_customer_sk:bigint,wr_returning_addr_sk:bigint,wr_...
      :     :     :                 :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
      :     :     :                 :        +- *(1) Project [d_date_sk#26]
      :     :     :                 :           +- *(1) Filter ((isnotnull(d_year#27) && (d_year#27 = 2002)) && isnotnull(d_date_sk#26))
      :     :     :                 :              +- *(1) FileScan parquet default.date_dim[d_date_sk#26,d_year#27] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2002), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_year:int>
      :     :     :                 +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
      :     :     :                    +- *(2) Project [ca_address_sk#15, ca_state#22]
      :     :     :                       +- *(2) Filter (isnotnull(ca_address_sk#15) && isnotnull(ca_state#22))
      :     :     :                          +- *(2) FileScan parquet default.customer_address[ca_address_sk#15,ca_state#22] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/customer_address], PartitionFilters: [], PushedFilters: [IsNotNull(ca_address_sk), IsNotNull(ca_state)], ReadSchema: struct<ca_address_sk:int,ca_state:string>
      :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]))
      :     :        +- *(8) Filter isnotnull((CAST(avg(ctr_total_return) AS DECIMAL(21,6)) * CAST(1.2 AS DECIMAL(21,6)))#20)
      :     :           +- *(8) HashAggregate(keys=[ctr_state#18], functions=[avg(ctr_total_return#13)])
      :     :              +- Exchange hashpartitioning(ctr_state#18, 5)
      :     :                 +- *(7) HashAggregate(keys=[ctr_state#18], functions=[partial_avg(ctr_total_return#13)])
      :     :                    +- *(7) HashAggregate(keys=[wr_returning_customer_sk#21, ca_state#22], functions=[sum(UnscaledValue(wr_return_amt#23))])
      :     :                       +- Exchange hashpartitioning(wr_returning_customer_sk#21, ca_state#22, 5)
      :     :                          +- *(6) HashAggregate(keys=[wr_returning_customer_sk#21, ca_state#22], functions=[partial_sum(UnscaledValue(wr_return_amt#23))])
      :     :                             +- *(6) Project [wr_returning_customer_sk#21, wr_return_amt#23, ca_state#22]
      :     :                                +- *(6) BroadcastHashJoin [wr_returning_addr_sk#24], [cast(ca_address_sk#15 as bigint)], Inner, BuildRight
      :     :                                   :- *(6) Project [wr_returning_customer_sk#21, wr_returning_addr_sk#24, wr_return_amt#23]
      :     :                                   :  +- *(6) BroadcastHashJoin [wr_returned_date_sk#25], [cast(d_date_sk#26 as bigint)], Inner, BuildRight
      :     :                                   :     :- *(6) Project [wr_returned_date_sk#25, wr_returning_customer_sk#21, wr_returning_addr_sk#24, wr_return_amt#23]
      :     :                                   :     :  +- *(6) Filter (isnotnull(wr_returned_date_sk#25) && isnotnull(wr_returning_addr_sk#24))
      :     :                                   :     :     +- *(6) FileScan parquet default.web_returns[wr_returned_date_sk#25,wr_returning_customer_sk#21,wr_returning_addr_sk#24,wr_return_amt#23] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/web_returns], PartitionFilters: [], PushedFilters: [IsNotNull(wr_returned_date_sk), IsNotNull(wr_returning_addr_sk)], ReadSchema: struct<wr_returned_date_sk:bigint,wr_returning_customer_sk:bigint,wr_returning_addr_sk:bigint,wr_...
      :     :                                   :     +- ReusedExchange [d_date_sk#26], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
      :     :                                   +- ReusedExchange [ca_address_sk#15, ca_state#22], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
      :        +- *(9) Project [c_customer_sk#17, c_customer_id#1, c_current_addr_sk#14, c_salutation#2, c_first_name#3, c_last_name#4, c_preferred_cust_flag#5, c_birth_day#6, c_birth_month#7, c_birth_year#8, c_birth_country#9, c_login#10, c_email_address#11, c_last_review_date#12]
      :           +- *(9) Filter (isnotnull(c_customer_sk#17) && isnotnull(c_current_addr_sk#14))
      :              +- *(9) FileScan parquet default.customer[c_customer_sk#17,c_customer_id#1,c_current_addr_sk#14,c_salutation#2,c_first_name#3,c_last_name#4,c_preferred_cust_flag#5,c_birth_day#6,c_birth_month#7,c_birth_year#8,c_birth_country#9,c_login#10,c_email_address#11,c_last_review_date#12] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/customer], PartitionFilters: [], PushedFilters: [IsNotNull(c_customer_sk), IsNotNull(c_current_addr_sk)], ReadSchema: struct<c_customer_sk:int,c_customer_id:string,c_current_addr_sk:int,c_salutation:string,c_first_n...
      +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
         +- *(10) Project [ca_address_sk#15]
            +- *(10) Filter ((isnotnull(ca_state#22) && (ca_state#22 = GA)) && isnotnull(ca_address_sk#15))
               +- *(10) FileScan parquet default.customer_address[ca_address_sk#15,ca_state#22] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/customer_address], PartitionFilters: [], PushedFilters: [IsNotNull(ca_state), EqualTo(ca_state,GA), IsNotNull(ca_address_sk)], ReadSchema: struct<ca_address_sk:int,ca_state:string>