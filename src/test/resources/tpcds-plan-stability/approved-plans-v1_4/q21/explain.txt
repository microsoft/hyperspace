== Physical Plan ==
TakeOrderedAndProject(limit=100, orderBy=[w_warehouse_name#1 ASC NULLS FIRST,i_item_id#2 ASC NULLS FIRST], output=[w_warehouse_name#1,i_item_id#2,inv_before#3,inv_after#4])
+- *(5) Filter ((CASE WHEN (inv_before#3 > 0) THEN (cast(inv_after#4 as double) / cast(inv_before#3 as double)) ELSE null END >= 0.666667) && (CASE WHEN (inv_before#3 > 0) THEN (cast(inv_after#4 as double) / cast(inv_before#3 as double)) ELSE null END <= 1.5))
   +- *(5) HashAggregate(keys=[w_warehouse_name#1, i_item_id#2], functions=[sum(cast(CASE WHEN (d_date#5 < 11027) THEN inv_quantity_on_hand#6 ELSE 0 END as bigint)), sum(cast(CASE WHEN (d_date#5 >= 11027) THEN inv_quantity_on_hand#6 ELSE 0 END as bigint))])
      +- Exchange hashpartitioning(w_warehouse_name#1, i_item_id#2, 200)
         +- *(4) HashAggregate(keys=[w_warehouse_name#1, i_item_id#2], functions=[partial_sum(cast(CASE WHEN (d_date#5 < 11027) THEN inv_quantity_on_hand#6 ELSE 0 END as bigint)), partial_sum(cast(CASE WHEN (d_date#5 >= 11027) THEN inv_quantity_on_hand#6 ELSE 0 END as bigint))])
            +- *(4) Project [inv_quantity_on_hand#6, w_warehouse_name#1, i_item_id#2, d_date#5]
               +- *(4) BroadcastHashJoin [inv_date_sk#7], [d_date_sk#8], Inner, BuildRight
                  :- *(4) Project [inv_date_sk#7, inv_quantity_on_hand#6, w_warehouse_name#1, i_item_id#2]
                  :  +- *(4) BroadcastHashJoin [inv_item_sk#9], [i_item_sk#10], Inner, BuildRight
                  :     :- *(4) Project [inv_date_sk#7, inv_item_sk#9, inv_quantity_on_hand#6, w_warehouse_name#1]
                  :     :  +- *(4) BroadcastHashJoin [inv_warehouse_sk#11], [w_warehouse_sk#12], Inner, BuildRight
                  :     :     :- *(4) Project [inv_date_sk#7, inv_item_sk#9, inv_warehouse_sk#11, inv_quantity_on_hand#6]
                  :     :     :  +- *(4) Filter ((isnotnull(inv_warehouse_sk#11) && isnotnull(inv_item_sk#9)) && isnotnull(inv_date_sk#7))
                  :     :     :     +- *(4) FileScan parquet default.inventory[inv_date_sk#7,inv_item_sk#9,inv_warehouse_sk#11,inv_quantity_on_hand#6] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/inventory], PartitionFilters: [], PushedFilters: [IsNotNull(inv_warehouse_sk), IsNotNull(inv_item_sk), IsNotNull(inv_date_sk)], ReadSchema: struct<inv_date_sk:int,inv_item_sk:int,inv_warehouse_sk:int,inv_quantity_on_hand:int>
                  :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                  :     :        +- *(1) Project [w_warehouse_sk#12, w_warehouse_name#1]
                  :     :           +- *(1) Filter isnotnull(w_warehouse_sk#12)
                  :     :              +- *(1) FileScan parquet default.warehouse[w_warehouse_sk#12,w_warehouse_name#1] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/warehouse], PartitionFilters: [], PushedFilters: [IsNotNull(w_warehouse_sk)], ReadSchema: struct<w_warehouse_sk:int,w_warehouse_name:string>
                  :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                  :        +- *(2) Project [i_item_sk#10, i_item_id#2]
                  :           +- *(2) Filter (((isnotnull(i_current_price#13) && (i_current_price#13 >= 0.99)) && (i_current_price#13 <= 1.49)) && isnotnull(i_item_sk#10))
                  :              +- *(2) FileScan parquet default.item[i_item_sk#10,i_item_id#2,i_current_price#13] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_current_price), GreaterThanOrEqual(i_current_price,0.99), LessThanOrEqual(i_current_..., ReadSchema: struct<i_item_sk:int,i_item_id:string,i_current_price:decimal(7,2)>
                  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                     +- *(3) Project [d_date_sk#8, d_date#5]
                        +- *(3) Filter (((isnotnull(d_date#5) && (d_date#5 >= 10997)) && (d_date#5 <= 11057)) && isnotnull(d_date_sk#8))
                           +- *(3) FileScan parquet default.date_dim[d_date_sk#8,d_date#5] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_date), GreaterThanOrEqual(d_date,2000-02-10), LessThanOrEqual(d_date,2000-04-10), Is..., ReadSchema: struct<d_date_sk:int,d_date:date>