== Physical Plan ==
TakeOrderedAndProject(limit=100, orderBy=[ratio#1 ASC NULLS FIRST,ss_qty#2 DESC NULLS LAST,ss_wc#3 DESC NULLS LAST,ss_sp#4 DESC NULLS LAST,other_chan_qty#5 ASC NULLS FIRST,other_chan_wholesale_cost#6 ASC NULLS FIRST,other_chan_sales_price#7 ASC NULLS FIRST,round((cast(ss_qty#2 as double) / cast(coalesce((ws_qty#8 + cs_qty#9), 1) as double)), 2) ASC NULLS FIRST], output=[ratio#1,store_qty#10,store_wholesale_cost#11,store_sales_price#12,other_chan_qty#5,other_chan_wholesale_cost#6,other_chan_sales_price#7])
+- *(12) Project [round((cast(ss_qty#2 as double) / cast(coalesce((ws_qty#8 + cs_qty#9), 1) as double)), 2) AS ratio#1, ss_qty#2 AS store_qty#10, ss_wc#3 AS store_wholesale_cost#11, ss_sp#4 AS store_sales_price#12, (coalesce(ws_qty#8, 0) + coalesce(cs_qty#9, 0)) AS other_chan_qty#5, CheckOverflow((promote_precision(cast(coalesce(ws_wc#13, 0.00) as decimal(18,2))) + promote_precision(cast(coalesce(cs_wc#14, 0.00) as decimal(18,2)))), DecimalType(18,2)) AS other_chan_wholesale_cost#6, CheckOverflow((promote_precision(cast(coalesce(ws_sp#15, 0.00) as decimal(18,2))) + promote_precision(cast(coalesce(cs_sp#16, 0.00) as decimal(18,2)))), DecimalType(18,2)) AS other_chan_sales_price#7, cs_qty#9, ws_qty#8, ss_qty#2, ss_sp#4, ss_wc#3]
   +- *(12) BroadcastHashJoin [ss_sold_year#17, ss_item_sk#18, ss_customer_sk#19], [cs_sold_year#20, cs_item_sk#21, cs_customer_sk#22], Inner, BuildRight
      :- *(12) Project [ss_sold_year#17, ss_item_sk#18, ss_customer_sk#19, ss_qty#2, ss_wc#3, ss_sp#4, ws_qty#8, ws_wc#13, ws_sp#15]
      :  +- *(12) BroadcastHashJoin [ss_sold_year#17, ss_item_sk#18, ss_customer_sk#19], [ws_sold_year#23, ws_item_sk#24, ws_customer_sk#25], Inner, BuildRight
      :     :- *(12) HashAggregate(keys=[d_year#26, ss_item_sk#18, ss_customer_sk#19], functions=[sum(cast(ss_quantity#27 as bigint)), sum(UnscaledValue(ss_wholesale_cost#28)), sum(UnscaledValue(ss_sales_price#29))])
      :     :  +- Exchange hashpartitioning(d_year#26, ss_item_sk#18, ss_customer_sk#19, 200)
      :     :     +- *(3) HashAggregate(keys=[d_year#26, ss_item_sk#18, ss_customer_sk#19], functions=[partial_sum(cast(ss_quantity#27 as bigint)), partial_sum(UnscaledValue(ss_wholesale_cost#28)), partial_sum(UnscaledValue(ss_sales_price#29))])
      :     :        +- *(3) Project [ss_item_sk#18, ss_customer_sk#19, ss_quantity#27, ss_wholesale_cost#28, ss_sales_price#29, d_year#26]
      :     :           +- *(3) BroadcastHashJoin [ss_sold_date_sk#30], [d_date_sk#31], Inner, BuildRight
      :     :              :- *(3) Project [ss_sold_date_sk#30, ss_item_sk#18, ss_customer_sk#19, ss_quantity#27, ss_wholesale_cost#28, ss_sales_price#29]
      :     :              :  +- *(3) Filter isnull(sr_ticket_number#32)
      :     :              :     +- *(3) BroadcastHashJoin [cast(ss_ticket_number#33 as bigint), cast(ss_item_sk#18 as bigint)], [sr_ticket_number#32, sr_item_sk#34], LeftOuter, BuildRight
      :     :              :        :- *(3) Project [ss_sold_date_sk#30, ss_item_sk#18, ss_customer_sk#19, ss_ticket_number#33, ss_quantity#27, ss_wholesale_cost#28, ss_sales_price#29]
      :     :              :        :  +- *(3) Filter ((isnotnull(ss_sold_date_sk#30) && isnotnull(ss_item_sk#18)) && isnotnull(ss_customer_sk#19))
      :     :              :        :     +- *(3) FileScan parquet default.store_sales[ss_sold_date_sk#30,ss_item_sk#18,ss_customer_sk#19,ss_ticket_number#33,ss_quantity#27,ss_wholesale_cost#28,ss_sales_price#29] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_sold_date_sk), IsNotNull(ss_item_sk), IsNotNull(ss_customer_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_item_sk:int,ss_customer_sk:int,ss_ticket_number:int,ss_quantity:int...
      :     :              :        +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, bigint, true], input[0, bigint, true]))
      :     :              :           +- *(1) Project [sr_item_sk#34, sr_ticket_number#32]
      :     :              :              +- *(1) Filter (isnotnull(sr_ticket_number#32) && isnotnull(sr_item_sk#34))
      :     :              :                 +- *(1) FileScan parquet default.store_returns[sr_item_sk#34,sr_ticket_number#32] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_returns], PartitionFilters: [], PushedFilters: [IsNotNull(sr_ticket_number), IsNotNull(sr_item_sk)], ReadSchema: struct<sr_item_sk:bigint,sr_ticket_number:bigint>
      :     :              +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
      :     :                 +- *(2) Project [d_date_sk#31, d_year#26]
      :     :                    +- *(2) Filter ((isnotnull(d_year#26) && (d_year#26 = 2000)) && isnotnull(d_date_sk#31))
      :     :                       +- *(2) FileScan parquet default.date_dim[d_date_sk#31,d_year#26] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2000), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_year:int>
      :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, int, true], input[1, int, true], input[2, int, true]))
      :        +- *(7) Filter (coalesce(ws_qty#8, 0) > 0)
      :           +- *(7) HashAggregate(keys=[d_year#26, ws_item_sk#24, ws_bill_customer_sk#35], functions=[sum(cast(ws_quantity#36 as bigint)), sum(UnscaledValue(ws_wholesale_cost#37)), sum(UnscaledValue(ws_sales_price#38))])
      :              +- Exchange hashpartitioning(d_year#26, ws_item_sk#24, ws_bill_customer_sk#35, 200)
      :                 +- *(6) HashAggregate(keys=[d_year#26, ws_item_sk#24, ws_bill_customer_sk#35], functions=[partial_sum(cast(ws_quantity#36 as bigint)), partial_sum(UnscaledValue(ws_wholesale_cost#37)), partial_sum(UnscaledValue(ws_sales_price#38))])
      :                    +- *(6) Project [ws_item_sk#24, ws_bill_customer_sk#35, ws_quantity#36, ws_wholesale_cost#37, ws_sales_price#38, d_year#26]
      :                       +- *(6) BroadcastHashJoin [ws_sold_date_sk#39], [d_date_sk#31], Inner, BuildRight
      :                          :- *(6) Project [ws_sold_date_sk#39, ws_item_sk#24, ws_bill_customer_sk#35, ws_quantity#36, ws_wholesale_cost#37, ws_sales_price#38]
      :                          :  +- *(6) Filter isnull(wr_order_number#40)
      :                          :     +- *(6) BroadcastHashJoin [cast(ws_order_number#41 as bigint), cast(ws_item_sk#24 as bigint)], [wr_order_number#40, wr_item_sk#42], LeftOuter, BuildRight
      :                          :        :- *(6) Project [ws_sold_date_sk#39, ws_item_sk#24, ws_bill_customer_sk#35, ws_order_number#41, ws_quantity#36, ws_wholesale_cost#37, ws_sales_price#38]
      :                          :        :  +- *(6) Filter ((isnotnull(ws_sold_date_sk#39) && isnotnull(ws_item_sk#24)) && isnotnull(ws_bill_customer_sk#35))
      :                          :        :     +- *(6) FileScan parquet default.web_sales[ws_sold_date_sk#39,ws_item_sk#24,ws_bill_customer_sk#35,ws_order_number#41,ws_quantity#36,ws_wholesale_cost#37,ws_sales_price#38] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/web_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ws_sold_date_sk), IsNotNull(ws_item_sk), IsNotNull(ws_bill_customer_sk)], ReadSchema: struct<ws_sold_date_sk:int,ws_item_sk:int,ws_bill_customer_sk:int,ws_order_number:int,ws_quantity...
      :                          :        +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, bigint, true], input[0, bigint, true]))
      :                          :           +- *(4) Project [wr_item_sk#42, wr_order_number#40]
      :                          :              +- *(4) Filter (isnotnull(wr_order_number#40) && isnotnull(wr_item_sk#42))
      :                          :                 +- *(4) FileScan parquet default.web_returns[wr_item_sk#42,wr_order_number#40] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/web_returns], PartitionFilters: [], PushedFilters: [IsNotNull(wr_order_number), IsNotNull(wr_item_sk)], ReadSchema: struct<wr_item_sk:bigint,wr_order_number:bigint>
      :                          +- ReusedExchange [d_date_sk#31, d_year#26], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, int, true], input[1, int, true], input[2, int, true]))
         +- *(11) Filter (coalesce(cs_qty#9, 0) > 0)
            +- *(11) HashAggregate(keys=[d_year#26, cs_item_sk#21, cs_bill_customer_sk#43], functions=[sum(cast(cs_quantity#44 as bigint)), sum(UnscaledValue(cs_wholesale_cost#45)), sum(UnscaledValue(cs_sales_price#46))])
               +- Exchange hashpartitioning(d_year#26, cs_item_sk#21, cs_bill_customer_sk#43, 200)
                  +- *(10) HashAggregate(keys=[d_year#26, cs_item_sk#21, cs_bill_customer_sk#43], functions=[partial_sum(cast(cs_quantity#44 as bigint)), partial_sum(UnscaledValue(cs_wholesale_cost#45)), partial_sum(UnscaledValue(cs_sales_price#46))])
                     +- *(10) Project [cs_bill_customer_sk#43, cs_item_sk#21, cs_quantity#44, cs_wholesale_cost#45, cs_sales_price#46, d_year#26]
                        +- *(10) BroadcastHashJoin [cs_sold_date_sk#47], [d_date_sk#31], Inner, BuildRight
                           :- *(10) Project [cs_sold_date_sk#47, cs_bill_customer_sk#43, cs_item_sk#21, cs_quantity#44, cs_wholesale_cost#45, cs_sales_price#46]
                           :  +- *(10) Filter isnull(cr_order_number#48)
                           :     +- *(10) BroadcastHashJoin [cs_order_number#49, cs_item_sk#21], [cr_order_number#48, cr_item_sk#50], LeftOuter, BuildRight
                           :        :- *(10) Project [cs_sold_date_sk#47, cs_bill_customer_sk#43, cs_item_sk#21, cs_order_number#49, cs_quantity#44, cs_wholesale_cost#45, cs_sales_price#46]
                           :        :  +- *(10) Filter ((isnotnull(cs_sold_date_sk#47) && isnotnull(cs_item_sk#21)) && isnotnull(cs_bill_customer_sk#43))
                           :        :     +- *(10) FileScan parquet default.catalog_sales[cs_sold_date_sk#47,cs_bill_customer_sk#43,cs_item_sk#21,cs_order_number#49,cs_quantity#44,cs_wholesale_cost#45,cs_sales_price#46] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/catalog_sales], PartitionFilters: [], PushedFilters: [IsNotNull(cs_sold_date_sk), IsNotNull(cs_item_sk), IsNotNull(cs_bill_customer_sk)], ReadSchema: struct<cs_sold_date_sk:int,cs_bill_customer_sk:int,cs_item_sk:int,cs_order_number:int,cs_quantity...
                           :        +- BroadcastExchange HashedRelationBroadcastMode(List((shiftleft(cast(input[1, int, true] as bigint), 32) | (cast(input[0, int, true] as bigint) & 4294967295))))
                           :           +- *(8) Project [cr_item_sk#50, cr_order_number#48]
                           :              +- *(8) Filter (isnotnull(cr_order_number#48) && isnotnull(cr_item_sk#50))
                           :                 +- *(8) FileScan parquet default.catalog_returns[cr_item_sk#50,cr_order_number#48] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/catalog_returns], PartitionFilters: [], PushedFilters: [IsNotNull(cr_order_number), IsNotNull(cr_item_sk)], ReadSchema: struct<cr_item_sk:int,cr_order_number:int>
                           +- ReusedExchange [d_date_sk#31, d_year#26], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))