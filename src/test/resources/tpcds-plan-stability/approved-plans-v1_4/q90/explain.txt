== Physical Plan ==
TakeOrderedAndProject(limit=100, orderBy=[am_pm_ratio#1 ASC NULLS FIRST], output=[am_pm_ratio#1])
+- *(11) Project [CheckOverflow((promote_precision(cast(amc#2 as decimal(15,4))) / promote_precision(cast(pmc#3 as decimal(15,4)))), DecimalType(35,20)) AS am_pm_ratio#1]
   +- BroadcastNestedLoopJoin BuildRight, Inner
      :- *(5) HashAggregate(keys=[], functions=[count(1)])
      :  +- Exchange SinglePartition
      :     +- *(4) HashAggregate(keys=[], functions=[partial_count(1)])
      :        +- *(4) Project
      :           +- *(4) BroadcastHashJoin [ws_web_page_sk#4], [wp_web_page_sk#5], Inner, BuildRight
      :              :- *(4) Project [ws_web_page_sk#4]
      :              :  +- *(4) BroadcastHashJoin [ws_sold_time_sk#6], [t_time_sk#7], Inner, BuildRight
      :              :     :- *(4) Project [ws_sold_time_sk#6, ws_web_page_sk#4]
      :              :     :  +- *(4) BroadcastHashJoin [ws_ship_hdemo_sk#8], [hd_demo_sk#9], Inner, BuildRight
      :              :     :     :- *(4) Project [ws_sold_time_sk#6, ws_ship_hdemo_sk#8, ws_web_page_sk#4]
      :              :     :     :  +- *(4) Filter ((isnotnull(ws_ship_hdemo_sk#8) && isnotnull(ws_sold_time_sk#6)) && isnotnull(ws_web_page_sk#4))
      :              :     :     :     +- *(4) FileScan parquet default.web_sales[ws_sold_time_sk#6,ws_ship_hdemo_sk#8,ws_web_page_sk#4] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/web_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ws_ship_hdemo_sk), IsNotNull(ws_sold_time_sk), IsNotNull(ws_web_page_sk)], ReadSchema: struct<ws_sold_time_sk:int,ws_ship_hdemo_sk:int,ws_web_page_sk:int>
      :              :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
      :              :     :        +- *(1) Project [hd_demo_sk#9]
      :              :     :           +- *(1) Filter ((isnotnull(hd_dep_count#10) && (hd_dep_count#10 = 6)) && isnotnull(hd_demo_sk#9))
      :              :     :              +- *(1) FileScan parquet default.household_demographics[hd_demo_sk#9,hd_dep_count#10] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/household_demographics], PartitionFilters: [], PushedFilters: [IsNotNull(hd_dep_count), EqualTo(hd_dep_count,6), IsNotNull(hd_demo_sk)], ReadSchema: struct<hd_demo_sk:int,hd_dep_count:int>
      :              :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
      :              :        +- *(2) Project [t_time_sk#7]
      :              :           +- *(2) Filter (((isnotnull(t_hour#11) && (t_hour#11 >= 8)) && (t_hour#11 <= 9)) && isnotnull(t_time_sk#7))
      :              :              +- *(2) FileScan parquet default.time_dim[t_time_sk#7,t_hour#11] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/time_dim], PartitionFilters: [], PushedFilters: [IsNotNull(t_hour), GreaterThanOrEqual(t_hour,8), LessThanOrEqual(t_hour,9), IsNotNull(t_time_sk)], ReadSchema: struct<t_time_sk:int,t_hour:int>
      :              +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
      :                 +- *(3) Project [wp_web_page_sk#5]
      :                    +- *(3) Filter (((isnotnull(wp_char_count#12) && (wp_char_count#12 >= 5000)) && (wp_char_count#12 <= 5200)) && isnotnull(wp_web_page_sk#5))
      :                       +- *(3) FileScan parquet default.web_page[wp_web_page_sk#5,wp_char_count#12] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/web_page], PartitionFilters: [], PushedFilters: [IsNotNull(wp_char_count), GreaterThanOrEqual(wp_char_count,5000), LessThanOrEqual(wp_char_count,..., ReadSchema: struct<wp_web_page_sk:int,wp_char_count:int>
      +- BroadcastExchange IdentityBroadcastMode
         +- *(10) HashAggregate(keys=[], functions=[count(1)])
            +- Exchange SinglePartition
               +- *(9) HashAggregate(keys=[], functions=[partial_count(1)])
                  +- *(9) Project
                     +- *(9) BroadcastHashJoin [ws_web_page_sk#4], [wp_web_page_sk#5], Inner, BuildRight
                        :- *(9) Project [ws_web_page_sk#4]
                        :  +- *(9) BroadcastHashJoin [ws_sold_time_sk#6], [t_time_sk#7], Inner, BuildRight
                        :     :- *(9) Project [ws_sold_time_sk#6, ws_web_page_sk#4]
                        :     :  +- *(9) BroadcastHashJoin [ws_ship_hdemo_sk#8], [hd_demo_sk#9], Inner, BuildRight
                        :     :     :- *(9) Project [ws_sold_time_sk#6, ws_ship_hdemo_sk#8, ws_web_page_sk#4]
                        :     :     :  +- *(9) Filter ((isnotnull(ws_ship_hdemo_sk#8) && isnotnull(ws_sold_time_sk#6)) && isnotnull(ws_web_page_sk#4))
                        :     :     :     +- *(9) FileScan parquet default.web_sales[ws_sold_time_sk#6,ws_ship_hdemo_sk#8,ws_web_page_sk#4] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/web_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ws_ship_hdemo_sk), IsNotNull(ws_sold_time_sk), IsNotNull(ws_web_page_sk)], ReadSchema: struct<ws_sold_time_sk:int,ws_ship_hdemo_sk:int,ws_web_page_sk:int>
                        :     :     +- ReusedExchange [hd_demo_sk#9], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                        :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                        :        +- *(7) Project [t_time_sk#7]
                        :           +- *(7) Filter (((isnotnull(t_hour#11) && (t_hour#11 >= 19)) && (t_hour#11 <= 20)) && isnotnull(t_time_sk#7))
                        :              +- *(7) FileScan parquet default.time_dim[t_time_sk#7,t_hour#11] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/time_dim], PartitionFilters: [], PushedFilters: [IsNotNull(t_hour), GreaterThanOrEqual(t_hour,19), LessThanOrEqual(t_hour,20), IsNotNull(t_time_sk)], ReadSchema: struct<t_time_sk:int,t_hour:int>
                        +- ReusedExchange [wp_web_page_sk#5], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))