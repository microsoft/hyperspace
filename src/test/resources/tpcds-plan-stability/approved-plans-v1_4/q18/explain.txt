== Physical Plan ==
TakeOrderedAndProject(limit=100, orderBy=[ca_country#1 ASC NULLS FIRST,ca_state#2 ASC NULLS FIRST,ca_county#3 ASC NULLS FIRST,i_item_id#4 ASC NULLS FIRST], output=[i_item_id#4,ca_country#1,ca_state#2,ca_county#3,agg1#5,agg2#6,agg3#7,agg4#8,agg5#9,agg6#10,agg7#11])
+- *(8) HashAggregate(keys=[i_item_id#4, ca_country#1, ca_state#2, ca_county#3, spark_grouping_id#12], functions=[avg(cast(cs_quantity#13 as decimal(12,2))), avg(cast(cs_list_price#14 as decimal(12,2))), avg(cast(cs_coupon_amt#15 as decimal(12,2))), avg(cast(cs_sales_price#16 as decimal(12,2))), avg(cast(cs_net_profit#17 as decimal(12,2))), avg(cast(c_birth_year#18 as decimal(12,2))), avg(cast(cd_dep_count#19 as decimal(12,2)))])
   +- Exchange hashpartitioning(i_item_id#4, ca_country#1, ca_state#2, ca_county#3, spark_grouping_id#12, 200)
      +- *(7) HashAggregate(keys=[i_item_id#4, ca_country#1, ca_state#2, ca_county#3, spark_grouping_id#12], functions=[partial_avg(cast(cs_quantity#13 as decimal(12,2))), partial_avg(cast(cs_list_price#14 as decimal(12,2))), partial_avg(cast(cs_coupon_amt#15 as decimal(12,2))), partial_avg(cast(cs_sales_price#16 as decimal(12,2))), partial_avg(cast(cs_net_profit#17 as decimal(12,2))), partial_avg(cast(c_birth_year#18 as decimal(12,2))), partial_avg(cast(cd_dep_count#19 as decimal(12,2)))])
         +- *(7) Expand [List(cs_quantity#13, cs_list_price#14, cs_sales_price#16, cs_coupon_amt#15, cs_net_profit#17, cd_dep_count#19, c_birth_year#18, i_item_id#20, ca_country#21, ca_state#22, ca_county#23, 0), List(cs_quantity#13, cs_list_price#14, cs_sales_price#16, cs_coupon_amt#15, cs_net_profit#17, cd_dep_count#19, c_birth_year#18, i_item_id#20, ca_country#21, ca_state#22, null, 1), List(cs_quantity#13, cs_list_price#14, cs_sales_price#16, cs_coupon_amt#15, cs_net_profit#17, cd_dep_count#19, c_birth_year#18, i_item_id#20, ca_country#21, null, null, 3), List(cs_quantity#13, cs_list_price#14, cs_sales_price#16, cs_coupon_amt#15, cs_net_profit#17, cd_dep_count#19, c_birth_year#18, i_item_id#20, null, null, null, 7), List(cs_quantity#13, cs_list_price#14, cs_sales_price#16, cs_coupon_amt#15, cs_net_profit#17, cd_dep_count#19, c_birth_year#18, null, null, null, null, 15)], [cs_quantity#13, cs_list_price#14, cs_sales_price#16, cs_coupon_amt#15, cs_net_profit#17, cd_dep_count#19, c_birth_year#18, i_item_id#4, ca_country#1, ca_state#2, ca_county#3, spark_grouping_id#12]
            +- *(7) Project [cs_quantity#13, cs_list_price#14, cs_sales_price#16, cs_coupon_amt#15, cs_net_profit#17, cd_dep_count#19, c_birth_year#18, i_item_id#24 AS i_item_id#20, ca_country#25 AS ca_country#21, ca_state#26 AS ca_state#22, ca_county#27 AS ca_county#23]
               +- *(7) BroadcastHashJoin [cs_item_sk#28], [i_item_sk#29], Inner, BuildRight
                  :- *(7) Project [cs_item_sk#28, cs_quantity#13, cs_list_price#14, cs_sales_price#16, cs_coupon_amt#15, cs_net_profit#17, cd_dep_count#19, c_birth_year#18, ca_county#27, ca_state#26, ca_country#25]
                  :  +- *(7) BroadcastHashJoin [cs_sold_date_sk#30], [d_date_sk#31], Inner, BuildRight
                  :     :- *(7) Project [cs_sold_date_sk#30, cs_item_sk#28, cs_quantity#13, cs_list_price#14, cs_sales_price#16, cs_coupon_amt#15, cs_net_profit#17, cd_dep_count#19, c_birth_year#18, ca_county#27, ca_state#26, ca_country#25]
                  :     :  +- *(7) BroadcastHashJoin [c_current_addr_sk#32], [ca_address_sk#33], Inner, BuildRight
                  :     :     :- *(7) Project [cs_sold_date_sk#30, cs_item_sk#28, cs_quantity#13, cs_list_price#14, cs_sales_price#16, cs_coupon_amt#15, cs_net_profit#17, cd_dep_count#19, c_current_addr_sk#32, c_birth_year#18]
                  :     :     :  +- *(7) BroadcastHashJoin [c_current_cdemo_sk#34], [cd_demo_sk#35], Inner, BuildRight
                  :     :     :     :- *(7) Project [cs_sold_date_sk#30, cs_item_sk#28, cs_quantity#13, cs_list_price#14, cs_sales_price#16, cs_coupon_amt#15, cs_net_profit#17, cd_dep_count#19, c_current_cdemo_sk#34, c_current_addr_sk#32, c_birth_year#18]
                  :     :     :     :  +- *(7) BroadcastHashJoin [cs_bill_customer_sk#36], [c_customer_sk#37], Inner, BuildRight
                  :     :     :     :     :- *(7) Project [cs_sold_date_sk#30, cs_bill_customer_sk#36, cs_item_sk#28, cs_quantity#13, cs_list_price#14, cs_sales_price#16, cs_coupon_amt#15, cs_net_profit#17, cd_dep_count#19]
                  :     :     :     :     :  +- *(7) BroadcastHashJoin [cs_bill_cdemo_sk#38], [cd_demo_sk#39], Inner, BuildRight
                  :     :     :     :     :     :- *(7) Project [cs_sold_date_sk#30, cs_bill_customer_sk#36, cs_bill_cdemo_sk#38, cs_item_sk#28, cs_quantity#13, cs_list_price#14, cs_sales_price#16, cs_coupon_amt#15, cs_net_profit#17]
                  :     :     :     :     :     :  +- *(7) Filter (((isnotnull(cs_bill_cdemo_sk#38) && isnotnull(cs_bill_customer_sk#36)) && isnotnull(cs_sold_date_sk#30)) && isnotnull(cs_item_sk#28))
                  :     :     :     :     :     :     +- *(7) FileScan parquet default.catalog_sales[cs_sold_date_sk#30,cs_bill_customer_sk#36,cs_bill_cdemo_sk#38,cs_item_sk#28,cs_quantity#13,cs_list_price#14,cs_sales_price#16,cs_coupon_amt#15,cs_net_profit#17] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/catalog_sales], PartitionFilters: [], PushedFilters: [IsNotNull(cs_bill_cdemo_sk), IsNotNull(cs_bill_customer_sk), IsNotNull(cs_sold_date_sk), IsNotNu..., ReadSchema: struct<cs_sold_date_sk:int,cs_bill_customer_sk:int,cs_bill_cdemo_sk:int,cs_item_sk:int,cs_quantit...
                  :     :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                  :     :     :     :     :        +- *(1) Project [cd_demo_sk#39, cd_dep_count#19]
                  :     :     :     :     :           +- *(1) Filter ((((isnotnull(cd_gender#40) && isnotnull(cd_education_status#41)) && (cd_gender#40 = F)) && (cd_education_status#41 = Unknown)) && isnotnull(cd_demo_sk#39))
                  :     :     :     :     :              +- *(1) FileScan parquet default.customer_demographics[cd_demo_sk#39,cd_gender#40,cd_education_status#41,cd_dep_count#19] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/customer_demographics], PartitionFilters: [], PushedFilters: [IsNotNull(cd_gender), IsNotNull(cd_education_status), EqualTo(cd_gender,F), EqualTo(cd_education..., ReadSchema: struct<cd_demo_sk:int,cd_gender:string,cd_education_status:string,cd_dep_count:int>
                  :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                  :     :     :     :        +- *(2) Project [c_customer_sk#37, c_current_cdemo_sk#34, c_current_addr_sk#32, c_birth_year#18]
                  :     :     :     :           +- *(2) Filter (((c_birth_month#42 IN (1,6,8,9,12,2) && isnotnull(c_customer_sk#37)) && isnotnull(c_current_cdemo_sk#34)) && isnotnull(c_current_addr_sk#32))
                  :     :     :     :              +- *(2) FileScan parquet default.customer[c_customer_sk#37,c_current_cdemo_sk#34,c_current_addr_sk#32,c_birth_month#42,c_birth_year#18] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/customer], PartitionFilters: [], PushedFilters: [In(c_birth_month, [1,6,8,9,12,2]), IsNotNull(c_customer_sk), IsNotNull(c_current_cdemo_sk), IsNo..., ReadSchema: struct<c_customer_sk:int,c_current_cdemo_sk:int,c_current_addr_sk:int,c_birth_month:int,c_birth_y...
                  :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                  :     :     :        +- *(3) Project [cd_demo_sk#35]
                  :     :     :           +- *(3) Filter isnotnull(cd_demo_sk#35)
                  :     :     :              +- *(3) FileScan parquet default.customer_demographics[cd_demo_sk#35] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/customer_demographics], PartitionFilters: [], PushedFilters: [IsNotNull(cd_demo_sk)], ReadSchema: struct<cd_demo_sk:int>
                  :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                  :     :        +- *(4) Project [ca_address_sk#33, ca_county#27, ca_state#26, ca_country#25]
                  :     :           +- *(4) Filter (ca_state#26 IN (MS,IN,ND,OK,NM,VA) && isnotnull(ca_address_sk#33))
                  :     :              +- *(4) FileScan parquet default.customer_address[ca_address_sk#33,ca_county#27,ca_state#26,ca_country#25] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/customer_address], PartitionFilters: [], PushedFilters: [In(ca_state, [MS,IN,ND,OK,NM,VA]), IsNotNull(ca_address_sk)], ReadSchema: struct<ca_address_sk:int,ca_county:string,ca_state:string,ca_country:string>
                  :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                  :        +- *(5) Project [d_date_sk#31]
                  :           +- *(5) Filter ((isnotnull(d_year#43) && (d_year#43 = 1998)) && isnotnull(d_date_sk#31))
                  :              +- *(5) FileScan parquet default.date_dim[d_date_sk#31,d_year#43] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,1998), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_year:int>
                  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                     +- *(6) Project [i_item_sk#29, i_item_id#24]
                        +- *(6) Filter isnotnull(i_item_sk#29)
                           +- *(6) FileScan parquet default.item[i_item_sk#29,i_item_id#24] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_item_sk)], ReadSchema: struct<i_item_sk:int,i_item_id:string>