== Physical Plan ==
TakeOrderedAndProject(limit=100, orderBy=[sum(ss_ext_sales_price)#1 DESC NULLS LAST,d_year#2 ASC NULLS FIRST,i_category_id#3 ASC NULLS FIRST,i_category#4 ASC NULLS FIRST], output=[d_year#2,i_category_id#3,i_category#4,sum(ss_ext_sales_price)#1])
+- *(4) HashAggregate(keys=[d_year#2, i_category_id#3, i_category#4], functions=[sum(UnscaledValue(ss_ext_sales_price#5))])
   +- Exchange hashpartitioning(d_year#2, i_category_id#3, i_category#4, 200)
      +- *(3) HashAggregate(keys=[d_year#2, i_category_id#3, i_category#4], functions=[partial_sum(UnscaledValue(ss_ext_sales_price#5))])
         +- *(3) Project [d_year#2, ss_ext_sales_price#5, i_category_id#3, i_category#4]
            +- *(3) BroadcastHashJoin [ss_item_sk#6], [i_item_sk#7], Inner, BuildRight
               :- *(3) Project [d_year#2, ss_item_sk#6, ss_ext_sales_price#5]
               :  +- *(3) BroadcastHashJoin [d_date_sk#8], [ss_sold_date_sk#9], Inner, BuildRight
               :     :- *(3) Project [d_date_sk#8, d_year#2]
               :     :  +- *(3) Filter ((((isnotnull(d_moy#10) && isnotnull(d_year#2)) && (d_moy#10 = 11)) && (d_year#2 = 2000)) && isnotnull(d_date_sk#8))
               :     :     +- *(3) FileScan parquet default.date_dim[d_date_sk#8,d_year#2,d_moy#10] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_moy), IsNotNull(d_year), EqualTo(d_moy,11), EqualTo(d_year,2000), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_year:int,d_moy:int>
               :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :        +- *(1) Project [ss_sold_date_sk#9, ss_item_sk#6, ss_ext_sales_price#5]
               :           +- *(1) Filter (isnotnull(ss_sold_date_sk#9) && isnotnull(ss_item_sk#6))
               :              +- *(1) FileScan parquet default.store_sales[ss_sold_date_sk#9,ss_item_sk#6,ss_ext_sales_price#5] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_sold_date_sk), IsNotNull(ss_item_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_item_sk:int,ss_ext_sales_price:decimal(7,2)>
               +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                  +- *(2) Project [i_item_sk#7, i_category_id#3, i_category#4]
                     +- *(2) Filter ((isnotnull(i_manager_id#11) && (i_manager_id#11 = 1)) && isnotnull(i_item_sk#7))
                        +- *(2) FileScan parquet default.item[i_item_sk#7,i_category_id#3,i_category#4,i_manager_id#11] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_manager_id), EqualTo(i_manager_id,1), IsNotNull(i_item_sk)], ReadSchema: struct<i_item_sk:int,i_category_id:int,i_category:string,i_manager_id:int>