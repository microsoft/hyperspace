== Physical Plan ==
TakeOrderedAndProject(limit=100, orderBy=[ext_price#1 DESC NULLS LAST,brand#2 ASC NULLS FIRST,brand_id#3 ASC NULLS FIRST,i_manufact_id#4 ASC NULLS FIRST,i_manufact#5 ASC NULLS FIRST], output=[brand_id#3,brand#2,i_manufact_id#4,i_manufact#5,ext_price#1])
+- *(7) HashAggregate(keys=[i_brand#6, i_brand_id#7, i_manufact_id#4, i_manufact#5], functions=[sum(UnscaledValue(ss_ext_sales_price#8))])
   +- Exchange hashpartitioning(i_brand#6, i_brand_id#7, i_manufact_id#4, i_manufact#5, 5)
      +- *(6) HashAggregate(keys=[i_brand#6, i_brand_id#7, i_manufact_id#4, i_manufact#5], functions=[partial_sum(UnscaledValue(ss_ext_sales_price#8))])
         +- *(6) Project [ss_ext_sales_price#8, i_brand_id#7, i_brand#6, i_manufact_id#4, i_manufact#5]
            +- *(6) BroadcastHashJoin [ss_store_sk#9], [s_store_sk#10], Inner, BuildRight, NOT (substring(ca_zip#11, 1, 5) = substring(s_zip#12, 1, 5))
               :- *(6) Project [ss_store_sk#9, ss_ext_sales_price#8, i_brand_id#7, i_brand#6, i_manufact_id#4, i_manufact#5, ca_zip#11]
               :  +- *(6) BroadcastHashJoin [c_current_addr_sk#13], [ca_address_sk#14], Inner, BuildRight
               :     :- *(6) Project [ss_store_sk#9, ss_ext_sales_price#8, i_brand_id#7, i_brand#6, i_manufact_id#4, i_manufact#5, c_current_addr_sk#13]
               :     :  +- *(6) BroadcastHashJoin [ss_customer_sk#15], [c_customer_sk#16], Inner, BuildRight
               :     :     :- *(6) Project [ss_customer_sk#15, ss_store_sk#9, ss_ext_sales_price#8, i_brand_id#7, i_brand#6, i_manufact_id#4, i_manufact#5]
               :     :     :  +- *(6) BroadcastHashJoin [ss_item_sk#17], [i_item_sk#18], Inner, BuildRight
               :     :     :     :- *(6) Project [ss_item_sk#17, ss_customer_sk#15, ss_store_sk#9, ss_ext_sales_price#8]
               :     :     :     :  +- *(6) BroadcastHashJoin [d_date_sk#19], [ss_sold_date_sk#20], Inner, BuildRight
               :     :     :     :     :- *(6) Project [d_date_sk#19]
               :     :     :     :     :  +- *(6) Filter ((((isnotnull(d_moy#21) && isnotnull(d_year#22)) && (d_moy#21 = 11)) && (d_year#22 = 1998)) && isnotnull(d_date_sk#19))
               :     :     :     :     :     +- *(6) FileScan parquet default.date_dim[d_date_sk#19,d_year#22,d_moy#21] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_moy), IsNotNull(d_year), EqualTo(d_moy,11), EqualTo(d_year,1998), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_year:int,d_moy:int>
               :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :     :     :     :        +- *(1) Project [ss_sold_date_sk#20, ss_item_sk#17, ss_customer_sk#15, ss_store_sk#9, ss_ext_sales_price#8]
               :     :     :     :           +- *(1) Filter (((isnotnull(ss_sold_date_sk#20) && isnotnull(ss_item_sk#17)) && isnotnull(ss_customer_sk#15)) && isnotnull(ss_store_sk#9))
               :     :     :     :              +- *(1) FileScan parquet default.store_sales[ss_sold_date_sk#20,ss_item_sk#17,ss_customer_sk#15,ss_store_sk#9,ss_ext_sales_price#8] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_sold_date_sk), IsNotNull(ss_item_sk), IsNotNull(ss_customer_sk), IsNotNull(ss_store..., ReadSchema: struct<ss_sold_date_sk:int,ss_item_sk:int,ss_customer_sk:int,ss_store_sk:int,ss_ext_sales_price:d...
               :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :     :     :        +- *(2) Project [i_item_sk#18, i_brand_id#7, i_brand#6, i_manufact_id#4, i_manufact#5]
               :     :     :           +- *(2) Filter ((isnotnull(i_manager_id#23) && (i_manager_id#23 = 8)) && isnotnull(i_item_sk#18))
               :     :     :              +- *(2) FileScan parquet default.item[i_item_sk#18,i_brand_id#7,i_brand#6,i_manufact_id#4,i_manufact#5,i_manager_id#23] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_manager_id), EqualTo(i_manager_id,8), IsNotNull(i_item_sk)], ReadSchema: struct<i_item_sk:int,i_brand_id:int,i_brand:string,i_manufact_id:int,i_manufact:string,i_manager_...
               :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :     :        +- *(3) Project [c_customer_sk#16, c_current_addr_sk#13]
               :     :           +- *(3) Filter (isnotnull(c_customer_sk#16) && isnotnull(c_current_addr_sk#13))
               :     :              +- *(3) FileScan parquet default.customer[c_customer_sk#16,c_current_addr_sk#13] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/customer], PartitionFilters: [], PushedFilters: [IsNotNull(c_customer_sk), IsNotNull(c_current_addr_sk)], ReadSchema: struct<c_customer_sk:int,c_current_addr_sk:int>
               :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :        +- *(4) Project [ca_address_sk#14, ca_zip#11]
               :           +- *(4) Filter (isnotnull(ca_address_sk#14) && isnotnull(ca_zip#11))
               :              +- *(4) FileScan parquet default.customer_address[ca_address_sk#14,ca_zip#11] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/customer_address], PartitionFilters: [], PushedFilters: [IsNotNull(ca_address_sk), IsNotNull(ca_zip)], ReadSchema: struct<ca_address_sk:int,ca_zip:string>
               +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                  +- *(5) Project [s_store_sk#10, s_zip#12]
                     +- *(5) Filter (isnotnull(s_zip#12) && isnotnull(s_store_sk#10))
                        +- *(5) FileScan parquet default.store[s_store_sk#10,s_zip#12] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store], PartitionFilters: [], PushedFilters: [IsNotNull(s_zip), IsNotNull(s_store_sk)], ReadSchema: struct<s_store_sk:int,s_zip:string>