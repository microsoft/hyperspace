== Physical Plan ==
TakeOrderedAndProject(limit=100, orderBy=[Excess Discount Amount #1 ASC NULLS FIRST], output=[Excess Discount Amount #1])
+- *(7) HashAggregate(keys=[], functions=[sum(UnscaledValue(ws_ext_discount_amt#2))])
   +- Exchange SinglePartition
      +- *(6) HashAggregate(keys=[], functions=[partial_sum(UnscaledValue(ws_ext_discount_amt#2))])
         +- *(6) Project [ws_ext_discount_amt#2]
            +- *(6) BroadcastHashJoin [ws_sold_date_sk#3], [d_date_sk#4], Inner, BuildRight
               :- *(6) Project [ws_sold_date_sk#3, ws_ext_discount_amt#2]
               :  +- *(6) BroadcastHashJoin [i_item_sk#5], [ws_item_sk#6#7], Inner, BuildRight, (cast(ws_ext_discount_amt#2 as decimal(14,7)) > (CAST(1.3 AS DECIMAL(11,6)) * CAST(avg(ws_ext_discount_amt) AS DECIMAL(11,6)))#8)
               :     :- *(6) Project [ws_sold_date_sk#3, ws_ext_discount_amt#2, i_item_sk#5]
               :     :  +- *(6) BroadcastHashJoin [ws_item_sk#6], [i_item_sk#5], Inner, BuildRight
               :     :     :- *(6) Project [ws_sold_date_sk#3, ws_item_sk#6, ws_ext_discount_amt#2]
               :     :     :  +- *(6) Filter ((isnotnull(ws_item_sk#6) && isnotnull(ws_ext_discount_amt#2)) && isnotnull(ws_sold_date_sk#3))
               :     :     :     +- *(6) FileScan parquet default.web_sales[ws_sold_date_sk#3,ws_item_sk#6,ws_ext_discount_amt#2] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/web_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ws_item_sk), IsNotNull(ws_ext_discount_amt), IsNotNull(ws_sold_date_sk)], ReadSchema: struct<ws_sold_date_sk:int,ws_item_sk:int,ws_ext_discount_amt:decimal(7,2)>
               :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :     :        +- *(1) Project [i_item_sk#5]
               :     :           +- *(1) Filter ((isnotnull(i_manufact_id#9) && (i_manufact_id#9 = 350)) && isnotnull(i_item_sk#5))
               :     :              +- *(1) FileScan parquet default.item[i_item_sk#5,i_manufact_id#9] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_manufact_id), EqualTo(i_manufact_id,350), IsNotNull(i_item_sk)], ReadSchema: struct<i_item_sk:int,i_manufact_id:int>
               :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[1, int, true] as bigint)))
               :        +- *(4) Filter isnotnull((CAST(1.3 AS DECIMAL(11,6)) * CAST(avg(ws_ext_discount_amt) AS DECIMAL(11,6)))#8)
               :           +- *(4) HashAggregate(keys=[ws_item_sk#6], functions=[avg(UnscaledValue(ws_ext_discount_amt#2))])
               :              +- Exchange hashpartitioning(ws_item_sk#6, 5)
               :                 +- *(3) HashAggregate(keys=[ws_item_sk#6], functions=[partial_avg(UnscaledValue(ws_ext_discount_amt#2))])
               :                    +- *(3) Project [ws_item_sk#6, ws_ext_discount_amt#2]
               :                       +- *(3) BroadcastHashJoin [ws_sold_date_sk#3], [d_date_sk#4], Inner, BuildRight
               :                          :- *(3) Project [ws_sold_date_sk#3, ws_item_sk#6, ws_ext_discount_amt#2]
               :                          :  +- *(3) Filter (isnotnull(ws_sold_date_sk#3) && isnotnull(ws_item_sk#6))
               :                          :     +- *(3) FileScan parquet default.web_sales[ws_sold_date_sk#3,ws_item_sk#6,ws_ext_discount_amt#2] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/web_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ws_sold_date_sk), IsNotNull(ws_item_sk)], ReadSchema: struct<ws_sold_date_sk:int,ws_item_sk:int,ws_ext_discount_amt:decimal(7,2)>
               :                          +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :                             +- *(2) Project [d_date_sk#4]
               :                                +- *(2) Filter (((isnotnull(d_date#10) && (cast(d_date#10 as string) >= 2000-01-27)) && (d_date#10 <= 11073)) && isnotnull(d_date_sk#4))
               :                                   +- *(2) FileScan parquet default.date_dim[d_date_sk#4,d_date#10] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_date), LessThanOrEqual(d_date,2000-04-26), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_date:date>
               +- ReusedExchange [d_date_sk#4], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))