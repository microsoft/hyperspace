== Physical Plan ==
TakeOrderedAndProject(limit=100, orderBy=[lochierarchy#1 DESC NULLS LAST,CASE WHEN (cast(lochierarchy#1 as int) = 0) THEN s_state#2 END ASC NULLS FIRST,rank_within_parent#3 ASC NULLS FIRST], output=[total_sum#4,s_state#2,s_county#5,lochierarchy#1,rank_within_parent#3])
+- *(11) Project [total_sum#4, s_state#2, s_county#5, lochierarchy#1, rank_within_parent#3]
   +- Window [rank(_w3#6) windowspecdefinition(_w1#7, _w2#8, _w3#6 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank_within_parent#3], [_w1#7, _w2#8], [_w3#6 DESC NULLS LAST]
      +- *(10) Sort [_w1#7 ASC NULLS FIRST, _w2#8 ASC NULLS FIRST, _w3#6 DESC NULLS LAST], false, 0
         +- Exchange hashpartitioning(_w1#7, _w2#8, 200)
            +- *(9) HashAggregate(keys=[s_state#2, s_county#5, spark_grouping_id#9], functions=[sum(UnscaledValue(ss_net_profit#10))])
               +- Exchange hashpartitioning(s_state#2, s_county#5, spark_grouping_id#9, 200)
                  +- *(8) HashAggregate(keys=[s_state#2, s_county#5, spark_grouping_id#9], functions=[partial_sum(UnscaledValue(ss_net_profit#10))])
                     +- *(8) Expand [List(ss_net_profit#10, s_state#11, s_county#12, 0), List(ss_net_profit#10, s_state#11, null, 1), List(ss_net_profit#10, null, null, 3)], [ss_net_profit#10, s_state#2, s_county#5, spark_grouping_id#9]
                        +- *(8) Project [ss_net_profit#10, s_state#13 AS s_state#11, s_county#14 AS s_county#12]
                           +- *(8) BroadcastHashJoin [ss_store_sk#15], [s_store_sk#16], Inner, BuildRight
                              :- *(8) Project [ss_store_sk#15, ss_net_profit#10]
                              :  +- *(8) BroadcastHashJoin [ss_sold_date_sk#17], [d_date_sk#18], Inner, BuildRight
                              :     :- *(8) Project [ss_sold_date_sk#17, ss_store_sk#15, ss_net_profit#10]
                              :     :  +- *(8) Filter (isnotnull(ss_sold_date_sk#17) && isnotnull(ss_store_sk#15))
                              :     :     +- *(8) FileScan parquet default.store_sales[ss_sold_date_sk#17,ss_store_sk#15,ss_net_profit#10] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_sold_date_sk), IsNotNull(ss_store_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_store_sk:int,ss_net_profit:decimal(7,2)>
                              :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                              :        +- *(1) Project [d_date_sk#18]
                              :           +- *(1) Filter (((isnotnull(d_month_seq#19) && (d_month_seq#19 >= 1200)) && (d_month_seq#19 <= 1211)) && isnotnull(d_date_sk#18))
                              :              +- *(1) FileScan parquet default.date_dim[d_date_sk#18,d_month_seq#19] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_month_seq), GreaterThanOrEqual(d_month_seq,1200), LessThanOrEqual(d_month_seq,1211),..., ReadSchema: struct<d_date_sk:int,d_month_seq:int>
                              +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                                 +- *(7) BroadcastHashJoin [s_state#13], [s_state#20], LeftSemi, BuildRight
                                    :- *(7) Project [s_store_sk#16, s_county#14, s_state#13]
                                    :  +- *(7) Filter isnotnull(s_store_sk#16)
                                    :     +- *(7) FileScan parquet default.store[s_store_sk#16,s_county#14,s_state#13] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store], PartitionFilters: [], PushedFilters: [IsNotNull(s_store_sk)], ReadSchema: struct<s_store_sk:int,s_county:string,s_state:string>
                                    +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))
                                       +- *(6) Project [s_state#20]
                                          +- *(6) Filter (isnotnull(ranking#21) && (ranking#21 <= 5))
                                             +- Window [rank(_w2#22) windowspecdefinition(s_state#13, _w2#22 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS ranking#21], [s_state#13], [_w2#22 DESC NULLS LAST]
                                                +- *(5) Sort [s_state#13 ASC NULLS FIRST, _w2#22 DESC NULLS LAST], false, 0
                                                   +- *(5) HashAggregate(keys=[s_state#13], functions=[sum(UnscaledValue(ss_net_profit#10))])
                                                      +- Exchange hashpartitioning(s_state#13, 200)
                                                         +- *(4) HashAggregate(keys=[s_state#13], functions=[partial_sum(UnscaledValue(ss_net_profit#10))])
                                                            +- *(4) Project [ss_net_profit#10, s_state#13]
                                                               +- *(4) BroadcastHashJoin [ss_sold_date_sk#17], [d_date_sk#18], Inner, BuildRight
                                                                  :- *(4) Project [ss_sold_date_sk#17, ss_net_profit#10, s_state#13]
                                                                  :  +- *(4) BroadcastHashJoin [ss_store_sk#15], [s_store_sk#16], Inner, BuildRight
                                                                  :     :- *(4) Project [ss_sold_date_sk#17, ss_store_sk#15, ss_net_profit#10]
                                                                  :     :  +- *(4) Filter (isnotnull(ss_store_sk#15) && isnotnull(ss_sold_date_sk#17))
                                                                  :     :     +- *(4) FileScan parquet default.store_sales[ss_sold_date_sk#17,ss_store_sk#15,ss_net_profit#10] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_store_sk), IsNotNull(ss_sold_date_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_store_sk:int,ss_net_profit:decimal(7,2)>
                                                                  :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                                                                  :        +- *(2) Project [s_store_sk#16, s_state#13]
                                                                  :           +- *(2) Filter isnotnull(s_store_sk#16)
                                                                  :              +- *(2) FileScan parquet default.store[s_store_sk#16,s_state#13] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store], PartitionFilters: [], PushedFilters: [IsNotNull(s_store_sk)], ReadSchema: struct<s_store_sk:int,s_state:string>
                                                                  +- ReusedExchange [d_date_sk#18], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))