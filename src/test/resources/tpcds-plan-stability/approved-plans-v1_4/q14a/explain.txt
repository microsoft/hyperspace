== Physical Plan ==
TakeOrderedAndProject(limit=100, orderBy=[channel#1 ASC NULLS FIRST,i_brand_id#2 ASC NULLS FIRST,i_class_id#3 ASC NULLS FIRST,i_category_id#4 ASC NULLS FIRST], output=[channel#1,i_brand_id#2,i_class_id#3,i_category_id#4,sum(sales)#5,sum(number_sales)#6])
+- *(80) HashAggregate(keys=[channel#1, i_brand_id#2, i_class_id#3, i_category_id#4, spark_grouping_id#7], functions=[sum(sales#8), sum(number_sales#9)])
   +- Exchange hashpartitioning(channel#1, i_brand_id#2, i_class_id#3, i_category_id#4, spark_grouping_id#7, 200)
      +- *(79) HashAggregate(keys=[channel#1, i_brand_id#2, i_class_id#3, i_category_id#4, spark_grouping_id#7], functions=[partial_sum(sales#8), partial_sum(number_sales#9)])
         +- *(79) Expand [List(sales#8, number_sales#9, channel#10, i_brand_id#11, i_class_id#12, i_category_id#13, 0), List(sales#8, number_sales#9, channel#10, i_brand_id#11, i_class_id#12, null, 1), List(sales#8, number_sales#9, channel#10, i_brand_id#11, null, null, 3), List(sales#8, number_sales#9, channel#10, null, null, null, 7), List(sales#8, number_sales#9, null, null, null, null, 15)], [sales#8, number_sales#9, channel#1, i_brand_id#2, i_class_id#3, i_category_id#4, spark_grouping_id#7]
            +- Union
               :- *(26) Project [sales#8, number_sales#9, channel#10, i_brand_id#11, i_class_id#12, i_category_id#13]
               :  +- *(26) Filter (isnotnull(sum(CheckOverflow((promote_precision(cast(cast(ss_quantity#14 as decimal(10,0)) as decimal(12,2))) * promote_precision(cast(ss_list_price#15 as decimal(12,2)))), DecimalType(18,2)))#16) && (cast(sum(CheckOverflow((promote_precision(cast(cast(ss_quantity#14 as decimal(10,0)) as decimal(12,2))) * promote_precision(cast(ss_list_price#15 as decimal(12,2)))), DecimalType(18,2)))#16 as decimal(32,6)) > cast(Subquery subquery1662 as decimal(32,6))))
               :     :  +- Subquery subquery1662
               :     :     +- *(8) HashAggregate(keys=[], functions=[avg(CheckOverflow((promote_precision(cast(cast(quantity#17 as decimal(10,0)) as decimal(12,2))) * promote_precision(cast(list_price#18 as decimal(12,2)))), DecimalType(18,2)))])
               :     :        +- Exchange SinglePartition
               :     :           +- *(7) HashAggregate(keys=[], functions=[partial_avg(CheckOverflow((promote_precision(cast(cast(quantity#17 as decimal(10,0)) as decimal(12,2))) * promote_precision(cast(list_price#18 as decimal(12,2)))), DecimalType(18,2)))])
               :     :              +- Union
               :     :                 :- *(2) Project [ss_quantity#14 AS quantity#17, ss_list_price#15 AS list_price#18]
               :     :                 :  +- *(2) BroadcastHashJoin [ss_sold_date_sk#19], [d_date_sk#20], Inner, BuildRight
               :     :                 :     :- *(2) Project [ss_sold_date_sk#19, ss_quantity#14, ss_list_price#15]
               :     :                 :     :  +- *(2) Filter isnotnull(ss_sold_date_sk#19)
               :     :                 :     :     +- *(2) FileScan parquet default.store_sales[ss_sold_date_sk#19,ss_quantity#14,ss_list_price#15] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_sold_date_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_quantity:int,ss_list_price:decimal(7,2)>
               :     :                 :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :     :                 :        +- *(1) Project [d_date_sk#20]
               :     :                 :           +- *(1) Filter (((isnotnull(d_year#21) && (d_year#21 >= 1999)) && (d_year#21 <= 2001)) && isnotnull(d_date_sk#20))
               :     :                 :              +- *(1) FileScan parquet default.date_dim[d_date_sk#20,d_year#21] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), GreaterThanOrEqual(d_year,1999), LessThanOrEqual(d_year,2001), IsNotNull(d_da..., ReadSchema: struct<d_date_sk:int,d_year:int>
               :     :                 :- *(4) Project [cs_quantity#22 AS quantity#23, cs_list_price#24 AS list_price#25]
               :     :                 :  +- *(4) BroadcastHashJoin [cs_sold_date_sk#26], [d_date_sk#20], Inner, BuildRight
               :     :                 :     :- *(4) Project [cs_sold_date_sk#26, cs_quantity#22, cs_list_price#24]
               :     :                 :     :  +- *(4) Filter isnotnull(cs_sold_date_sk#26)
               :     :                 :     :     +- *(4) FileScan parquet default.catalog_sales[cs_sold_date_sk#26,cs_quantity#22,cs_list_price#24] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/catalog_sales], PartitionFilters: [], PushedFilters: [IsNotNull(cs_sold_date_sk)], ReadSchema: struct<cs_sold_date_sk:int,cs_quantity:int,cs_list_price:decimal(7,2)>
               :     :                 :     +- ReusedExchange [d_date_sk#20], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :     :                 +- *(6) Project [ws_quantity#27 AS quantity#28, ws_list_price#29 AS list_price#30]
               :     :                    +- *(6) BroadcastHashJoin [ws_sold_date_sk#31], [d_date_sk#20], Inner, BuildRight
               :     :                       :- *(6) Project [ws_sold_date_sk#31, ws_quantity#27, ws_list_price#29]
               :     :                       :  +- *(6) Filter isnotnull(ws_sold_date_sk#31)
               :     :                       :     +- *(6) FileScan parquet default.web_sales[ws_sold_date_sk#31,ws_quantity#27,ws_list_price#29] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/web_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ws_sold_date_sk)], ReadSchema: struct<ws_sold_date_sk:int,ws_quantity:int,ws_list_price:decimal(7,2)>
               :     :                       +- ReusedExchange [d_date_sk#20], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :     +- *(26) HashAggregate(keys=[i_brand_id#11, i_class_id#12, i_category_id#13], functions=[sum(CheckOverflow((promote_precision(cast(cast(ss_quantity#14 as decimal(10,0)) as decimal(12,2))) * promote_precision(cast(ss_list_price#15 as decimal(12,2)))), DecimalType(18,2))), count(1)])
               :        +- Exchange hashpartitioning(i_brand_id#11, i_class_id#12, i_category_id#13, 200)
               :           +- *(25) HashAggregate(keys=[i_brand_id#11, i_class_id#12, i_category_id#13], functions=[partial_sum(CheckOverflow((promote_precision(cast(cast(ss_quantity#14 as decimal(10,0)) as decimal(12,2))) * promote_precision(cast(ss_list_price#15 as decimal(12,2)))), DecimalType(18,2))), partial_count(1)])
               :              +- *(25) Project [ss_quantity#14, ss_list_price#15, i_brand_id#11, i_class_id#12, i_category_id#13]
               :                 +- *(25) BroadcastHashJoin [ss_sold_date_sk#19], [d_date_sk#20], Inner, BuildRight
               :                    :- *(25) Project [ss_sold_date_sk#19, ss_quantity#14, ss_list_price#15, i_brand_id#11, i_class_id#12, i_category_id#13]
               :                    :  +- *(25) BroadcastHashJoin [ss_item_sk#32], [i_item_sk#33], Inner, BuildRight
               :                    :     :- *(25) BroadcastHashJoin [ss_item_sk#32], [ss_item_sk#34], LeftSemi, BuildRight
               :                    :     :  :- *(25) Project [ss_sold_date_sk#19, ss_item_sk#32, ss_quantity#14, ss_list_price#15]
               :                    :     :  :  +- *(25) Filter (isnotnull(ss_item_sk#32) && isnotnull(ss_sold_date_sk#19))
               :                    :     :  :     +- *(25) FileScan parquet default.store_sales[ss_sold_date_sk#19,ss_item_sk#32,ss_quantity#14,ss_list_price#15] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_item_sk), IsNotNull(ss_sold_date_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_item_sk:int,ss_quantity:int,ss_list_price:decimal(7,2)>
               :                    :     :  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :                    :     :     +- *(11) Project [i_item_sk#33 AS ss_item_sk#34]
               :                    :     :        +- *(11) BroadcastHashJoin [i_brand_id#11, i_class_id#12, i_category_id#13], [brand_id#35, class_id#36, category_id#37], Inner, BuildRight
               :                    :     :           :- *(11) Project [i_item_sk#33, i_brand_id#11, i_class_id#12, i_category_id#13]
               :                    :     :           :  +- *(11) Filter ((isnotnull(i_class_id#12) && isnotnull(i_brand_id#11)) && isnotnull(i_category_id#13))
               :                    :     :           :     +- *(11) FileScan parquet default.item[i_item_sk#33,i_brand_id#11,i_class_id#12,i_category_id#13] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_class_id), IsNotNull(i_brand_id), IsNotNull(i_category_id)], ReadSchema: struct<i_item_sk:int,i_brand_id:int,i_class_id:int,i_category_id:int>
               :                    :     :           +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, int, true], input[1, int, true], input[2, int, true]))
               :                    :     :              +- *(10) HashAggregate(keys=[brand_id#35, class_id#36, category_id#37], functions=[])
               :                    :     :                 +- *(10) HashAggregate(keys=[brand_id#35, class_id#36, category_id#37], functions=[])
               :                    :     :                    +- *(10) BroadcastHashJoin [coalesce(brand_id#35, 0), coalesce(class_id#36, 0), coalesce(category_id#37, 0)], [coalesce(i_brand_id#11, 0), coalesce(i_class_id#12, 0), coalesce(i_category_id#13, 0)], LeftSemi, BuildRight, (((brand_id#35 <=> i_brand_id#11) && (class_id#36 <=> i_class_id#12)) && (category_id#37 <=> i_category_id#13))
               :                    :     :                       :- *(10) HashAggregate(keys=[brand_id#35, class_id#36, category_id#37], functions=[])
               :                    :     :                       :  +- Exchange hashpartitioning(brand_id#35, class_id#36, category_id#37, 200)
               :                    :     :                       :     +- *(6) HashAggregate(keys=[brand_id#35, class_id#36, category_id#37], functions=[])
               :                    :     :                       :        +- *(6) BroadcastHashJoin [coalesce(brand_id#35, 0), coalesce(class_id#36, 0), coalesce(category_id#37, 0)], [coalesce(i_brand_id#11, 0), coalesce(i_class_id#12, 0), coalesce(i_category_id#13, 0)], LeftSemi, BuildRight, (((brand_id#35 <=> i_brand_id#11) && (class_id#36 <=> i_class_id#12)) && (category_id#37 <=> i_category_id#13))
               :                    :     :                       :           :- *(6) Project [i_brand_id#11 AS brand_id#35, i_class_id#12 AS class_id#36, i_category_id#13 AS category_id#37]
               :                    :     :                       :           :  +- *(6) BroadcastHashJoin [ss_sold_date_sk#19], [d_date_sk#20], Inner, BuildRight
               :                    :     :                       :           :     :- *(6) Project [ss_sold_date_sk#19, i_brand_id#11, i_class_id#12, i_category_id#13]
               :                    :     :                       :           :     :  +- *(6) BroadcastHashJoin [ss_item_sk#32], [i_item_sk#33], Inner, BuildRight
               :                    :     :                       :           :     :     :- *(6) Project [ss_sold_date_sk#19, ss_item_sk#32]
               :                    :     :                       :           :     :     :  +- *(6) Filter (isnotnull(ss_item_sk#32) && isnotnull(ss_sold_date_sk#19))
               :                    :     :                       :           :     :     :     +- *(6) FileScan parquet default.store_sales[ss_sold_date_sk#19,ss_item_sk#32] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_item_sk), IsNotNull(ss_sold_date_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_item_sk:int>
               :                    :     :                       :           :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :                    :     :                       :           :     :        +- *(1) Project [i_item_sk#33, i_brand_id#11, i_class_id#12, i_category_id#13]
               :                    :     :                       :           :     :           +- *(1) Filter (((isnotnull(i_item_sk#33) && isnotnull(i_class_id#12)) && isnotnull(i_brand_id#11)) && isnotnull(i_category_id#13))
               :                    :     :                       :           :     :              +- *(1) FileScan parquet default.item[i_item_sk#33,i_brand_id#11,i_class_id#12,i_category_id#13] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_item_sk), IsNotNull(i_class_id), IsNotNull(i_brand_id), IsNotNull(i_category_id)], ReadSchema: struct<i_item_sk:int,i_brand_id:int,i_class_id:int,i_category_id:int>
               :                    :     :                       :           :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :                    :     :                       :           :        +- *(2) Project [d_date_sk#20]
               :                    :     :                       :           :           +- *(2) Filter (((isnotnull(d_year#21) && (d_year#21 >= 1999)) && (d_year#21 <= 2001)) && isnotnull(d_date_sk#20))
               :                    :     :                       :           :              +- *(2) FileScan parquet default.date_dim[d_date_sk#20,d_year#21] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), GreaterThanOrEqual(d_year,1999), LessThanOrEqual(d_year,2001), IsNotNull(d_da..., ReadSchema: struct<d_date_sk:int,d_year:int>
               :                    :     :                       :           +- BroadcastExchange HashedRelationBroadcastMode(List(coalesce(input[0, int, true], 0), coalesce(input[1, int, true], 0), coalesce(input[2, int, true], 0)))
               :                    :     :                       :              +- *(5) Project [i_brand_id#11, i_class_id#12, i_category_id#13]
               :                    :     :                       :                 +- *(5) BroadcastHashJoin [cs_sold_date_sk#26], [d_date_sk#20], Inner, BuildRight
               :                    :     :                       :                    :- *(5) Project [cs_sold_date_sk#26, i_brand_id#11, i_class_id#12, i_category_id#13]
               :                    :     :                       :                    :  +- *(5) BroadcastHashJoin [cs_item_sk#38], [i_item_sk#33], Inner, BuildRight
               :                    :     :                       :                    :     :- *(5) Project [cs_sold_date_sk#26, cs_item_sk#38]
               :                    :     :                       :                    :     :  +- *(5) Filter (isnotnull(cs_item_sk#38) && isnotnull(cs_sold_date_sk#26))
               :                    :     :                       :                    :     :     +- *(5) FileScan parquet default.catalog_sales[cs_sold_date_sk#26,cs_item_sk#38] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/catalog_sales], PartitionFilters: [], PushedFilters: [IsNotNull(cs_item_sk), IsNotNull(cs_sold_date_sk)], ReadSchema: struct<cs_sold_date_sk:int,cs_item_sk:int>
               :                    :     :                       :                    :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :                    :     :                       :                    :        +- *(3) Project [i_item_sk#33, i_brand_id#11, i_class_id#12, i_category_id#13]
               :                    :     :                       :                    :           +- *(3) Filter isnotnull(i_item_sk#33)
               :                    :     :                       :                    :              +- *(3) FileScan parquet default.item[i_item_sk#33,i_brand_id#11,i_class_id#12,i_category_id#13] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_item_sk)], ReadSchema: struct<i_item_sk:int,i_brand_id:int,i_class_id:int,i_category_id:int>
               :                    :     :                       :                    +- ReusedExchange [d_date_sk#20], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :                    :     :                       +- BroadcastExchange HashedRelationBroadcastMode(List(coalesce(input[0, int, true], 0), coalesce(input[1, int, true], 0), coalesce(input[2, int, true], 0)))
               :                    :     :                          +- *(9) Project [i_brand_id#11, i_class_id#12, i_category_id#13]
               :                    :     :                             +- *(9) BroadcastHashJoin [ws_sold_date_sk#31], [d_date_sk#20], Inner, BuildRight
               :                    :     :                                :- *(9) Project [ws_sold_date_sk#31, i_brand_id#11, i_class_id#12, i_category_id#13]
               :                    :     :                                :  +- *(9) BroadcastHashJoin [ws_item_sk#39], [i_item_sk#33], Inner, BuildRight
               :                    :     :                                :     :- *(9) Project [ws_sold_date_sk#31, ws_item_sk#39]
               :                    :     :                                :     :  +- *(9) Filter (isnotnull(ws_item_sk#39) && isnotnull(ws_sold_date_sk#31))
               :                    :     :                                :     :     +- *(9) FileScan parquet default.web_sales[ws_sold_date_sk#31,ws_item_sk#39] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/web_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ws_item_sk), IsNotNull(ws_sold_date_sk)], ReadSchema: struct<ws_sold_date_sk:int,ws_item_sk:int>
               :                    :     :                                :     +- ReusedExchange [i_item_sk#33, i_brand_id#11, i_class_id#12, i_category_id#13], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :                    :     :                                +- ReusedExchange [d_date_sk#20], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :                    :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :                    :        +- *(23) BroadcastHashJoin [i_item_sk#33], [ss_item_sk#34], LeftSemi, BuildRight
               :                    :           :- *(23) Project [i_item_sk#33, i_brand_id#11, i_class_id#12, i_category_id#13]
               :                    :           :  +- *(23) Filter isnotnull(i_item_sk#33)
               :                    :           :     +- *(23) FileScan parquet default.item[i_item_sk#33,i_brand_id#11,i_class_id#12,i_category_id#13] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_item_sk)], ReadSchema: struct<i_item_sk:int,i_brand_id:int,i_class_id:int,i_category_id:int>
               :                    :           +- ReusedExchange [ss_item_sk#34], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :                    +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :                       +- *(24) Project [d_date_sk#20]
               :                          +- *(24) Filter ((((isnotnull(d_year#21) && isnotnull(d_moy#40)) && (d_year#21 = 2001)) && (d_moy#40 = 11)) && isnotnull(d_date_sk#20))
               :                             +- *(24) FileScan parquet default.date_dim[d_date_sk#20,d_year#21,d_moy#40] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), IsNotNull(d_moy), EqualTo(d_year,2001), EqualTo(d_moy,11), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_year:int,d_moy:int>
               :- *(52) Project [sales#41, number_sales#42, channel#43, i_brand_id#11, i_class_id#12, i_category_id#13]
               :  +- *(52) Filter (isnotnull(sum(CheckOverflow((promote_precision(cast(cast(cs_quantity#22 as decimal(10,0)) as decimal(12,2))) * promote_precision(cast(cs_list_price#24 as decimal(12,2)))), DecimalType(18,2)))#44) && (cast(sum(CheckOverflow((promote_precision(cast(cast(cs_quantity#22 as decimal(10,0)) as decimal(12,2))) * promote_precision(cast(cs_list_price#24 as decimal(12,2)))), DecimalType(18,2)))#44 as decimal(32,6)) > cast(Subquery subquery1667 as decimal(32,6))))
               :     :  +- Subquery subquery1667
               :     :     +- *(8) HashAggregate(keys=[], functions=[avg(CheckOverflow((promote_precision(cast(cast(quantity#17 as decimal(10,0)) as decimal(12,2))) * promote_precision(cast(list_price#18 as decimal(12,2)))), DecimalType(18,2)))])
               :     :        +- Exchange SinglePartition
               :     :           +- *(7) HashAggregate(keys=[], functions=[partial_avg(CheckOverflow((promote_precision(cast(cast(quantity#17 as decimal(10,0)) as decimal(12,2))) * promote_precision(cast(list_price#18 as decimal(12,2)))), DecimalType(18,2)))])
               :     :              +- Union
               :     :                 :- *(2) Project [ss_quantity#14 AS quantity#17, ss_list_price#15 AS list_price#18]
               :     :                 :  +- *(2) BroadcastHashJoin [ss_sold_date_sk#19], [d_date_sk#20], Inner, BuildRight
               :     :                 :     :- *(2) Project [ss_sold_date_sk#19, ss_quantity#14, ss_list_price#15]
               :     :                 :     :  +- *(2) Filter isnotnull(ss_sold_date_sk#19)
               :     :                 :     :     +- *(2) FileScan parquet default.store_sales[ss_sold_date_sk#19,ss_quantity#14,ss_list_price#15] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_sold_date_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_quantity:int,ss_list_price:decimal(7,2)>
               :     :                 :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :     :                 :        +- *(1) Project [d_date_sk#20]
               :     :                 :           +- *(1) Filter (((isnotnull(d_year#21) && (d_year#21 >= 1999)) && (d_year#21 <= 2001)) && isnotnull(d_date_sk#20))
               :     :                 :              +- *(1) FileScan parquet default.date_dim[d_date_sk#20,d_year#21] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), GreaterThanOrEqual(d_year,1999), LessThanOrEqual(d_year,2001), IsNotNull(d_da..., ReadSchema: struct<d_date_sk:int,d_year:int>
               :     :                 :- *(4) Project [cs_quantity#22 AS quantity#23, cs_list_price#24 AS list_price#25]
               :     :                 :  +- *(4) BroadcastHashJoin [cs_sold_date_sk#26], [d_date_sk#20], Inner, BuildRight
               :     :                 :     :- *(4) Project [cs_sold_date_sk#26, cs_quantity#22, cs_list_price#24]
               :     :                 :     :  +- *(4) Filter isnotnull(cs_sold_date_sk#26)
               :     :                 :     :     +- *(4) FileScan parquet default.catalog_sales[cs_sold_date_sk#26,cs_quantity#22,cs_list_price#24] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/catalog_sales], PartitionFilters: [], PushedFilters: [IsNotNull(cs_sold_date_sk)], ReadSchema: struct<cs_sold_date_sk:int,cs_quantity:int,cs_list_price:decimal(7,2)>
               :     :                 :     +- ReusedExchange [d_date_sk#20], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :     :                 +- *(6) Project [ws_quantity#27 AS quantity#28, ws_list_price#29 AS list_price#30]
               :     :                    +- *(6) BroadcastHashJoin [ws_sold_date_sk#31], [d_date_sk#20], Inner, BuildRight
               :     :                       :- *(6) Project [ws_sold_date_sk#31, ws_quantity#27, ws_list_price#29]
               :     :                       :  +- *(6) Filter isnotnull(ws_sold_date_sk#31)
               :     :                       :     +- *(6) FileScan parquet default.web_sales[ws_sold_date_sk#31,ws_quantity#27,ws_list_price#29] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/web_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ws_sold_date_sk)], ReadSchema: struct<ws_sold_date_sk:int,ws_quantity:int,ws_list_price:decimal(7,2)>
               :     :                       +- ReusedExchange [d_date_sk#20], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :     +- *(52) HashAggregate(keys=[i_brand_id#11, i_class_id#12, i_category_id#13], functions=[sum(CheckOverflow((promote_precision(cast(cast(cs_quantity#22 as decimal(10,0)) as decimal(12,2))) * promote_precision(cast(cs_list_price#24 as decimal(12,2)))), DecimalType(18,2))), count(1)])
               :        +- Exchange hashpartitioning(i_brand_id#11, i_class_id#12, i_category_id#13, 200)
               :           +- *(51) HashAggregate(keys=[i_brand_id#11, i_class_id#12, i_category_id#13], functions=[partial_sum(CheckOverflow((promote_precision(cast(cast(cs_quantity#22 as decimal(10,0)) as decimal(12,2))) * promote_precision(cast(cs_list_price#24 as decimal(12,2)))), DecimalType(18,2))), partial_count(1)])
               :              +- *(51) Project [cs_quantity#22, cs_list_price#24, i_brand_id#11, i_class_id#12, i_category_id#13]
               :                 +- *(51) BroadcastHashJoin [cs_sold_date_sk#26], [d_date_sk#20], Inner, BuildRight
               :                    :- *(51) Project [cs_sold_date_sk#26, cs_quantity#22, cs_list_price#24, i_brand_id#11, i_class_id#12, i_category_id#13]
               :                    :  +- *(51) BroadcastHashJoin [cs_item_sk#38], [i_item_sk#33], Inner, BuildRight
               :                    :     :- *(51) BroadcastHashJoin [cs_item_sk#38], [ss_item_sk#34], LeftSemi, BuildRight
               :                    :     :  :- *(51) Project [cs_sold_date_sk#26, cs_item_sk#38, cs_quantity#22, cs_list_price#24]
               :                    :     :  :  +- *(51) Filter (isnotnull(cs_item_sk#38) && isnotnull(cs_sold_date_sk#26))
               :                    :     :  :     +- *(51) FileScan parquet default.catalog_sales[cs_sold_date_sk#26,cs_item_sk#38,cs_quantity#22,cs_list_price#24] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/catalog_sales], PartitionFilters: [], PushedFilters: [IsNotNull(cs_item_sk), IsNotNull(cs_sold_date_sk)], ReadSchema: struct<cs_sold_date_sk:int,cs_item_sk:int,cs_quantity:int,cs_list_price:decimal(7,2)>
               :                    :     :  +- ReusedExchange [ss_item_sk#34], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :                    :     +- ReusedExchange [i_item_sk#33, i_brand_id#11, i_class_id#12, i_category_id#13], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :                    +- ReusedExchange [d_date_sk#20], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               +- *(78) Project [sales#45, number_sales#46, channel#47, i_brand_id#11, i_class_id#12, i_category_id#13]
                  +- *(78) Filter (isnotnull(sum(CheckOverflow((promote_precision(cast(cast(ws_quantity#27 as decimal(10,0)) as decimal(12,2))) * promote_precision(cast(ws_list_price#29 as decimal(12,2)))), DecimalType(18,2)))#48) && (cast(sum(CheckOverflow((promote_precision(cast(cast(ws_quantity#27 as decimal(10,0)) as decimal(12,2))) * promote_precision(cast(ws_list_price#29 as decimal(12,2)))), DecimalType(18,2)))#48 as decimal(32,6)) > cast(Subquery subquery1672 as decimal(32,6))))
                     :  +- Subquery subquery1672
                     :     +- *(8) HashAggregate(keys=[], functions=[avg(CheckOverflow((promote_precision(cast(cast(quantity#17 as decimal(10,0)) as decimal(12,2))) * promote_precision(cast(list_price#18 as decimal(12,2)))), DecimalType(18,2)))])
                     :        +- Exchange SinglePartition
                     :           +- *(7) HashAggregate(keys=[], functions=[partial_avg(CheckOverflow((promote_precision(cast(cast(quantity#17 as decimal(10,0)) as decimal(12,2))) * promote_precision(cast(list_price#18 as decimal(12,2)))), DecimalType(18,2)))])
                     :              +- Union
                     :                 :- *(2) Project [ss_quantity#14 AS quantity#17, ss_list_price#15 AS list_price#18]
                     :                 :  +- *(2) BroadcastHashJoin [ss_sold_date_sk#19], [d_date_sk#20], Inner, BuildRight
                     :                 :     :- *(2) Project [ss_sold_date_sk#19, ss_quantity#14, ss_list_price#15]
                     :                 :     :  +- *(2) Filter isnotnull(ss_sold_date_sk#19)
                     :                 :     :     +- *(2) FileScan parquet default.store_sales[ss_sold_date_sk#19,ss_quantity#14,ss_list_price#15] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_sold_date_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_quantity:int,ss_list_price:decimal(7,2)>
                     :                 :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                     :                 :        +- *(1) Project [d_date_sk#20]
                     :                 :           +- *(1) Filter (((isnotnull(d_year#21) && (d_year#21 >= 1999)) && (d_year#21 <= 2001)) && isnotnull(d_date_sk#20))
                     :                 :              +- *(1) FileScan parquet default.date_dim[d_date_sk#20,d_year#21] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), GreaterThanOrEqual(d_year,1999), LessThanOrEqual(d_year,2001), IsNotNull(d_da..., ReadSchema: struct<d_date_sk:int,d_year:int>
                     :                 :- *(4) Project [cs_quantity#22 AS quantity#23, cs_list_price#24 AS list_price#25]
                     :                 :  +- *(4) BroadcastHashJoin [cs_sold_date_sk#26], [d_date_sk#20], Inner, BuildRight
                     :                 :     :- *(4) Project [cs_sold_date_sk#26, cs_quantity#22, cs_list_price#24]
                     :                 :     :  +- *(4) Filter isnotnull(cs_sold_date_sk#26)
                     :                 :     :     +- *(4) FileScan parquet default.catalog_sales[cs_sold_date_sk#26,cs_quantity#22,cs_list_price#24] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/catalog_sales], PartitionFilters: [], PushedFilters: [IsNotNull(cs_sold_date_sk)], ReadSchema: struct<cs_sold_date_sk:int,cs_quantity:int,cs_list_price:decimal(7,2)>
                     :                 :     +- ReusedExchange [d_date_sk#20], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                     :                 +- *(6) Project [ws_quantity#27 AS quantity#28, ws_list_price#29 AS list_price#30]
                     :                    +- *(6) BroadcastHashJoin [ws_sold_date_sk#31], [d_date_sk#20], Inner, BuildRight
                     :                       :- *(6) Project [ws_sold_date_sk#31, ws_quantity#27, ws_list_price#29]
                     :                       :  +- *(6) Filter isnotnull(ws_sold_date_sk#31)
                     :                       :     +- *(6) FileScan parquet default.web_sales[ws_sold_date_sk#31,ws_quantity#27,ws_list_price#29] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/web_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ws_sold_date_sk)], ReadSchema: struct<ws_sold_date_sk:int,ws_quantity:int,ws_list_price:decimal(7,2)>
                     :                       +- ReusedExchange [d_date_sk#20], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                     +- *(78) HashAggregate(keys=[i_brand_id#11, i_class_id#12, i_category_id#13], functions=[sum(CheckOverflow((promote_precision(cast(cast(ws_quantity#27 as decimal(10,0)) as decimal(12,2))) * promote_precision(cast(ws_list_price#29 as decimal(12,2)))), DecimalType(18,2))), count(1)])
                        +- Exchange hashpartitioning(i_brand_id#11, i_class_id#12, i_category_id#13, 200)
                           +- *(77) HashAggregate(keys=[i_brand_id#11, i_class_id#12, i_category_id#13], functions=[partial_sum(CheckOverflow((promote_precision(cast(cast(ws_quantity#27 as decimal(10,0)) as decimal(12,2))) * promote_precision(cast(ws_list_price#29 as decimal(12,2)))), DecimalType(18,2))), partial_count(1)])
                              +- *(77) Project [ws_quantity#27, ws_list_price#29, i_brand_id#11, i_class_id#12, i_category_id#13]
                                 +- *(77) BroadcastHashJoin [ws_sold_date_sk#31], [d_date_sk#20], Inner, BuildRight
                                    :- *(77) Project [ws_sold_date_sk#31, ws_quantity#27, ws_list_price#29, i_brand_id#11, i_class_id#12, i_category_id#13]
                                    :  +- *(77) BroadcastHashJoin [ws_item_sk#39], [i_item_sk#33], Inner, BuildRight
                                    :     :- *(77) BroadcastHashJoin [ws_item_sk#39], [ss_item_sk#34], LeftSemi, BuildRight
                                    :     :  :- *(77) Project [ws_sold_date_sk#31, ws_item_sk#39, ws_quantity#27, ws_list_price#29]
                                    :     :  :  +- *(77) Filter (isnotnull(ws_item_sk#39) && isnotnull(ws_sold_date_sk#31))
                                    :     :  :     +- *(77) FileScan parquet default.web_sales[ws_sold_date_sk#31,ws_item_sk#39,ws_quantity#27,ws_list_price#29] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/web_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ws_item_sk), IsNotNull(ws_sold_date_sk)], ReadSchema: struct<ws_sold_date_sk:int,ws_item_sk:int,ws_quantity:int,ws_list_price:decimal(7,2)>
                                    :     :  +- ReusedExchange [ss_item_sk#34], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                                    :     +- ReusedExchange [i_item_sk#33, i_brand_id#11, i_class_id#12, i_category_id#13], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                                    +- ReusedExchange [d_date_sk#20], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))