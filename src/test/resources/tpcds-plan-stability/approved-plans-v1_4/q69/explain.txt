== Physical Plan ==
TakeOrderedAndProject(limit=100, orderBy=[cd_gender#1 ASC NULLS FIRST,cd_marital_status#2 ASC NULLS FIRST,cd_education_status#3 ASC NULLS FIRST,cd_purchase_estimate#4 ASC NULLS FIRST,cd_credit_rating#5 ASC NULLS FIRST], output=[cd_gender#1,cd_marital_status#2,cd_education_status#3,cnt1#6,cd_purchase_estimate#4,cnt2#7,cd_credit_rating#5,cnt3#8])
+- *(10) HashAggregate(keys=[cd_gender#1, cd_marital_status#2, cd_education_status#3, cd_purchase_estimate#4, cd_credit_rating#5], functions=[count(1)])
   +- Exchange hashpartitioning(cd_gender#1, cd_marital_status#2, cd_education_status#3, cd_purchase_estimate#4, cd_credit_rating#5, 5)
      +- *(9) HashAggregate(keys=[cd_gender#1, cd_marital_status#2, cd_education_status#3, cd_purchase_estimate#4, cd_credit_rating#5], functions=[partial_count(1)])
         +- *(9) Project [cd_gender#1, cd_marital_status#2, cd_education_status#3, cd_purchase_estimate#4, cd_credit_rating#5]
            +- *(9) BroadcastHashJoin [c_current_cdemo_sk#9], [cd_demo_sk#10], Inner, BuildRight
               :- *(9) Project [c_current_cdemo_sk#9]
               :  +- *(9) BroadcastHashJoin [c_current_addr_sk#11], [ca_address_sk#12], Inner, BuildRight
               :     :- *(9) Project [c_current_cdemo_sk#9, c_current_addr_sk#11]
               :     :  +- *(9) BroadcastHashJoin [c_customer_sk#13], [cs_ship_customer_sk#14], LeftAnti, BuildRight
               :     :     :- *(9) BroadcastHashJoin [c_customer_sk#13], [ws_bill_customer_sk#15], LeftAnti, BuildRight
               :     :     :  :- *(9) BroadcastHashJoin [c_customer_sk#13], [ss_customer_sk#16], LeftSemi, BuildRight
               :     :     :  :  :- *(9) Project [c_customer_sk#13, c_current_cdemo_sk#9, c_current_addr_sk#11]
               :     :     :  :  :  +- *(9) Filter (isnotnull(c_current_addr_sk#11) && isnotnull(c_current_cdemo_sk#9))
               :     :     :  :  :     +- *(9) FileScan parquet default.customer[c_customer_sk#13,c_current_cdemo_sk#9,c_current_addr_sk#11] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/customer], PartitionFilters: [], PushedFilters: [IsNotNull(c_current_addr_sk), IsNotNull(c_current_cdemo_sk)], ReadSchema: struct<c_customer_sk:int,c_current_cdemo_sk:int,c_current_addr_sk:int>
               :     :     :  :  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :     :     :  :     +- *(2) Project [ss_customer_sk#16]
               :     :     :  :        +- *(2) BroadcastHashJoin [ss_sold_date_sk#17], [d_date_sk#18], Inner, BuildRight
               :     :     :  :           :- *(2) Project [ss_sold_date_sk#17, ss_customer_sk#16]
               :     :     :  :           :  +- *(2) Filter isnotnull(ss_sold_date_sk#17)
               :     :     :  :           :     +- *(2) FileScan parquet default.store_sales[ss_sold_date_sk#17,ss_customer_sk#16] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_sold_date_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_customer_sk:int>
               :     :     :  :           +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :     :     :  :              +- *(1) Project [d_date_sk#18]
               :     :     :  :                 +- *(1) Filter (((((isnotnull(d_year#19) && isnotnull(d_moy#20)) && (d_year#19 = 2001)) && (d_moy#20 >= 4)) && (d_moy#20 <= 6)) && isnotnull(d_date_sk#18))
               :     :     :  :                    +- *(1) FileScan parquet default.date_dim[d_date_sk#18,d_year#19,d_moy#20] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), IsNotNull(d_moy), EqualTo(d_year,2001), GreaterThanOrEqual(d_moy,4), LessThan..., ReadSchema: struct<d_date_sk:int,d_year:int,d_moy:int>
               :     :     :  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :     :     :     +- *(4) Project [ws_bill_customer_sk#15]
               :     :     :        +- *(4) BroadcastHashJoin [ws_sold_date_sk#21], [d_date_sk#18], Inner, BuildRight
               :     :     :           :- *(4) Project [ws_sold_date_sk#21, ws_bill_customer_sk#15]
               :     :     :           :  +- *(4) Filter isnotnull(ws_sold_date_sk#21)
               :     :     :           :     +- *(4) FileScan parquet default.web_sales[ws_sold_date_sk#21,ws_bill_customer_sk#15] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/web_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ws_sold_date_sk)], ReadSchema: struct<ws_sold_date_sk:int,ws_bill_customer_sk:int>
               :     :     :           +- ReusedExchange [d_date_sk#18], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :     :        +- *(6) Project [cs_ship_customer_sk#14]
               :     :           +- *(6) BroadcastHashJoin [cs_sold_date_sk#22], [d_date_sk#18], Inner, BuildRight
               :     :              :- *(6) Project [cs_sold_date_sk#22, cs_ship_customer_sk#14]
               :     :              :  +- *(6) Filter isnotnull(cs_sold_date_sk#22)
               :     :              :     +- *(6) FileScan parquet default.catalog_sales[cs_sold_date_sk#22,cs_ship_customer_sk#14] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/catalog_sales], PartitionFilters: [], PushedFilters: [IsNotNull(cs_sold_date_sk)], ReadSchema: struct<cs_sold_date_sk:int,cs_ship_customer_sk:int>
               :     :              +- ReusedExchange [d_date_sk#18], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :        +- *(7) Project [ca_address_sk#12]
               :           +- *(7) Filter (ca_state#23 IN (KY,GA,NM) && isnotnull(ca_address_sk#12))
               :              +- *(7) FileScan parquet default.customer_address[ca_address_sk#12,ca_state#23] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/customer_address], PartitionFilters: [], PushedFilters: [In(ca_state, [KY,GA,NM]), IsNotNull(ca_address_sk)], ReadSchema: struct<ca_address_sk:int,ca_state:string>
               +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                  +- *(8) Project [cd_demo_sk#10, cd_gender#1, cd_marital_status#2, cd_education_status#3, cd_purchase_estimate#4, cd_credit_rating#5]
                     +- *(8) Filter isnotnull(cd_demo_sk#10)
                        +- *(8) FileScan parquet default.customer_demographics[cd_demo_sk#10,cd_gender#1,cd_marital_status#2,cd_education_status#3,cd_purchase_estimate#4,cd_credit_rating#5] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/customer_demographics], PartitionFilters: [], PushedFilters: [IsNotNull(cd_demo_sk)], ReadSchema: struct<cd_demo_sk:int,cd_gender:string,cd_marital_status:string,cd_education_status:string,cd_pur...