== Physical Plan ==
TakeOrderedAndProject(limit=100, orderBy=[channel#1 ASC NULLS FIRST,col_name#2 ASC NULLS FIRST,d_year#3 ASC NULLS FIRST,d_qoy#4 ASC NULLS FIRST,i_category#5 ASC NULLS FIRST], output=[channel#1,col_name#2,d_year#3,d_qoy#4,i_category#5,sales_cnt#6,sales_amt#7])
+- *(11) HashAggregate(keys=[channel#1, col_name#2, d_year#3, d_qoy#4, i_category#5], functions=[count(1), sum(UnscaledValue(ext_sales_price#8))])
   +- Exchange hashpartitioning(channel#1, col_name#2, d_year#3, d_qoy#4, i_category#5, 200)
      +- *(10) HashAggregate(keys=[channel#1, col_name#2, d_year#3, d_qoy#4, i_category#5], functions=[partial_count(1), partial_sum(UnscaledValue(ext_sales_price#8))])
         +- Union
            :- *(3) Project [store AS channel#1, ss_store_sk#9 AS col_name#2, d_year#3, d_qoy#4, i_category#5, ss_ext_sales_price#10 AS ext_sales_price#8]
            :  +- *(3) BroadcastHashJoin [ss_sold_date_sk#11], [d_date_sk#12], Inner, BuildRight
            :     :- *(3) Project [ss_sold_date_sk#11, ss_store_sk#9, ss_ext_sales_price#10, i_category#5]
            :     :  +- *(3) BroadcastHashJoin [ss_item_sk#13], [i_item_sk#14], Inner, BuildRight
            :     :     :- *(3) Project [ss_sold_date_sk#11, ss_item_sk#13, ss_store_sk#9, ss_ext_sales_price#10]
            :     :     :  +- *(3) Filter ((isnull(ss_store_sk#9) && isnotnull(ss_item_sk#13)) && isnotnull(ss_sold_date_sk#11))
            :     :     :     +- *(3) FileScan parquet default.store_sales[ss_sold_date_sk#11,ss_item_sk#13,ss_store_sk#9,ss_ext_sales_price#10] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNull(ss_store_sk), IsNotNull(ss_item_sk), IsNotNull(ss_sold_date_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_item_sk:int,ss_store_sk:int,ss_ext_sales_price:decimal(7,2)>
            :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
            :     :        +- *(1) Project [i_item_sk#14, i_category#5]
            :     :           +- *(1) Filter isnotnull(i_item_sk#14)
            :     :              +- *(1) FileScan parquet default.item[i_item_sk#14,i_category#5] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_item_sk)], ReadSchema: struct<i_item_sk:int,i_category:string>
            :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
            :        +- *(2) Project [d_date_sk#12, d_year#3, d_qoy#4]
            :           +- *(2) Filter isnotnull(d_date_sk#12)
            :              +- *(2) FileScan parquet default.date_dim[d_date_sk#12,d_year#3,d_qoy#4] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_year:int,d_qoy:int>
            :- *(6) Project [web AS channel#15, ws_ship_customer_sk#16 AS col_name#17, d_year#3, d_qoy#4, i_category#5, ws_ext_sales_price#18 AS ext_sales_price#19]
            :  +- *(6) BroadcastHashJoin [ws_sold_date_sk#20], [d_date_sk#12], Inner, BuildRight
            :     :- *(6) Project [ws_sold_date_sk#20, ws_ship_customer_sk#16, ws_ext_sales_price#18, i_category#5]
            :     :  +- *(6) BroadcastHashJoin [ws_item_sk#21], [i_item_sk#14], Inner, BuildRight
            :     :     :- *(6) Project [ws_sold_date_sk#20, ws_item_sk#21, ws_ship_customer_sk#16, ws_ext_sales_price#18]
            :     :     :  +- *(6) Filter ((isnull(ws_ship_customer_sk#16) && isnotnull(ws_item_sk#21)) && isnotnull(ws_sold_date_sk#20))
            :     :     :     +- *(6) FileScan parquet default.web_sales[ws_sold_date_sk#20,ws_item_sk#21,ws_ship_customer_sk#16,ws_ext_sales_price#18] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/web_sales], PartitionFilters: [], PushedFilters: [IsNull(ws_ship_customer_sk), IsNotNull(ws_item_sk), IsNotNull(ws_sold_date_sk)], ReadSchema: struct<ws_sold_date_sk:int,ws_item_sk:int,ws_ship_customer_sk:int,ws_ext_sales_price:decimal(7,2)>
            :     :     +- ReusedExchange [i_item_sk#14, i_category#5], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
            :     +- ReusedExchange [d_date_sk#12, d_year#3, d_qoy#4], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
            +- *(9) Project [catalog AS channel#22, cs_ship_addr_sk#23 AS col_name#24, d_year#3, d_qoy#4, i_category#5, cs_ext_sales_price#25 AS ext_sales_price#26]
               +- *(9) BroadcastHashJoin [cs_sold_date_sk#27], [d_date_sk#12], Inner, BuildRight
                  :- *(9) Project [cs_sold_date_sk#27, cs_ship_addr_sk#23, cs_ext_sales_price#25, i_category#5]
                  :  +- *(9) BroadcastHashJoin [cs_item_sk#28], [i_item_sk#14], Inner, BuildRight
                  :     :- *(9) Project [cs_sold_date_sk#27, cs_ship_addr_sk#23, cs_item_sk#28, cs_ext_sales_price#25]
                  :     :  +- *(9) Filter ((isnull(cs_ship_addr_sk#23) && isnotnull(cs_item_sk#28)) && isnotnull(cs_sold_date_sk#27))
                  :     :     +- *(9) FileScan parquet default.catalog_sales[cs_sold_date_sk#27,cs_ship_addr_sk#23,cs_item_sk#28,cs_ext_sales_price#25] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/catalog_sales], PartitionFilters: [], PushedFilters: [IsNull(cs_ship_addr_sk), IsNotNull(cs_item_sk), IsNotNull(cs_sold_date_sk)], ReadSchema: struct<cs_sold_date_sk:int,cs_ship_addr_sk:int,cs_item_sk:int,cs_ext_sales_price:decimal(7,2)>
                  :     +- ReusedExchange [i_item_sk#14, i_category#5], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                  +- ReusedExchange [d_date_sk#12, d_year#3, d_qoy#4], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))