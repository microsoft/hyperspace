== Physical Plan ==
TakeOrderedAndProject(limit=100, orderBy=[i_item_id#1 ASC NULLS FIRST,total_sales#2 ASC NULLS FIRST], output=[i_item_id#1,total_sales#2])
+- *(20) HashAggregate(keys=[i_item_id#1], functions=[sum(total_sales#3)])
   +- Exchange hashpartitioning(i_item_id#1, 200)
      +- *(19) HashAggregate(keys=[i_item_id#1], functions=[partial_sum(total_sales#3)])
         +- Union
            :- *(6) HashAggregate(keys=[i_item_id#1], functions=[sum(UnscaledValue(ss_ext_sales_price#4))])
            :  +- Exchange hashpartitioning(i_item_id#1, 200)
            :     +- *(5) HashAggregate(keys=[i_item_id#1], functions=[partial_sum(UnscaledValue(ss_ext_sales_price#4))])
            :        +- *(5) Project [ss_ext_sales_price#4, i_item_id#1]
            :           +- *(5) BroadcastHashJoin [ss_item_sk#5], [i_item_sk#6], Inner, BuildRight
            :              :- *(5) Project [ss_item_sk#5, ss_ext_sales_price#4]
            :              :  +- *(5) BroadcastHashJoin [ss_addr_sk#7], [ca_address_sk#8], Inner, BuildRight
            :              :     :- *(5) Project [ss_item_sk#5, ss_addr_sk#7, ss_ext_sales_price#4]
            :              :     :  +- *(5) BroadcastHashJoin [ss_sold_date_sk#9], [d_date_sk#10], Inner, BuildRight
            :              :     :     :- *(5) Project [ss_sold_date_sk#9, ss_item_sk#5, ss_addr_sk#7, ss_ext_sales_price#4]
            :              :     :     :  +- *(5) Filter ((isnotnull(ss_sold_date_sk#9) && isnotnull(ss_addr_sk#7)) && isnotnull(ss_item_sk#5))
            :              :     :     :     +- *(5) FileScan parquet default.store_sales[ss_sold_date_sk#9,ss_item_sk#5,ss_addr_sk#7,ss_ext_sales_price#4] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_sold_date_sk), IsNotNull(ss_addr_sk), IsNotNull(ss_item_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_item_sk:int,ss_addr_sk:int,ss_ext_sales_price:decimal(7,2)>
            :              :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
            :              :     :        +- *(1) Project [d_date_sk#10]
            :              :     :           +- *(1) Filter ((((isnotnull(d_year#11) && isnotnull(d_moy#12)) && (d_year#11 = 1998)) && (d_moy#12 = 9)) && isnotnull(d_date_sk#10))
            :              :     :              +- *(1) FileScan parquet default.date_dim[d_date_sk#10,d_year#11,d_moy#12] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), IsNotNull(d_moy), EqualTo(d_year,1998), EqualTo(d_moy,9), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_year:int,d_moy:int>
            :              :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
            :              :        +- *(2) Project [ca_address_sk#8]
            :              :           +- *(2) Filter ((isnotnull(ca_gmt_offset#13) && (ca_gmt_offset#13 = -5.00)) && isnotnull(ca_address_sk#8))
            :              :              +- *(2) FileScan parquet default.customer_address[ca_address_sk#8,ca_gmt_offset#13] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/customer_address], PartitionFilters: [], PushedFilters: [IsNotNull(ca_gmt_offset), EqualTo(ca_gmt_offset,-5.00), IsNotNull(ca_address_sk)], ReadSchema: struct<ca_address_sk:int,ca_gmt_offset:decimal(5,2)>
            :              +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
            :                 +- *(4) BroadcastHashJoin [i_item_id#1], [i_item_id#1#14], LeftSemi, BuildRight
            :                    :- *(4) Project [i_item_sk#6, i_item_id#1]
            :                    :  +- *(4) Filter isnotnull(i_item_sk#6)
            :                    :     +- *(4) FileScan parquet default.item[i_item_sk#6,i_item_id#1] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_item_sk)], ReadSchema: struct<i_item_sk:int,i_item_id:string>
            :                    +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))
            :                       +- *(3) Project [i_item_id#1 AS i_item_id#1#14]
            :                          +- *(3) Filter (isnotnull(i_category#15) && (i_category#15 = Music))
            :                             +- *(3) FileScan parquet default.item[i_item_id#1,i_category#15] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_category), EqualTo(i_category,Music)], ReadSchema: struct<i_item_id:string,i_category:string>
            :- *(12) HashAggregate(keys=[i_item_id#1], functions=[sum(UnscaledValue(cs_ext_sales_price#16))])
            :  +- Exchange hashpartitioning(i_item_id#1, 200)
            :     +- *(11) HashAggregate(keys=[i_item_id#1], functions=[partial_sum(UnscaledValue(cs_ext_sales_price#16))])
            :        +- *(11) Project [cs_ext_sales_price#16, i_item_id#1]
            :           +- *(11) BroadcastHashJoin [cs_item_sk#17], [i_item_sk#6], Inner, BuildRight
            :              :- *(11) Project [cs_item_sk#17, cs_ext_sales_price#16]
            :              :  +- *(11) BroadcastHashJoin [cs_bill_addr_sk#18], [ca_address_sk#8], Inner, BuildRight
            :              :     :- *(11) Project [cs_bill_addr_sk#18, cs_item_sk#17, cs_ext_sales_price#16]
            :              :     :  +- *(11) BroadcastHashJoin [cs_sold_date_sk#19], [d_date_sk#10], Inner, BuildRight
            :              :     :     :- *(11) Project [cs_sold_date_sk#19, cs_bill_addr_sk#18, cs_item_sk#17, cs_ext_sales_price#16]
            :              :     :     :  +- *(11) Filter ((isnotnull(cs_sold_date_sk#19) && isnotnull(cs_bill_addr_sk#18)) && isnotnull(cs_item_sk#17))
            :              :     :     :     +- *(11) FileScan parquet default.catalog_sales[cs_sold_date_sk#19,cs_bill_addr_sk#18,cs_item_sk#17,cs_ext_sales_price#16] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/catalog_sales], PartitionFilters: [], PushedFilters: [IsNotNull(cs_sold_date_sk), IsNotNull(cs_bill_addr_sk), IsNotNull(cs_item_sk)], ReadSchema: struct<cs_sold_date_sk:int,cs_bill_addr_sk:int,cs_item_sk:int,cs_ext_sales_price:decimal(7,2)>
            :              :     :     +- ReusedExchange [d_date_sk#10], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
            :              :     +- ReusedExchange [ca_address_sk#8], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
            :              +- ReusedExchange [i_item_sk#6, i_item_id#1], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
            +- *(18) HashAggregate(keys=[i_item_id#1], functions=[sum(UnscaledValue(ws_ext_sales_price#20))])
               +- Exchange hashpartitioning(i_item_id#1, 200)
                  +- *(17) HashAggregate(keys=[i_item_id#1], functions=[partial_sum(UnscaledValue(ws_ext_sales_price#20))])
                     +- *(17) Project [ws_ext_sales_price#20, i_item_id#1]
                        +- *(17) BroadcastHashJoin [ws_item_sk#21], [i_item_sk#6], Inner, BuildRight
                           :- *(17) Project [ws_item_sk#21, ws_ext_sales_price#20]
                           :  +- *(17) BroadcastHashJoin [ws_bill_addr_sk#22], [ca_address_sk#8], Inner, BuildRight
                           :     :- *(17) Project [ws_item_sk#21, ws_bill_addr_sk#22, ws_ext_sales_price#20]
                           :     :  +- *(17) BroadcastHashJoin [ws_sold_date_sk#23], [d_date_sk#10], Inner, BuildRight
                           :     :     :- *(17) Project [ws_sold_date_sk#23, ws_item_sk#21, ws_bill_addr_sk#22, ws_ext_sales_price#20]
                           :     :     :  +- *(17) Filter ((isnotnull(ws_sold_date_sk#23) && isnotnull(ws_bill_addr_sk#22)) && isnotnull(ws_item_sk#21))
                           :     :     :     +- *(17) FileScan parquet default.web_sales[ws_sold_date_sk#23,ws_item_sk#21,ws_bill_addr_sk#22,ws_ext_sales_price#20] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/web_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ws_sold_date_sk), IsNotNull(ws_bill_addr_sk), IsNotNull(ws_item_sk)], ReadSchema: struct<ws_sold_date_sk:int,ws_item_sk:int,ws_bill_addr_sk:int,ws_ext_sales_price:decimal(7,2)>
                           :     :     +- ReusedExchange [d_date_sk#10], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                           :     +- ReusedExchange [ca_address_sk#8], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                           +- ReusedExchange [i_item_sk#6, i_item_id#1], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))