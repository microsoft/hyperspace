== Physical Plan ==
TakeOrderedAndProject(limit=100, orderBy=[i_item_id#1 ASC NULLS FIRST,i_item_desc#2 ASC NULLS FIRST,s_store_id#3 ASC NULLS FIRST,s_store_name#4 ASC NULLS FIRST], output=[i_item_id#1,i_item_desc#2,s_store_id#3,s_store_name#4,store_sales_profit#5,store_returns_loss#6,catalog_sales_profit#7])
+- *(9) HashAggregate(keys=[i_item_id#1, i_item_desc#2, s_store_id#3, s_store_name#4], functions=[sum(UnscaledValue(ss_net_profit#8)), sum(UnscaledValue(sr_net_loss#9)), sum(UnscaledValue(cs_net_profit#10))])
   +- Exchange hashpartitioning(i_item_id#1, i_item_desc#2, s_store_id#3, s_store_name#4, 5)
      +- *(8) HashAggregate(keys=[i_item_id#1, i_item_desc#2, s_store_id#3, s_store_name#4], functions=[partial_sum(UnscaledValue(ss_net_profit#8)), partial_sum(UnscaledValue(sr_net_loss#9)), partial_sum(UnscaledValue(cs_net_profit#10))])
         +- *(8) Project [ss_net_profit#8, sr_net_loss#9, cs_net_profit#10, s_store_id#3, s_store_name#4, i_item_id#1, i_item_desc#2]
            +- *(8) BroadcastHashJoin [ss_item_sk#11], [i_item_sk#12], Inner, BuildRight
               :- *(8) Project [ss_item_sk#11, ss_net_profit#8, sr_net_loss#9, cs_net_profit#10, s_store_id#3, s_store_name#4]
               :  +- *(8) BroadcastHashJoin [ss_store_sk#13], [s_store_sk#14], Inner, BuildRight
               :     :- *(8) Project [ss_item_sk#11, ss_store_sk#13, ss_net_profit#8, sr_net_loss#9, cs_net_profit#10]
               :     :  +- *(8) BroadcastHashJoin [cs_sold_date_sk#15], [d_date_sk#16], Inner, BuildRight
               :     :     :- *(8) Project [ss_item_sk#11, ss_store_sk#13, ss_net_profit#8, sr_net_loss#9, cs_sold_date_sk#15, cs_net_profit#10]
               :     :     :  +- *(8) BroadcastHashJoin [sr_returned_date_sk#17], [cast(d_date_sk#18 as bigint)], Inner, BuildRight
               :     :     :     :- *(8) Project [ss_item_sk#11, ss_store_sk#13, ss_net_profit#8, sr_returned_date_sk#17, sr_net_loss#9, cs_sold_date_sk#15, cs_net_profit#10]
               :     :     :     :  +- *(8) BroadcastHashJoin [ss_sold_date_sk#19], [d_date_sk#20], Inner, BuildRight
               :     :     :     :     :- *(8) Project [ss_sold_date_sk#19, ss_item_sk#11, ss_store_sk#13, ss_net_profit#8, sr_returned_date_sk#17, sr_net_loss#9, cs_sold_date_sk#15, cs_net_profit#10]
               :     :     :     :     :  +- *(8) BroadcastHashJoin [sr_customer_sk#21, sr_item_sk#22], [cast(cs_bill_customer_sk#23 as bigint), cast(cs_item_sk#24 as bigint)], Inner, BuildRight
               :     :     :     :     :     :- *(8) Project [ss_sold_date_sk#19, ss_item_sk#11, ss_store_sk#13, ss_net_profit#8, sr_returned_date_sk#17, sr_item_sk#22, sr_customer_sk#21, sr_net_loss#9]
               :     :     :     :     :     :  +- *(8) BroadcastHashJoin [cast(ss_customer_sk#25 as bigint), cast(ss_item_sk#11 as bigint), cast(ss_ticket_number#26 as bigint)], [sr_customer_sk#21, sr_item_sk#22, sr_ticket_number#27], Inner, BuildRight
               :     :     :     :     :     :     :- *(8) Project [ss_sold_date_sk#19, ss_item_sk#11, ss_customer_sk#25, ss_store_sk#13, ss_ticket_number#26, ss_net_profit#8]
               :     :     :     :     :     :     :  +- *(8) Filter ((((isnotnull(ss_ticket_number#26) && isnotnull(ss_item_sk#11)) && isnotnull(ss_customer_sk#25)) && isnotnull(ss_sold_date_sk#19)) && isnotnull(ss_store_sk#13))
               :     :     :     :     :     :     :     +- *(8) FileScan parquet default.store_sales[ss_sold_date_sk#19,ss_item_sk#11,ss_customer_sk#25,ss_store_sk#13,ss_ticket_number#26,ss_net_profit#8] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_ticket_number), IsNotNull(ss_item_sk), IsNotNull(ss_customer_sk), IsNotNull(ss_sold..., ReadSchema: struct<ss_sold_date_sk:int,ss_item_sk:int,ss_customer_sk:int,ss_store_sk:int,ss_ticket_number:int...
               :     :     :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[2, bigint, true], input[1, bigint, true], input[3, bigint, true]))
               :     :     :     :     :     :        +- *(1) Project [sr_returned_date_sk#17, sr_item_sk#22, sr_customer_sk#21, sr_ticket_number#27, sr_net_loss#9]
               :     :     :     :     :     :           +- *(1) Filter (((isnotnull(sr_ticket_number#27) && isnotnull(sr_item_sk#22)) && isnotnull(sr_customer_sk#21)) && isnotnull(sr_returned_date_sk#17))
               :     :     :     :     :     :              +- *(1) FileScan parquet default.store_returns[sr_returned_date_sk#17,sr_item_sk#22,sr_customer_sk#21,sr_ticket_number#27,sr_net_loss#9] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_returns], PartitionFilters: [], PushedFilters: [IsNotNull(sr_ticket_number), IsNotNull(sr_item_sk), IsNotNull(sr_customer_sk), IsNotNull(sr_retu..., ReadSchema: struct<sr_returned_date_sk:bigint,sr_item_sk:bigint,sr_customer_sk:bigint,sr_ticket_number:bigint...
               :     :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[1, int, true] as bigint), cast(input[2, int, true] as bigint)))
               :     :     :     :     :        +- *(2) Project [cs_sold_date_sk#15, cs_bill_customer_sk#23, cs_item_sk#24, cs_net_profit#10]
               :     :     :     :     :           +- *(2) Filter ((isnotnull(cs_item_sk#24) && isnotnull(cs_bill_customer_sk#23)) && isnotnull(cs_sold_date_sk#15))
               :     :     :     :     :              +- *(2) FileScan parquet default.catalog_sales[cs_sold_date_sk#15,cs_bill_customer_sk#23,cs_item_sk#24,cs_net_profit#10] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/catalog_sales], PartitionFilters: [], PushedFilters: [IsNotNull(cs_item_sk), IsNotNull(cs_bill_customer_sk), IsNotNull(cs_sold_date_sk)], ReadSchema: struct<cs_sold_date_sk:int,cs_bill_customer_sk:int,cs_item_sk:int,cs_net_profit:decimal(7,2)>
               :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :     :     :     :        +- *(3) Project [d_date_sk#20]
               :     :     :     :           +- *(3) Filter ((((isnotnull(d_moy#28) && isnotnull(d_year#29)) && (d_moy#28 = 4)) && (d_year#29 = 2001)) && isnotnull(d_date_sk#20))
               :     :     :     :              +- *(3) FileScan parquet default.date_dim[d_date_sk#20,d_year#29,d_moy#28] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_moy), IsNotNull(d_year), EqualTo(d_moy,4), EqualTo(d_year,2001), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_year:int,d_moy:int>
               :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :     :     :        +- *(4) Project [d_date_sk#18]
               :     :     :           +- *(4) Filter (((((isnotnull(d_year#30) && isnotnull(d_moy#31)) && (d_moy#31 >= 4)) && (d_moy#31 <= 10)) && (d_year#30 = 2001)) && isnotnull(d_date_sk#18))
               :     :     :              +- *(4) FileScan parquet default.date_dim[d_date_sk#18,d_year#30,d_moy#31] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), IsNotNull(d_moy), GreaterThanOrEqual(d_moy,4), LessThanOrEqual(d_moy,10), Equ..., ReadSchema: struct<d_date_sk:int,d_year:int,d_moy:int>
               :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :     :        +- *(5) Project [d_date_sk#16]
               :     :           +- *(5) Filter (((((isnotnull(d_moy#32) && isnotnull(d_year#33)) && (d_moy#32 >= 4)) && (d_moy#32 <= 10)) && (d_year#33 = 2001)) && isnotnull(d_date_sk#16))
               :     :              +- *(5) FileScan parquet default.date_dim[d_date_sk#16,d_year#33,d_moy#32] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_moy), IsNotNull(d_year), GreaterThanOrEqual(d_moy,4), LessThanOrEqual(d_moy,10), Equ..., ReadSchema: struct<d_date_sk:int,d_year:int,d_moy:int>
               :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :        +- *(6) Project [s_store_sk#14, s_store_id#3, s_store_name#4]
               :           +- *(6) Filter isnotnull(s_store_sk#14)
               :              +- *(6) FileScan parquet default.store[s_store_sk#14,s_store_id#3,s_store_name#4] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store], PartitionFilters: [], PushedFilters: [IsNotNull(s_store_sk)], ReadSchema: struct<s_store_sk:int,s_store_id:string,s_store_name:string>
               +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                  +- *(7) Project [i_item_sk#12, i_item_id#1, i_item_desc#2]
                     +- *(7) Filter isnotnull(i_item_sk#12)
                        +- *(7) FileScan parquet default.item[i_item_sk#12,i_item_id#1,i_item_desc#2] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_item_sk)], ReadSchema: struct<i_item_sk:int,i_item_id:string,i_item_desc:string>