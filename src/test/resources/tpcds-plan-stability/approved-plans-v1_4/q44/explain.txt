== Physical Plan ==
TakeOrderedAndProject(limit=100, orderBy=[rnk#1 ASC NULLS FIRST], output=[rnk#1,best_performing#2,worst_performing#3])
+- *(10) Project [rnk#1, i_product_name#4 AS best_performing#2, i_product_name#5 AS worst_performing#3]
   +- *(10) BroadcastHashJoin [item_sk#6], [i_item_sk#7], Inner, BuildRight
      :- *(10) Project [rnk#1, item_sk#6, i_product_name#4]
      :  +- *(10) BroadcastHashJoin [item_sk#8], [i_item_sk#9], Inner, BuildRight
      :     :- *(10) Project [item_sk#8, rnk#1, item_sk#6]
      :     :  +- *(10) BroadcastHashJoin [rnk#1], [rnk#10], Inner, BuildRight
      :     :     :- *(10) Project [item_sk#8, rnk#1]
      :     :     :  +- *(10) Filter ((isnotnull(rnk#1) && (rnk#1 < 11)) && isnotnull(item_sk#8))
      :     :     :     +- Window [rank(rank_col#11) windowspecdefinition(rank_col#11 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rnk#1], [rank_col#11 ASC NULLS FIRST]
      :     :     :        +- *(3) Sort [rank_col#11 ASC NULLS FIRST], false, 0
      :     :     :           +- Exchange SinglePartition
      :     :     :              +- *(2) Project [item_sk#8, rank_col#11]
      :     :     :                 +- *(2) Filter (isnotnull(avg(ss_net_profit#12)#13) && (cast(avg(ss_net_profit#12)#13 as decimal(13,7)) > CheckOverflow((0.900000 * promote_precision(Subquery subquery7089)), DecimalType(13,7))))
      :     :     :                    :  +- Subquery subquery7089
      :     :     :                    :     +- *(2) HashAggregate(keys=[ss_store_sk#14], functions=[avg(UnscaledValue(ss_net_profit#12))])
      :     :     :                    :        +- Exchange hashpartitioning(ss_store_sk#14, 200)
      :     :     :                    :           +- *(1) HashAggregate(keys=[ss_store_sk#14], functions=[partial_avg(UnscaledValue(ss_net_profit#12))])
      :     :     :                    :              +- *(1) Project [ss_store_sk#14, ss_net_profit#12]
      :     :     :                    :                 +- *(1) Filter ((isnotnull(ss_store_sk#14) && (ss_store_sk#14 = 4)) && isnull(ss_addr_sk#15))
      :     :     :                    :                    +- *(1) FileScan parquet default.store_sales[ss_addr_sk#15,ss_store_sk#14,ss_net_profit#12] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_store_sk), EqualTo(ss_store_sk,4), IsNull(ss_addr_sk)], ReadSchema: struct<ss_addr_sk:int,ss_store_sk:int,ss_net_profit:decimal(7,2)>
      :     :     :                    +- *(2) HashAggregate(keys=[ss_item_sk#16], functions=[avg(UnscaledValue(ss_net_profit#12))])
      :     :     :                       +- Exchange hashpartitioning(ss_item_sk#16, 200)
      :     :     :                          +- *(1) HashAggregate(keys=[ss_item_sk#16], functions=[partial_avg(UnscaledValue(ss_net_profit#12))])
      :     :     :                             +- *(1) Project [ss_item_sk#16, ss_net_profit#12]
      :     :     :                                +- *(1) Filter (isnotnull(ss_store_sk#14) && (ss_store_sk#14 = 4))
      :     :     :                                   +- *(1) FileScan parquet default.store_sales[ss_item_sk#16,ss_store_sk#14,ss_net_profit#12] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_store_sk), EqualTo(ss_store_sk,4)], ReadSchema: struct<ss_item_sk:int,ss_store_sk:int,ss_net_profit:decimal(7,2)>
      :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[1, int, true] as bigint)))
      :     :        +- *(7) Project [item_sk#6, rnk#10]
      :     :           +- *(7) Filter ((isnotnull(rnk#10) && (rnk#10 < 11)) && isnotnull(item_sk#6))
      :     :              +- Window [rank(rank_col#17) windowspecdefinition(rank_col#17 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rnk#10], [rank_col#17 DESC NULLS LAST]
      :     :                 +- *(6) Sort [rank_col#17 DESC NULLS LAST], false, 0
      :     :                    +- Exchange SinglePartition
      :     :                       +- *(5) Project [item_sk#6, rank_col#17]
      :     :                          +- *(5) Filter (isnotnull(avg(ss_net_profit#12)#18) && (cast(avg(ss_net_profit#12)#18 as decimal(13,7)) > CheckOverflow((0.900000 * promote_precision(Subquery subquery7094)), DecimalType(13,7))))
      :     :                             :  +- Subquery subquery7094
      :     :                             :     +- *(2) HashAggregate(keys=[ss_store_sk#14], functions=[avg(UnscaledValue(ss_net_profit#12))])
      :     :                             :        +- Exchange hashpartitioning(ss_store_sk#14, 200)
      :     :                             :           +- *(1) HashAggregate(keys=[ss_store_sk#14], functions=[partial_avg(UnscaledValue(ss_net_profit#12))])
      :     :                             :              +- *(1) Project [ss_store_sk#14, ss_net_profit#12]
      :     :                             :                 +- *(1) Filter ((isnotnull(ss_store_sk#14) && (ss_store_sk#14 = 4)) && isnull(ss_addr_sk#15))
      :     :                             :                    +- *(1) FileScan parquet default.store_sales[ss_addr_sk#15,ss_store_sk#14,ss_net_profit#12] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_store_sk), EqualTo(ss_store_sk,4), IsNull(ss_addr_sk)], ReadSchema: struct<ss_addr_sk:int,ss_store_sk:int,ss_net_profit:decimal(7,2)>
      :     :                             +- *(5) HashAggregate(keys=[ss_item_sk#16], functions=[avg(UnscaledValue(ss_net_profit#12))])
      :     :                                +- ReusedExchange [ss_item_sk#16, sum#19, count#20], Exchange hashpartitioning(ss_item_sk#16, 200)
      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
      :        +- *(8) Project [i_item_sk#9, i_product_name#4]
      :           +- *(8) Filter isnotnull(i_item_sk#9)
      :              +- *(8) FileScan parquet default.item[i_item_sk#9,i_product_name#4] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_item_sk)], ReadSchema: struct<i_item_sk:int,i_product_name:string>
      +- ReusedExchange [i_item_sk#7, i_product_name#5], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))