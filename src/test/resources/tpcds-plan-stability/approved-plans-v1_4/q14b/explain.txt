== Physical Plan ==
TakeOrderedAndProject(limit=100, orderBy=[channel#1 ASC NULLS FIRST,i_brand_id#2 ASC NULLS FIRST,i_class_id#3 ASC NULLS FIRST,i_category_id#4 ASC NULLS FIRST], output=[channel#1,i_brand_id#2,i_class_id#3,i_category_id#4,sales#5,number_sales#6,channel#7,i_brand_id#8,i_class_id#9,i_category_id#10,sales#11,number_sales#12])
+- *(52) BroadcastHashJoin [i_brand_id#2, i_class_id#3, i_category_id#4], [i_brand_id#8, i_class_id#9, i_category_id#10], Inner, BuildRight
   :- *(52) Project [channel#1, i_brand_id#2, i_class_id#3, i_category_id#4, sales#5, number_sales#6]
   :  +- *(52) Filter (isnotnull(sum(CheckOverflow((promote_precision(cast(cast(ss_quantity#13 as decimal(10,0)) as decimal(12,2))) * promote_precision(cast(ss_list_price#14 as decimal(12,2)))), DecimalType(18,2)))#15) && (cast(sum(CheckOverflow((promote_precision(cast(cast(ss_quantity#13 as decimal(10,0)) as decimal(12,2))) * promote_precision(cast(ss_list_price#14 as decimal(12,2)))), DecimalType(18,2)))#15 as decimal(32,6)) > cast(Subquery subquery1884 as decimal(32,6))))
   :     :  +- Subquery subquery1884
   :     :     +- *(8) HashAggregate(keys=[], functions=[avg(CheckOverflow((promote_precision(cast(cast(quantity#16 as decimal(10,0)) as decimal(12,2))) * promote_precision(cast(list_price#17 as decimal(12,2)))), DecimalType(18,2)))])
   :     :        +- Exchange SinglePartition
   :     :           +- *(7) HashAggregate(keys=[], functions=[partial_avg(CheckOverflow((promote_precision(cast(cast(quantity#16 as decimal(10,0)) as decimal(12,2))) * promote_precision(cast(list_price#17 as decimal(12,2)))), DecimalType(18,2)))])
   :     :              +- Union
   :     :                 :- *(2) Project [ss_quantity#13 AS quantity#16, ss_list_price#14 AS list_price#17]
   :     :                 :  +- *(2) BroadcastHashJoin [ss_sold_date_sk#18], [d_date_sk#19], Inner, BuildRight
   :     :                 :     :- *(2) Project [ss_sold_date_sk#18, ss_quantity#13, ss_list_price#14]
   :     :                 :     :  +- *(2) Filter isnotnull(ss_sold_date_sk#18)
   :     :                 :     :     +- *(2) FileScan parquet default.store_sales[ss_sold_date_sk#18,ss_quantity#13,ss_list_price#14] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_sold_date_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_quantity:int,ss_list_price:decimal(7,2)>
   :     :                 :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
   :     :                 :        +- *(1) Project [d_date_sk#19]
   :     :                 :           +- *(1) Filter (((isnotnull(d_year#20) && (d_year#20 >= 1999)) && (d_year#20 <= 2001)) && isnotnull(d_date_sk#19))
   :     :                 :              +- *(1) FileScan parquet default.date_dim[d_date_sk#19,d_year#20] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), GreaterThanOrEqual(d_year,1999), LessThanOrEqual(d_year,2001), IsNotNull(d_da..., ReadSchema: struct<d_date_sk:int,d_year:int>
   :     :                 :- *(4) Project [cs_quantity#21 AS quantity#22, cs_list_price#23 AS list_price#24]
   :     :                 :  +- *(4) BroadcastHashJoin [cs_sold_date_sk#25], [d_date_sk#19], Inner, BuildRight
   :     :                 :     :- *(4) Project [cs_sold_date_sk#25, cs_quantity#21, cs_list_price#23]
   :     :                 :     :  +- *(4) Filter isnotnull(cs_sold_date_sk#25)
   :     :                 :     :     +- *(4) FileScan parquet default.catalog_sales[cs_sold_date_sk#25,cs_quantity#21,cs_list_price#23] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/catalog_sales], PartitionFilters: [], PushedFilters: [IsNotNull(cs_sold_date_sk)], ReadSchema: struct<cs_sold_date_sk:int,cs_quantity:int,cs_list_price:decimal(7,2)>
   :     :                 :     +- ReusedExchange [d_date_sk#19], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
   :     :                 +- *(6) Project [ws_quantity#26 AS quantity#27, ws_list_price#28 AS list_price#29]
   :     :                    +- *(6) BroadcastHashJoin [ws_sold_date_sk#30], [d_date_sk#19], Inner, BuildRight
   :     :                       :- *(6) Project [ws_sold_date_sk#30, ws_quantity#26, ws_list_price#28]
   :     :                       :  +- *(6) Filter isnotnull(ws_sold_date_sk#30)
   :     :                       :     +- *(6) FileScan parquet default.web_sales[ws_sold_date_sk#30,ws_quantity#26,ws_list_price#28] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/web_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ws_sold_date_sk)], ReadSchema: struct<ws_sold_date_sk:int,ws_quantity:int,ws_list_price:decimal(7,2)>
   :     :                       +- ReusedExchange [d_date_sk#19], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
   :     +- *(52) HashAggregate(keys=[i_brand_id#2, i_class_id#3, i_category_id#4], functions=[sum(CheckOverflow((promote_precision(cast(cast(ss_quantity#13 as decimal(10,0)) as decimal(12,2))) * promote_precision(cast(ss_list_price#14 as decimal(12,2)))), DecimalType(18,2))), count(1)])
   :        +- Exchange hashpartitioning(i_brand_id#2, i_class_id#3, i_category_id#4, 5)
   :           +- *(25) HashAggregate(keys=[i_brand_id#2, i_class_id#3, i_category_id#4], functions=[partial_sum(CheckOverflow((promote_precision(cast(cast(ss_quantity#13 as decimal(10,0)) as decimal(12,2))) * promote_precision(cast(ss_list_price#14 as decimal(12,2)))), DecimalType(18,2))), partial_count(1)])
   :              +- *(25) Project [ss_quantity#13, ss_list_price#14, i_brand_id#2, i_class_id#3, i_category_id#4]
   :                 +- *(25) BroadcastHashJoin [ss_sold_date_sk#18], [d_date_sk#19], Inner, BuildRight
   :                    :- *(25) Project [ss_sold_date_sk#18, ss_quantity#13, ss_list_price#14, i_brand_id#2, i_class_id#3, i_category_id#4]
   :                    :  +- *(25) BroadcastHashJoin [ss_item_sk#31], [i_item_sk#32], Inner, BuildRight
   :                    :     :- *(25) BroadcastHashJoin [ss_item_sk#31], [ss_item_sk#33], LeftSemi, BuildRight
   :                    :     :  :- *(25) Project [ss_sold_date_sk#18, ss_item_sk#31, ss_quantity#13, ss_list_price#14]
   :                    :     :  :  +- *(25) Filter (isnotnull(ss_item_sk#31) && isnotnull(ss_sold_date_sk#18))
   :                    :     :  :     +- *(25) FileScan parquet default.store_sales[ss_sold_date_sk#18,ss_item_sk#31,ss_quantity#13,ss_list_price#14] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_item_sk), IsNotNull(ss_sold_date_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_item_sk:int,ss_quantity:int,ss_list_price:decimal(7,2)>
   :                    :     :  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
   :                    :     :     +- *(11) Project [i_item_sk#32 AS ss_item_sk#33]
   :                    :     :        +- *(11) BroadcastHashJoin [i_brand_id#2, i_class_id#3, i_category_id#4], [brand_id#34, class_id#35, category_id#36], Inner, BuildRight
   :                    :     :           :- *(11) Project [i_item_sk#32, i_brand_id#2, i_class_id#3, i_category_id#4]
   :                    :     :           :  +- *(11) Filter ((isnotnull(i_class_id#3) && isnotnull(i_brand_id#2)) && isnotnull(i_category_id#4))
   :                    :     :           :     +- *(11) FileScan parquet default.item[i_item_sk#32,i_brand_id#2,i_class_id#3,i_category_id#4] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_class_id), IsNotNull(i_brand_id), IsNotNull(i_category_id)], ReadSchema: struct<i_item_sk:int,i_brand_id:int,i_class_id:int,i_category_id:int>
   :                    :     :           +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, int, true], input[1, int, true], input[2, int, true]))
   :                    :     :              +- *(10) HashAggregate(keys=[brand_id#34, class_id#35, category_id#36], functions=[])
   :                    :     :                 +- *(10) HashAggregate(keys=[brand_id#34, class_id#35, category_id#36], functions=[])
   :                    :     :                    +- *(10) BroadcastHashJoin [coalesce(brand_id#34, 0), coalesce(class_id#35, 0), coalesce(category_id#36, 0)], [coalesce(i_brand_id#2, 0), coalesce(i_class_id#3, 0), coalesce(i_category_id#4, 0)], LeftSemi, BuildRight, (((brand_id#34 <=> i_brand_id#2) && (class_id#35 <=> i_class_id#3)) && (category_id#36 <=> i_category_id#4))
   :                    :     :                       :- *(10) HashAggregate(keys=[brand_id#34, class_id#35, category_id#36], functions=[])
   :                    :     :                       :  +- Exchange hashpartitioning(brand_id#34, class_id#35, category_id#36, 5)
   :                    :     :                       :     +- *(6) HashAggregate(keys=[brand_id#34, class_id#35, category_id#36], functions=[])
   :                    :     :                       :        +- *(6) BroadcastHashJoin [coalesce(brand_id#34, 0), coalesce(class_id#35, 0), coalesce(category_id#36, 0)], [coalesce(i_brand_id#2, 0), coalesce(i_class_id#3, 0), coalesce(i_category_id#4, 0)], LeftSemi, BuildRight, (((brand_id#34 <=> i_brand_id#2) && (class_id#35 <=> i_class_id#3)) && (category_id#36 <=> i_category_id#4))
   :                    :     :                       :           :- *(6) Project [i_brand_id#2 AS brand_id#34, i_class_id#3 AS class_id#35, i_category_id#4 AS category_id#36]
   :                    :     :                       :           :  +- *(6) BroadcastHashJoin [ss_sold_date_sk#18], [d_date_sk#19], Inner, BuildRight
   :                    :     :                       :           :     :- *(6) Project [ss_sold_date_sk#18, i_brand_id#2, i_class_id#3, i_category_id#4]
   :                    :     :                       :           :     :  +- *(6) BroadcastHashJoin [ss_item_sk#31], [i_item_sk#32], Inner, BuildRight
   :                    :     :                       :           :     :     :- *(6) Project [ss_sold_date_sk#18, ss_item_sk#31]
   :                    :     :                       :           :     :     :  +- *(6) Filter (isnotnull(ss_item_sk#31) && isnotnull(ss_sold_date_sk#18))
   :                    :     :                       :           :     :     :     +- *(6) FileScan parquet default.store_sales[ss_sold_date_sk#18,ss_item_sk#31] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_item_sk), IsNotNull(ss_sold_date_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_item_sk:int>
   :                    :     :                       :           :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
   :                    :     :                       :           :     :        +- *(1) Project [i_item_sk#32, i_brand_id#2, i_class_id#3, i_category_id#4]
   :                    :     :                       :           :     :           +- *(1) Filter (((isnotnull(i_item_sk#32) && isnotnull(i_class_id#3)) && isnotnull(i_brand_id#2)) && isnotnull(i_category_id#4))
   :                    :     :                       :           :     :              +- *(1) FileScan parquet default.item[i_item_sk#32,i_brand_id#2,i_class_id#3,i_category_id#4] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_item_sk), IsNotNull(i_class_id), IsNotNull(i_brand_id), IsNotNull(i_category_id)], ReadSchema: struct<i_item_sk:int,i_brand_id:int,i_class_id:int,i_category_id:int>
   :                    :     :                       :           :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
   :                    :     :                       :           :        +- *(2) Project [d_date_sk#19]
   :                    :     :                       :           :           +- *(2) Filter (((isnotnull(d_year#20) && (d_year#20 >= 1999)) && (d_year#20 <= 2001)) && isnotnull(d_date_sk#19))
   :                    :     :                       :           :              +- *(2) FileScan parquet default.date_dim[d_date_sk#19,d_year#20] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), GreaterThanOrEqual(d_year,1999), LessThanOrEqual(d_year,2001), IsNotNull(d_da..., ReadSchema: struct<d_date_sk:int,d_year:int>
   :                    :     :                       :           +- BroadcastExchange HashedRelationBroadcastMode(List(coalesce(input[0, int, true], 0), coalesce(input[1, int, true], 0), coalesce(input[2, int, true], 0)))
   :                    :     :                       :              +- *(5) Project [i_brand_id#2, i_class_id#3, i_category_id#4]
   :                    :     :                       :                 +- *(5) BroadcastHashJoin [cs_sold_date_sk#25], [d_date_sk#19], Inner, BuildRight
   :                    :     :                       :                    :- *(5) Project [cs_sold_date_sk#25, i_brand_id#2, i_class_id#3, i_category_id#4]
   :                    :     :                       :                    :  +- *(5) BroadcastHashJoin [cs_item_sk#37], [i_item_sk#32], Inner, BuildRight
   :                    :     :                       :                    :     :- *(5) Project [cs_sold_date_sk#25, cs_item_sk#37]
   :                    :     :                       :                    :     :  +- *(5) Filter (isnotnull(cs_item_sk#37) && isnotnull(cs_sold_date_sk#25))
   :                    :     :                       :                    :     :     +- *(5) FileScan parquet default.catalog_sales[cs_sold_date_sk#25,cs_item_sk#37] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/catalog_sales], PartitionFilters: [], PushedFilters: [IsNotNull(cs_item_sk), IsNotNull(cs_sold_date_sk)], ReadSchema: struct<cs_sold_date_sk:int,cs_item_sk:int>
   :                    :     :                       :                    :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
   :                    :     :                       :                    :        +- *(3) Project [i_item_sk#32, i_brand_id#2, i_class_id#3, i_category_id#4]
   :                    :     :                       :                    :           +- *(3) Filter isnotnull(i_item_sk#32)
   :                    :     :                       :                    :              +- *(3) FileScan parquet default.item[i_item_sk#32,i_brand_id#2,i_class_id#3,i_category_id#4] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_item_sk)], ReadSchema: struct<i_item_sk:int,i_brand_id:int,i_class_id:int,i_category_id:int>
   :                    :     :                       :                    +- ReusedExchange [d_date_sk#19], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
   :                    :     :                       +- BroadcastExchange HashedRelationBroadcastMode(List(coalesce(input[0, int, true], 0), coalesce(input[1, int, true], 0), coalesce(input[2, int, true], 0)))
   :                    :     :                          +- *(9) Project [i_brand_id#2, i_class_id#3, i_category_id#4]
   :                    :     :                             +- *(9) BroadcastHashJoin [ws_sold_date_sk#30], [d_date_sk#19], Inner, BuildRight
   :                    :     :                                :- *(9) Project [ws_sold_date_sk#30, i_brand_id#2, i_class_id#3, i_category_id#4]
   :                    :     :                                :  +- *(9) BroadcastHashJoin [ws_item_sk#38], [i_item_sk#32], Inner, BuildRight
   :                    :     :                                :     :- *(9) Project [ws_sold_date_sk#30, ws_item_sk#38]
   :                    :     :                                :     :  +- *(9) Filter (isnotnull(ws_item_sk#38) && isnotnull(ws_sold_date_sk#30))
   :                    :     :                                :     :     +- *(9) FileScan parquet default.web_sales[ws_sold_date_sk#30,ws_item_sk#38] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/web_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ws_item_sk), IsNotNull(ws_sold_date_sk)], ReadSchema: struct<ws_sold_date_sk:int,ws_item_sk:int>
   :                    :     :                                :     +- ReusedExchange [i_item_sk#32, i_brand_id#2, i_class_id#3, i_category_id#4], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
   :                    :     :                                +- ReusedExchange [d_date_sk#19], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
   :                    :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
   :                    :        +- *(23) BroadcastHashJoin [i_item_sk#32], [ss_item_sk#33], LeftSemi, BuildRight
   :                    :           :- *(23) Project [i_item_sk#32, i_brand_id#2, i_class_id#3, i_category_id#4]
   :                    :           :  +- *(23) Filter (((isnotnull(i_item_sk#32) && isnotnull(i_class_id#3)) && isnotnull(i_category_id#4)) && isnotnull(i_brand_id#2))
   :                    :           :     +- *(23) FileScan parquet default.item[i_item_sk#32,i_brand_id#2,i_class_id#3,i_category_id#4] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_item_sk), IsNotNull(i_class_id), IsNotNull(i_category_id), IsNotNull(i_brand_id)], ReadSchema: struct<i_item_sk:int,i_brand_id:int,i_class_id:int,i_category_id:int>
   :                    :           +- ReusedExchange [ss_item_sk#33], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
   :                    +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
   :                       +- *(24) Project [d_date_sk#19]
   :                          +- *(24) Filter ((isnotnull(d_week_seq#39) && (d_week_seq#39 = Subquery subquery1883)) && isnotnull(d_date_sk#19))
   :                             :  +- Subquery subquery1883
   :                             :     +- *(1) Project [d_week_seq#39]
   :                             :        +- *(1) Filter (((((isnotnull(d_year#20) && isnotnull(d_dom#40)) && isnotnull(d_moy#41)) && (d_year#20 = 2000)) && (d_moy#41 = 12)) && (d_dom#40 = 11))
   :                             :           +- *(1) FileScan parquet default.date_dim[d_week_seq#39,d_year#20,d_moy#41,d_dom#40] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), IsNotNull(d_dom), IsNotNull(d_moy), EqualTo(d_year,2000), EqualTo(d_moy,12), ..., ReadSchema: struct<d_week_seq:int,d_year:int,d_moy:int,d_dom:int>
   :                             +- *(24) FileScan parquet default.date_dim[d_date_sk#19,d_week_seq#39] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_week_seq), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_week_seq:int>
   :                                   +- Subquery subquery1883
   :                                      +- *(1) Project [d_week_seq#39]
   :                                         +- *(1) Filter (((((isnotnull(d_year#20) && isnotnull(d_dom#40)) && isnotnull(d_moy#41)) && (d_year#20 = 2000)) && (d_moy#41 = 12)) && (d_dom#40 = 11))
   :                                            +- *(1) FileScan parquet default.date_dim[d_week_seq#39,d_year#20,d_moy#41,d_dom#40] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), IsNotNull(d_dom), IsNotNull(d_moy), EqualTo(d_year,2000), EqualTo(d_moy,12), ..., ReadSchema: struct<d_week_seq:int,d_year:int,d_moy:int,d_dom:int>
   +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, int, true], input[2, int, true], input[3, int, true]))
      +- *(51) Project [channel#7, i_brand_id#8, i_class_id#9, i_category_id#10, sales#11, number_sales#12]
         +- *(51) Filter (isnotnull(sum(CheckOverflow((promote_precision(cast(cast(ss_quantity#13 as decimal(10,0)) as decimal(12,2))) * promote_precision(cast(ss_list_price#14 as decimal(12,2)))), DecimalType(18,2)))#42) && (cast(sum(CheckOverflow((promote_precision(cast(cast(ss_quantity#13 as decimal(10,0)) as decimal(12,2))) * promote_precision(cast(ss_list_price#14 as decimal(12,2)))), DecimalType(18,2)))#42 as decimal(32,6)) > cast(Subquery subquery1890 as decimal(32,6))))
            :  +- Subquery subquery1890
            :     +- *(8) HashAggregate(keys=[], functions=[avg(CheckOverflow((promote_precision(cast(cast(quantity#16 as decimal(10,0)) as decimal(12,2))) * promote_precision(cast(list_price#17 as decimal(12,2)))), DecimalType(18,2)))])
            :        +- Exchange SinglePartition
            :           +- *(7) HashAggregate(keys=[], functions=[partial_avg(CheckOverflow((promote_precision(cast(cast(quantity#16 as decimal(10,0)) as decimal(12,2))) * promote_precision(cast(list_price#17 as decimal(12,2)))), DecimalType(18,2)))])
            :              +- Union
            :                 :- *(2) Project [ss_quantity#13 AS quantity#16, ss_list_price#14 AS list_price#17]
            :                 :  +- *(2) BroadcastHashJoin [ss_sold_date_sk#18], [d_date_sk#19], Inner, BuildRight
            :                 :     :- *(2) Project [ss_sold_date_sk#18, ss_quantity#13, ss_list_price#14]
            :                 :     :  +- *(2) Filter isnotnull(ss_sold_date_sk#18)
            :                 :     :     +- *(2) FileScan parquet default.store_sales[ss_sold_date_sk#18,ss_quantity#13,ss_list_price#14] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_sold_date_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_quantity:int,ss_list_price:decimal(7,2)>
            :                 :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
            :                 :        +- *(1) Project [d_date_sk#19]
            :                 :           +- *(1) Filter (((isnotnull(d_year#20) && (d_year#20 >= 1999)) && (d_year#20 <= 2001)) && isnotnull(d_date_sk#19))
            :                 :              +- *(1) FileScan parquet default.date_dim[d_date_sk#19,d_year#20] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), GreaterThanOrEqual(d_year,1999), LessThanOrEqual(d_year,2001), IsNotNull(d_da..., ReadSchema: struct<d_date_sk:int,d_year:int>
            :                 :- *(4) Project [cs_quantity#21 AS quantity#22, cs_list_price#23 AS list_price#24]
            :                 :  +- *(4) BroadcastHashJoin [cs_sold_date_sk#25], [d_date_sk#19], Inner, BuildRight
            :                 :     :- *(4) Project [cs_sold_date_sk#25, cs_quantity#21, cs_list_price#23]
            :                 :     :  +- *(4) Filter isnotnull(cs_sold_date_sk#25)
            :                 :     :     +- *(4) FileScan parquet default.catalog_sales[cs_sold_date_sk#25,cs_quantity#21,cs_list_price#23] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/catalog_sales], PartitionFilters: [], PushedFilters: [IsNotNull(cs_sold_date_sk)], ReadSchema: struct<cs_sold_date_sk:int,cs_quantity:int,cs_list_price:decimal(7,2)>
            :                 :     +- ReusedExchange [d_date_sk#19], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
            :                 +- *(6) Project [ws_quantity#26 AS quantity#27, ws_list_price#28 AS list_price#29]
            :                    +- *(6) BroadcastHashJoin [ws_sold_date_sk#30], [d_date_sk#19], Inner, BuildRight
            :                       :- *(6) Project [ws_sold_date_sk#30, ws_quantity#26, ws_list_price#28]
            :                       :  +- *(6) Filter isnotnull(ws_sold_date_sk#30)
            :                       :     +- *(6) FileScan parquet default.web_sales[ws_sold_date_sk#30,ws_quantity#26,ws_list_price#28] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/web_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ws_sold_date_sk)], ReadSchema: struct<ws_sold_date_sk:int,ws_quantity:int,ws_list_price:decimal(7,2)>
            :                       +- ReusedExchange [d_date_sk#19], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
            +- *(51) HashAggregate(keys=[i_brand_id#8, i_class_id#9, i_category_id#10], functions=[sum(CheckOverflow((promote_precision(cast(cast(ss_quantity#13 as decimal(10,0)) as decimal(12,2))) * promote_precision(cast(ss_list_price#14 as decimal(12,2)))), DecimalType(18,2))), count(1)])
               +- Exchange hashpartitioning(i_brand_id#8, i_class_id#9, i_category_id#10, 5)
                  +- *(50) HashAggregate(keys=[i_brand_id#8, i_class_id#9, i_category_id#10], functions=[partial_sum(CheckOverflow((promote_precision(cast(cast(ss_quantity#13 as decimal(10,0)) as decimal(12,2))) * promote_precision(cast(ss_list_price#14 as decimal(12,2)))), DecimalType(18,2))), partial_count(1)])
                     +- *(50) Project [ss_quantity#13, ss_list_price#14, i_brand_id#8, i_class_id#9, i_category_id#10]
                        +- *(50) BroadcastHashJoin [ss_sold_date_sk#18], [d_date_sk#19], Inner, BuildRight
                           :- *(50) Project [ss_sold_date_sk#18, ss_quantity#13, ss_list_price#14, i_brand_id#8, i_class_id#9, i_category_id#10]
                           :  +- *(50) BroadcastHashJoin [ss_item_sk#31], [i_item_sk#43], Inner, BuildRight
                           :     :- *(50) BroadcastHashJoin [ss_item_sk#31], [ss_item_sk#33], LeftSemi, BuildRight
                           :     :  :- *(50) Project [ss_sold_date_sk#18, ss_item_sk#31, ss_quantity#13, ss_list_price#14]
                           :     :  :  +- *(50) Filter (isnotnull(ss_item_sk#31) && isnotnull(ss_sold_date_sk#18))
                           :     :  :     +- *(50) FileScan parquet default.store_sales[ss_sold_date_sk#18,ss_item_sk#31,ss_quantity#13,ss_list_price#14] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_item_sk), IsNotNull(ss_sold_date_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_item_sk:int,ss_quantity:int,ss_list_price:decimal(7,2)>
                           :     :  +- ReusedExchange [ss_item_sk#33], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                           :     +- ReusedExchange [i_item_sk#43, i_brand_id#8, i_class_id#9, i_category_id#10], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                           +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                              +- *(49) Project [d_date_sk#19]
                                 +- *(49) Filter ((isnotnull(d_week_seq#39) && (d_week_seq#39 = Subquery subquery1889)) && isnotnull(d_date_sk#19))
                                    :  +- Subquery subquery1889
                                    :     +- *(1) Project [d_week_seq#39]
                                    :        +- *(1) Filter (((((isnotnull(d_year#20) && isnotnull(d_dom#40)) && isnotnull(d_moy#41)) && (d_year#20 = 1999)) && (d_moy#41 = 12)) && (d_dom#40 = 11))
                                    :           +- *(1) FileScan parquet default.date_dim[d_week_seq#39,d_year#20,d_moy#41,d_dom#40] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), IsNotNull(d_dom), IsNotNull(d_moy), EqualTo(d_year,1999), EqualTo(d_moy,12), ..., ReadSchema: struct<d_week_seq:int,d_year:int,d_moy:int,d_dom:int>
                                    +- *(49) FileScan parquet default.date_dim[d_date_sk#19,d_week_seq#39] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_week_seq), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_week_seq:int>
                                          +- Subquery subquery1889
                                             +- *(1) Project [d_week_seq#39]
                                                +- *(1) Filter (((((isnotnull(d_year#20) && isnotnull(d_dom#40)) && isnotnull(d_moy#41)) && (d_year#20 = 1999)) && (d_moy#41 = 12)) && (d_dom#40 = 11))
                                                   +- *(1) FileScan parquet default.date_dim[d_week_seq#39,d_year#20,d_moy#41,d_dom#40] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), IsNotNull(d_dom), IsNotNull(d_moy), EqualTo(d_year,1999), EqualTo(d_moy,12), ..., ReadSchema: struct<d_week_seq:int,d_year:int,d_moy:int,d_dom:int>