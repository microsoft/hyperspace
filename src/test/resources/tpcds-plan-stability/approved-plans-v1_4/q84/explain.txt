== Physical Plan ==
TakeOrderedAndProject(limit=100, orderBy=[c_customer_id#1 ASC NULLS FIRST], output=[customer_id#2,customername#3])
+- *(6) Project [c_customer_id#1 AS customer_id#2, concat(c_last_name#4, , , c_first_name#5) AS customername#3, c_customer_id#1]
   +- *(6) BroadcastHashJoin [cast(cd_demo_sk#6 as bigint)], [sr_cdemo_sk#7], Inner, BuildRight
      :- *(6) Project [c_customer_id#1, c_first_name#5, c_last_name#4, cd_demo_sk#6]
      :  +- *(6) BroadcastHashJoin [hd_income_band_sk#8], [ib_income_band_sk#9], Inner, BuildRight
      :     :- *(6) Project [c_customer_id#1, c_first_name#5, c_last_name#4, cd_demo_sk#6, hd_income_band_sk#8]
      :     :  +- *(6) BroadcastHashJoin [c_current_hdemo_sk#10], [hd_demo_sk#11], Inner, BuildRight
      :     :     :- *(6) Project [c_customer_id#1, c_current_hdemo_sk#10, c_first_name#5, c_last_name#4, cd_demo_sk#6]
      :     :     :  +- *(6) BroadcastHashJoin [c_current_cdemo_sk#12], [cd_demo_sk#6], Inner, BuildRight
      :     :     :     :- *(6) Project [c_customer_id#1, c_current_cdemo_sk#12, c_current_hdemo_sk#10, c_first_name#5, c_last_name#4]
      :     :     :     :  +- *(6) BroadcastHashJoin [c_current_addr_sk#13], [ca_address_sk#14], Inner, BuildRight
      :     :     :     :     :- *(6) Project [c_customer_id#1, c_current_cdemo_sk#12, c_current_hdemo_sk#10, c_current_addr_sk#13, c_first_name#5, c_last_name#4]
      :     :     :     :     :  +- *(6) Filter ((isnotnull(c_current_addr_sk#13) && isnotnull(c_current_cdemo_sk#12)) && isnotnull(c_current_hdemo_sk#10))
      :     :     :     :     :     +- *(6) FileScan parquet default.customer[c_customer_id#1,c_current_cdemo_sk#12,c_current_hdemo_sk#10,c_current_addr_sk#13,c_first_name#5,c_last_name#4] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/customer], PartitionFilters: [], PushedFilters: [IsNotNull(c_current_addr_sk), IsNotNull(c_current_cdemo_sk), IsNotNull(c_current_hdemo_sk)], ReadSchema: struct<c_customer_id:string,c_current_cdemo_sk:int,c_current_hdemo_sk:int,c_current_addr_sk:int,c...
      :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
      :     :     :     :        +- *(1) Project [ca_address_sk#14]
      :     :     :     :           +- *(1) Filter ((isnotnull(ca_city#15) && (ca_city#15 = Edgewood)) && isnotnull(ca_address_sk#14))
      :     :     :     :              +- *(1) FileScan parquet default.customer_address[ca_address_sk#14,ca_city#15] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/customer_address], PartitionFilters: [], PushedFilters: [IsNotNull(ca_city), EqualTo(ca_city,Edgewood), IsNotNull(ca_address_sk)], ReadSchema: struct<ca_address_sk:int,ca_city:string>
      :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
      :     :     :        +- *(2) Project [cd_demo_sk#6]
      :     :     :           +- *(2) Filter isnotnull(cd_demo_sk#6)
      :     :     :              +- *(2) FileScan parquet default.customer_demographics[cd_demo_sk#6] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/customer_demographics], PartitionFilters: [], PushedFilters: [IsNotNull(cd_demo_sk)], ReadSchema: struct<cd_demo_sk:int>
      :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
      :     :        +- *(3) Project [hd_demo_sk#11, hd_income_band_sk#8]
      :     :           +- *(3) Filter (isnotnull(hd_demo_sk#11) && isnotnull(hd_income_band_sk#8))
      :     :              +- *(3) FileScan parquet default.household_demographics[hd_demo_sk#11,hd_income_band_sk#8] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/household_demographics], PartitionFilters: [], PushedFilters: [IsNotNull(hd_demo_sk), IsNotNull(hd_income_band_sk)], ReadSchema: struct<hd_demo_sk:int,hd_income_band_sk:int>
      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
      :        +- *(4) Project [ib_income_band_sk#9]
      :           +- *(4) Filter ((((isnotnull(ib_lower_bound#16) && isnotnull(ib_upper_bound#17)) && (ib_lower_bound#16 >= 38128)) && (ib_upper_bound#17 <= 88128)) && isnotnull(ib_income_band_sk#9))
      :              +- *(4) FileScan parquet default.income_band[ib_income_band_sk#9,ib_lower_bound#16,ib_upper_bound#17] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/income_band], PartitionFilters: [], PushedFilters: [IsNotNull(ib_lower_bound), IsNotNull(ib_upper_bound), GreaterThanOrEqual(ib_lower_bound,38128), ..., ReadSchema: struct<ib_income_band_sk:int,ib_lower_bound:int,ib_upper_bound:int>
      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true]))
         +- *(5) Project [sr_cdemo_sk#7]
            +- *(5) Filter isnotnull(sr_cdemo_sk#7)
               +- *(5) FileScan parquet default.store_returns[sr_cdemo_sk#7] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_returns], PartitionFilters: [], PushedFilters: [IsNotNull(sr_cdemo_sk)], ReadSchema: struct<sr_cdemo_sk:bigint>