== Physical Plan ==
*(13) HashAggregate(keys=[], functions=[count(1)])
+- Exchange SinglePartition
   +- *(12) HashAggregate(keys=[], functions=[partial_count(1)])
      +- *(12) HashAggregate(keys=[c_last_name#1, c_first_name#2, d_date#3], functions=[])
         +- *(12) HashAggregate(keys=[c_last_name#1, c_first_name#2, d_date#3], functions=[])
            +- *(12) BroadcastHashJoin [coalesce(c_last_name#1, ), coalesce(c_first_name#2, ), coalesce(d_date#3, 0)], [coalesce(c_last_name#4, ), coalesce(c_first_name#5, ), coalesce(d_date#6, 0)], LeftAnti, BuildRight, (((c_last_name#1 <=> c_last_name#4) && (c_first_name#2 <=> c_first_name#5)) && (d_date#3 <=> d_date#6))
               :- *(12) HashAggregate(keys=[c_last_name#1, c_first_name#2, d_date#3], functions=[])
               :  +- *(12) HashAggregate(keys=[c_last_name#1, c_first_name#2, d_date#3], functions=[])
               :     +- *(12) BroadcastHashJoin [coalesce(c_last_name#1, ), coalesce(c_first_name#2, ), coalesce(d_date#3, 0)], [coalesce(c_last_name#7, ), coalesce(c_first_name#8, ), coalesce(d_date#9, 0)], LeftAnti, BuildRight, (((c_last_name#1 <=> c_last_name#7) && (c_first_name#2 <=> c_first_name#8)) && (d_date#3 <=> d_date#9))
               :        :- *(12) HashAggregate(keys=[c_last_name#1, c_first_name#2, d_date#3], functions=[])
               :        :  +- Exchange hashpartitioning(c_last_name#1, c_first_name#2, d_date#3, 200)
               :        :     +- *(3) HashAggregate(keys=[c_last_name#1, c_first_name#2, d_date#3], functions=[])
               :        :        +- *(3) Project [c_last_name#1, c_first_name#2, d_date#3]
               :        :           +- *(3) BroadcastHashJoin [ss_customer_sk#10], [c_customer_sk#11], Inner, BuildRight
               :        :              :- *(3) Project [ss_customer_sk#10, d_date#3]
               :        :              :  +- *(3) BroadcastHashJoin [ss_sold_date_sk#12], [d_date_sk#13], Inner, BuildRight
               :        :              :     :- *(3) Project [ss_sold_date_sk#12, ss_customer_sk#10]
               :        :              :     :  +- *(3) Filter (isnotnull(ss_sold_date_sk#12) && isnotnull(ss_customer_sk#10))
               :        :              :     :     +- *(3) FileScan parquet default.store_sales[ss_sold_date_sk#12,ss_customer_sk#10] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_sold_date_sk), IsNotNull(ss_customer_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_customer_sk:int>
               :        :              :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :        :              :        +- *(1) Project [d_date_sk#13, d_date#3]
               :        :              :           +- *(1) Filter (((isnotnull(d_month_seq#14) && (d_month_seq#14 >= 1200)) && (d_month_seq#14 <= 1211)) && isnotnull(d_date_sk#13))
               :        :              :              +- *(1) FileScan parquet default.date_dim[d_date_sk#13,d_date#3,d_month_seq#14] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_month_seq), GreaterThanOrEqual(d_month_seq,1200), LessThanOrEqual(d_month_seq,1211),..., ReadSchema: struct<d_date_sk:int,d_date:date,d_month_seq:int>
               :        :              +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :        :                 +- *(2) Project [c_customer_sk#11, c_first_name#2, c_last_name#1]
               :        :                    +- *(2) Filter isnotnull(c_customer_sk#11)
               :        :                       +- *(2) FileScan parquet default.customer[c_customer_sk#11,c_first_name#2,c_last_name#1] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/customer], PartitionFilters: [], PushedFilters: [IsNotNull(c_customer_sk)], ReadSchema: struct<c_customer_sk:int,c_first_name:string,c_last_name:string>
               :        +- BroadcastExchange HashedRelationBroadcastMode(List(coalesce(input[0, string, true], ), coalesce(input[1, string, true], ), coalesce(input[2, date, true], 0)))
               :           +- *(7) HashAggregate(keys=[c_last_name#7, c_first_name#8, d_date#9], functions=[])
               :              +- Exchange hashpartitioning(c_last_name#7, c_first_name#8, d_date#9, 200)
               :                 +- *(6) HashAggregate(keys=[c_last_name#7, c_first_name#8, d_date#9], functions=[])
               :                    +- *(6) Project [c_last_name#7, c_first_name#8, d_date#9]
               :                       +- *(6) BroadcastHashJoin [cs_bill_customer_sk#15], [c_customer_sk#16], Inner, BuildRight
               :                          :- *(6) Project [cs_bill_customer_sk#15, d_date#9]
               :                          :  +- *(6) BroadcastHashJoin [cs_sold_date_sk#17], [d_date_sk#18], Inner, BuildRight
               :                          :     :- *(6) Project [cs_sold_date_sk#17, cs_bill_customer_sk#15]
               :                          :     :  +- *(6) Filter (isnotnull(cs_sold_date_sk#17) && isnotnull(cs_bill_customer_sk#15))
               :                          :     :     +- *(6) FileScan parquet default.catalog_sales[cs_sold_date_sk#17,cs_bill_customer_sk#15] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/catalog_sales], PartitionFilters: [], PushedFilters: [IsNotNull(cs_sold_date_sk), IsNotNull(cs_bill_customer_sk)], ReadSchema: struct<cs_sold_date_sk:int,cs_bill_customer_sk:int>
               :                          :     +- ReusedExchange [d_date_sk#18, d_date#9], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :                          +- ReusedExchange [c_customer_sk#16, c_first_name#8, c_last_name#7], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               +- BroadcastExchange HashedRelationBroadcastMode(List(coalesce(input[0, string, true], ), coalesce(input[1, string, true], ), coalesce(input[2, date, true], 0)))
                  +- *(11) HashAggregate(keys=[c_last_name#4, c_first_name#5, d_date#6], functions=[])
                     +- Exchange hashpartitioning(c_last_name#4, c_first_name#5, d_date#6, 200)
                        +- *(10) HashAggregate(keys=[c_last_name#4, c_first_name#5, d_date#6], functions=[])
                           +- *(10) Project [c_last_name#4, c_first_name#5, d_date#6]
                              +- *(10) BroadcastHashJoin [ws_bill_customer_sk#19], [c_customer_sk#20], Inner, BuildRight
                                 :- *(10) Project [ws_bill_customer_sk#19, d_date#6]
                                 :  +- *(10) BroadcastHashJoin [ws_sold_date_sk#21], [d_date_sk#22], Inner, BuildRight
                                 :     :- *(10) Project [ws_sold_date_sk#21, ws_bill_customer_sk#19]
                                 :     :  +- *(10) Filter (isnotnull(ws_sold_date_sk#21) && isnotnull(ws_bill_customer_sk#19))
                                 :     :     +- *(10) FileScan parquet default.web_sales[ws_sold_date_sk#21,ws_bill_customer_sk#19] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/web_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ws_sold_date_sk), IsNotNull(ws_bill_customer_sk)], ReadSchema: struct<ws_sold_date_sk:int,ws_bill_customer_sk:int>
                                 :     +- ReusedExchange [d_date_sk#22, d_date#6], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                                 +- ReusedExchange [c_customer_sk#20, c_first_name#5, c_last_name#4], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))