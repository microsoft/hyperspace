== Physical Plan ==
TakeOrderedAndProject(limit=100, orderBy=[i_item_id#1 ASC NULLS FIRST], output=[i_item_id#1,i_item_desc#2,i_current_price#3])
+- *(5) HashAggregate(keys=[i_item_id#1, i_item_desc#2, i_current_price#3], functions=[])
   +- Exchange hashpartitioning(i_item_id#1, i_item_desc#2, i_current_price#3, 5)
      +- *(4) HashAggregate(keys=[i_item_id#1, i_item_desc#2, i_current_price#3], functions=[])
         +- *(4) Project [i_item_id#1, i_item_desc#2, i_current_price#3]
            +- *(4) BroadcastHashJoin [i_item_sk#4], [cs_item_sk#5], Inner, BuildRight
               :- *(4) Project [i_item_sk#4, i_item_id#1, i_item_desc#2, i_current_price#3]
               :  +- *(4) BroadcastHashJoin [inv_date_sk#6], [d_date_sk#7], Inner, BuildRight
               :     :- *(4) Project [i_item_sk#4, i_item_id#1, i_item_desc#2, i_current_price#3, inv_date_sk#6]
               :     :  +- *(4) BroadcastHashJoin [i_item_sk#4], [inv_item_sk#8], Inner, BuildRight
               :     :     :- *(4) Project [i_item_sk#4, i_item_id#1, i_item_desc#2, i_current_price#3]
               :     :     :  +- *(4) Filter ((((isnotnull(i_current_price#3) && (i_current_price#3 >= 68.00)) && (cast(i_current_price#3 as decimal(12,2)) <= 98.00)) && i_manufact_id#9 IN (677,940,694,808)) && isnotnull(i_item_sk#4))
               :     :     :     +- *(4) FileScan parquet default.item[i_item_sk#4,i_item_id#1,i_item_desc#2,i_current_price#3,i_manufact_id#9] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_current_price), GreaterThanOrEqual(i_current_price,68.00), In(i_manufact_id, [677,94..., ReadSchema: struct<i_item_sk:int,i_item_id:string,i_item_desc:string,i_current_price:decimal(7,2),i_manufact_...
               :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[1, int, true] as bigint)))
               :     :        +- *(1) Project [inv_date_sk#6, inv_item_sk#8]
               :     :           +- *(1) Filter ((((isnotnull(inv_quantity_on_hand#10) && (inv_quantity_on_hand#10 >= 100)) && (inv_quantity_on_hand#10 <= 500)) && isnotnull(inv_item_sk#8)) && isnotnull(inv_date_sk#6))
               :     :              +- *(1) FileScan parquet default.inventory[inv_date_sk#6,inv_item_sk#8,inv_quantity_on_hand#10] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/inventory], PartitionFilters: [], PushedFilters: [IsNotNull(inv_quantity_on_hand), GreaterThanOrEqual(inv_quantity_on_hand,100), LessThanOrEqual(i..., ReadSchema: struct<inv_date_sk:int,inv_item_sk:int,inv_quantity_on_hand:int>
               :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :        +- *(2) Project [d_date_sk#7]
               :           +- *(2) Filter (((isnotnull(d_date#11) && (d_date#11 >= 10988)) && (d_date#11 <= 11048)) && isnotnull(d_date_sk#7))
               :              +- *(2) FileScan parquet default.date_dim[d_date_sk#7,d_date#11] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_date), GreaterThanOrEqual(d_date,2000-02-01), LessThanOrEqual(d_date,2000-04-01), Is..., ReadSchema: struct<d_date_sk:int,d_date:date>
               +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                  +- *(3) Project [cs_item_sk#5]
                     +- *(3) Filter isnotnull(cs_item_sk#5)
                        +- *(3) FileScan parquet default.catalog_sales[cs_item_sk#5] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/catalog_sales], PartitionFilters: [], PushedFilters: [IsNotNull(cs_item_sk)], ReadSchema: struct<cs_item_sk:int>