== Physical Plan ==
BroadcastNestedLoopJoin BuildRight, Inner
:- BroadcastNestedLoopJoin BuildRight, Inner
:  :- BroadcastNestedLoopJoin BuildRight, Inner
:  :  :- BroadcastNestedLoopJoin BuildRight, Inner
:  :  :  :- BroadcastNestedLoopJoin BuildRight, Inner
:  :  :  :  :- BroadcastNestedLoopJoin BuildRight, Inner
:  :  :  :  :  :- BroadcastNestedLoopJoin BuildRight, Inner
:  :  :  :  :  :  :- *(5) HashAggregate(keys=[], functions=[count(1)])
:  :  :  :  :  :  :  +- Exchange SinglePartition
:  :  :  :  :  :  :     +- *(4) HashAggregate(keys=[], functions=[partial_count(1)])
:  :  :  :  :  :  :        +- *(4) Project
:  :  :  :  :  :  :           +- *(4) BroadcastHashJoin [ss_store_sk#1], [s_store_sk#2], Inner, BuildRight
:  :  :  :  :  :  :              :- *(4) Project [ss_store_sk#1]
:  :  :  :  :  :  :              :  +- *(4) BroadcastHashJoin [ss_sold_time_sk#3], [t_time_sk#4], Inner, BuildRight
:  :  :  :  :  :  :              :     :- *(4) Project [ss_sold_time_sk#3, ss_store_sk#1]
:  :  :  :  :  :  :              :     :  +- *(4) BroadcastHashJoin [ss_hdemo_sk#5], [hd_demo_sk#6], Inner, BuildRight
:  :  :  :  :  :  :              :     :     :- *(4) Project [ss_sold_time_sk#3, ss_hdemo_sk#5, ss_store_sk#1]
:  :  :  :  :  :  :              :     :     :  +- *(4) Filter ((isnotnull(ss_hdemo_sk#5) && isnotnull(ss_sold_time_sk#3)) && isnotnull(ss_store_sk#1))
:  :  :  :  :  :  :              :     :     :     +- *(4) FileScan parquet default.store_sales[ss_sold_time_sk#3,ss_hdemo_sk#5,ss_store_sk#1] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_hdemo_sk), IsNotNull(ss_sold_time_sk), IsNotNull(ss_store_sk)], ReadSchema: struct<ss_sold_time_sk:int,ss_hdemo_sk:int,ss_store_sk:int>
:  :  :  :  :  :  :              :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
:  :  :  :  :  :  :              :     :        +- *(1) Project [hd_demo_sk#6]
:  :  :  :  :  :  :              :     :           +- *(1) Filter (((((hd_dep_count#7 = 4) && (hd_vehicle_count#8 <= 6)) || ((hd_dep_count#7 = 2) && (hd_vehicle_count#8 <= 4))) || ((hd_dep_count#7 = 0) && (hd_vehicle_count#8 <= 2))) && isnotnull(hd_demo_sk#6))
:  :  :  :  :  :  :              :     :              +- *(1) FileScan parquet default.household_demographics[hd_demo_sk#6,hd_dep_count#7,hd_vehicle_count#8] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/household_demographics], PartitionFilters: [], PushedFilters: [Or(Or(And(EqualTo(hd_dep_count,4),LessThanOrEqual(hd_vehicle_count,6)),And(EqualTo(hd_dep_count,..., ReadSchema: struct<hd_demo_sk:int,hd_dep_count:int,hd_vehicle_count:int>
:  :  :  :  :  :  :              :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
:  :  :  :  :  :  :              :        +- *(2) Project [t_time_sk#4]
:  :  :  :  :  :  :              :           +- *(2) Filter ((((isnotnull(t_hour#9) && isnotnull(t_minute#10)) && (t_hour#9 = 8)) && (t_minute#10 >= 30)) && isnotnull(t_time_sk#4))
:  :  :  :  :  :  :              :              +- *(2) FileScan parquet default.time_dim[t_time_sk#4,t_hour#9,t_minute#10] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/time_dim], PartitionFilters: [], PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,8), GreaterThanOrEqual(t_minute,30), IsNo..., ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>
:  :  :  :  :  :  :              +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
:  :  :  :  :  :  :                 +- *(3) Project [s_store_sk#2]
:  :  :  :  :  :  :                    +- *(3) Filter ((isnotnull(s_store_name#11) && (s_store_name#11 = ese)) && isnotnull(s_store_sk#2))
:  :  :  :  :  :  :                       +- *(3) FileScan parquet default.store[s_store_sk#2,s_store_name#11] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store], PartitionFilters: [], PushedFilters: [IsNotNull(s_store_name), EqualTo(s_store_name,ese), IsNotNull(s_store_sk)], ReadSchema: struct<s_store_sk:int,s_store_name:string>
:  :  :  :  :  :  +- BroadcastExchange IdentityBroadcastMode
:  :  :  :  :  :     +- *(10) HashAggregate(keys=[], functions=[count(1)])
:  :  :  :  :  :        +- Exchange SinglePartition
:  :  :  :  :  :           +- *(9) HashAggregate(keys=[], functions=[partial_count(1)])
:  :  :  :  :  :              +- *(9) Project
:  :  :  :  :  :                 +- *(9) BroadcastHashJoin [ss_store_sk#1], [s_store_sk#2], Inner, BuildRight
:  :  :  :  :  :                    :- *(9) Project [ss_store_sk#1]
:  :  :  :  :  :                    :  +- *(9) BroadcastHashJoin [ss_sold_time_sk#3], [t_time_sk#4], Inner, BuildRight
:  :  :  :  :  :                    :     :- *(9) Project [ss_sold_time_sk#3, ss_store_sk#1]
:  :  :  :  :  :                    :     :  +- *(9) BroadcastHashJoin [ss_hdemo_sk#5], [hd_demo_sk#6], Inner, BuildRight
:  :  :  :  :  :                    :     :     :- *(9) Project [ss_sold_time_sk#3, ss_hdemo_sk#5, ss_store_sk#1]
:  :  :  :  :  :                    :     :     :  +- *(9) Filter ((isnotnull(ss_hdemo_sk#5) && isnotnull(ss_sold_time_sk#3)) && isnotnull(ss_store_sk#1))
:  :  :  :  :  :                    :     :     :     +- *(9) FileScan parquet default.store_sales[ss_sold_time_sk#3,ss_hdemo_sk#5,ss_store_sk#1] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_hdemo_sk), IsNotNull(ss_sold_time_sk), IsNotNull(ss_store_sk)], ReadSchema: struct<ss_sold_time_sk:int,ss_hdemo_sk:int,ss_store_sk:int>
:  :  :  :  :  :                    :     :     +- ReusedExchange [hd_demo_sk#6], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
:  :  :  :  :  :                    :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
:  :  :  :  :  :                    :        +- *(7) Project [t_time_sk#4]
:  :  :  :  :  :                    :           +- *(7) Filter ((((isnotnull(t_hour#9) && isnotnull(t_minute#10)) && (t_hour#9 = 9)) && (t_minute#10 < 30)) && isnotnull(t_time_sk#4))
:  :  :  :  :  :                    :              +- *(7) FileScan parquet default.time_dim[t_time_sk#4,t_hour#9,t_minute#10] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/time_dim], PartitionFilters: [], PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,9), LessThan(t_minute,30), IsNotNull(t_ti..., ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>
:  :  :  :  :  :                    +- ReusedExchange [s_store_sk#2], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
:  :  :  :  :  +- BroadcastExchange IdentityBroadcastMode
:  :  :  :  :     +- *(15) HashAggregate(keys=[], functions=[count(1)])
:  :  :  :  :        +- Exchange SinglePartition
:  :  :  :  :           +- *(14) HashAggregate(keys=[], functions=[partial_count(1)])
:  :  :  :  :              +- *(14) Project
:  :  :  :  :                 +- *(14) BroadcastHashJoin [ss_store_sk#1], [s_store_sk#2], Inner, BuildRight
:  :  :  :  :                    :- *(14) Project [ss_store_sk#1]
:  :  :  :  :                    :  +- *(14) BroadcastHashJoin [ss_sold_time_sk#3], [t_time_sk#4], Inner, BuildRight
:  :  :  :  :                    :     :- *(14) Project [ss_sold_time_sk#3, ss_store_sk#1]
:  :  :  :  :                    :     :  +- *(14) BroadcastHashJoin [ss_hdemo_sk#5], [hd_demo_sk#6], Inner, BuildRight
:  :  :  :  :                    :     :     :- *(14) Project [ss_sold_time_sk#3, ss_hdemo_sk#5, ss_store_sk#1]
:  :  :  :  :                    :     :     :  +- *(14) Filter ((isnotnull(ss_hdemo_sk#5) && isnotnull(ss_sold_time_sk#3)) && isnotnull(ss_store_sk#1))
:  :  :  :  :                    :     :     :     +- *(14) FileScan parquet default.store_sales[ss_sold_time_sk#3,ss_hdemo_sk#5,ss_store_sk#1] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_hdemo_sk), IsNotNull(ss_sold_time_sk), IsNotNull(ss_store_sk)], ReadSchema: struct<ss_sold_time_sk:int,ss_hdemo_sk:int,ss_store_sk:int>
:  :  :  :  :                    :     :     +- ReusedExchange [hd_demo_sk#6], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
:  :  :  :  :                    :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
:  :  :  :  :                    :        +- *(12) Project [t_time_sk#4]
:  :  :  :  :                    :           +- *(12) Filter ((((isnotnull(t_hour#9) && isnotnull(t_minute#10)) && (t_hour#9 = 9)) && (t_minute#10 >= 30)) && isnotnull(t_time_sk#4))
:  :  :  :  :                    :              +- *(12) FileScan parquet default.time_dim[t_time_sk#4,t_hour#9,t_minute#10] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/time_dim], PartitionFilters: [], PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,9), GreaterThanOrEqual(t_minute,30), IsNo..., ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>
:  :  :  :  :                    +- ReusedExchange [s_store_sk#2], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
:  :  :  :  +- BroadcastExchange IdentityBroadcastMode
:  :  :  :     +- *(20) HashAggregate(keys=[], functions=[count(1)])
:  :  :  :        +- Exchange SinglePartition
:  :  :  :           +- *(19) HashAggregate(keys=[], functions=[partial_count(1)])
:  :  :  :              +- *(19) Project
:  :  :  :                 +- *(19) BroadcastHashJoin [ss_store_sk#1], [s_store_sk#2], Inner, BuildRight
:  :  :  :                    :- *(19) Project [ss_store_sk#1]
:  :  :  :                    :  +- *(19) BroadcastHashJoin [ss_sold_time_sk#3], [t_time_sk#4], Inner, BuildRight
:  :  :  :                    :     :- *(19) Project [ss_sold_time_sk#3, ss_store_sk#1]
:  :  :  :                    :     :  +- *(19) BroadcastHashJoin [ss_hdemo_sk#5], [hd_demo_sk#6], Inner, BuildRight
:  :  :  :                    :     :     :- *(19) Project [ss_sold_time_sk#3, ss_hdemo_sk#5, ss_store_sk#1]
:  :  :  :                    :     :     :  +- *(19) Filter ((isnotnull(ss_hdemo_sk#5) && isnotnull(ss_sold_time_sk#3)) && isnotnull(ss_store_sk#1))
:  :  :  :                    :     :     :     +- *(19) FileScan parquet default.store_sales[ss_sold_time_sk#3,ss_hdemo_sk#5,ss_store_sk#1] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_hdemo_sk), IsNotNull(ss_sold_time_sk), IsNotNull(ss_store_sk)], ReadSchema: struct<ss_sold_time_sk:int,ss_hdemo_sk:int,ss_store_sk:int>
:  :  :  :                    :     :     +- ReusedExchange [hd_demo_sk#6], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
:  :  :  :                    :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
:  :  :  :                    :        +- *(17) Project [t_time_sk#4]
:  :  :  :                    :           +- *(17) Filter ((((isnotnull(t_hour#9) && isnotnull(t_minute#10)) && (t_hour#9 = 10)) && (t_minute#10 < 30)) && isnotnull(t_time_sk#4))
:  :  :  :                    :              +- *(17) FileScan parquet default.time_dim[t_time_sk#4,t_hour#9,t_minute#10] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/time_dim], PartitionFilters: [], PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,10), LessThan(t_minute,30), IsNotNull(t_t..., ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>
:  :  :  :                    +- ReusedExchange [s_store_sk#2], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
:  :  :  +- BroadcastExchange IdentityBroadcastMode
:  :  :     +- *(25) HashAggregate(keys=[], functions=[count(1)])
:  :  :        +- Exchange SinglePartition
:  :  :           +- *(24) HashAggregate(keys=[], functions=[partial_count(1)])
:  :  :              +- *(24) Project
:  :  :                 +- *(24) BroadcastHashJoin [ss_store_sk#1], [s_store_sk#2], Inner, BuildRight
:  :  :                    :- *(24) Project [ss_store_sk#1]
:  :  :                    :  +- *(24) BroadcastHashJoin [ss_sold_time_sk#3], [t_time_sk#4], Inner, BuildRight
:  :  :                    :     :- *(24) Project [ss_sold_time_sk#3, ss_store_sk#1]
:  :  :                    :     :  +- *(24) BroadcastHashJoin [ss_hdemo_sk#5], [hd_demo_sk#6], Inner, BuildRight
:  :  :                    :     :     :- *(24) Project [ss_sold_time_sk#3, ss_hdemo_sk#5, ss_store_sk#1]
:  :  :                    :     :     :  +- *(24) Filter ((isnotnull(ss_hdemo_sk#5) && isnotnull(ss_sold_time_sk#3)) && isnotnull(ss_store_sk#1))
:  :  :                    :     :     :     +- *(24) FileScan parquet default.store_sales[ss_sold_time_sk#3,ss_hdemo_sk#5,ss_store_sk#1] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_hdemo_sk), IsNotNull(ss_sold_time_sk), IsNotNull(ss_store_sk)], ReadSchema: struct<ss_sold_time_sk:int,ss_hdemo_sk:int,ss_store_sk:int>
:  :  :                    :     :     +- ReusedExchange [hd_demo_sk#6], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
:  :  :                    :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
:  :  :                    :        +- *(22) Project [t_time_sk#4]
:  :  :                    :           +- *(22) Filter ((((isnotnull(t_hour#9) && isnotnull(t_minute#10)) && (t_hour#9 = 10)) && (t_minute#10 >= 30)) && isnotnull(t_time_sk#4))
:  :  :                    :              +- *(22) FileScan parquet default.time_dim[t_time_sk#4,t_hour#9,t_minute#10] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/time_dim], PartitionFilters: [], PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,10), GreaterThanOrEqual(t_minute,30), IsN..., ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>
:  :  :                    +- ReusedExchange [s_store_sk#2], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
:  :  +- BroadcastExchange IdentityBroadcastMode
:  :     +- *(30) HashAggregate(keys=[], functions=[count(1)])
:  :        +- Exchange SinglePartition
:  :           +- *(29) HashAggregate(keys=[], functions=[partial_count(1)])
:  :              +- *(29) Project
:  :                 +- *(29) BroadcastHashJoin [ss_store_sk#1], [s_store_sk#2], Inner, BuildRight
:  :                    :- *(29) Project [ss_store_sk#1]
:  :                    :  +- *(29) BroadcastHashJoin [ss_sold_time_sk#3], [t_time_sk#4], Inner, BuildRight
:  :                    :     :- *(29) Project [ss_sold_time_sk#3, ss_store_sk#1]
:  :                    :     :  +- *(29) BroadcastHashJoin [ss_hdemo_sk#5], [hd_demo_sk#6], Inner, BuildRight
:  :                    :     :     :- *(29) Project [ss_sold_time_sk#3, ss_hdemo_sk#5, ss_store_sk#1]
:  :                    :     :     :  +- *(29) Filter ((isnotnull(ss_hdemo_sk#5) && isnotnull(ss_sold_time_sk#3)) && isnotnull(ss_store_sk#1))
:  :                    :     :     :     +- *(29) FileScan parquet default.store_sales[ss_sold_time_sk#3,ss_hdemo_sk#5,ss_store_sk#1] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_hdemo_sk), IsNotNull(ss_sold_time_sk), IsNotNull(ss_store_sk)], ReadSchema: struct<ss_sold_time_sk:int,ss_hdemo_sk:int,ss_store_sk:int>
:  :                    :     :     +- ReusedExchange [hd_demo_sk#6], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
:  :                    :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
:  :                    :        +- *(27) Project [t_time_sk#4]
:  :                    :           +- *(27) Filter ((((isnotnull(t_hour#9) && isnotnull(t_minute#10)) && (t_hour#9 = 11)) && (t_minute#10 < 30)) && isnotnull(t_time_sk#4))
:  :                    :              +- *(27) FileScan parquet default.time_dim[t_time_sk#4,t_hour#9,t_minute#10] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/time_dim], PartitionFilters: [], PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,11), LessThan(t_minute,30), IsNotNull(t_t..., ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>
:  :                    +- ReusedExchange [s_store_sk#2], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
:  +- BroadcastExchange IdentityBroadcastMode
:     +- *(35) HashAggregate(keys=[], functions=[count(1)])
:        +- Exchange SinglePartition
:           +- *(34) HashAggregate(keys=[], functions=[partial_count(1)])
:              +- *(34) Project
:                 +- *(34) BroadcastHashJoin [ss_store_sk#1], [s_store_sk#2], Inner, BuildRight
:                    :- *(34) Project [ss_store_sk#1]
:                    :  +- *(34) BroadcastHashJoin [ss_sold_time_sk#3], [t_time_sk#4], Inner, BuildRight
:                    :     :- *(34) Project [ss_sold_time_sk#3, ss_store_sk#1]
:                    :     :  +- *(34) BroadcastHashJoin [ss_hdemo_sk#5], [hd_demo_sk#6], Inner, BuildRight
:                    :     :     :- *(34) Project [ss_sold_time_sk#3, ss_hdemo_sk#5, ss_store_sk#1]
:                    :     :     :  +- *(34) Filter ((isnotnull(ss_hdemo_sk#5) && isnotnull(ss_sold_time_sk#3)) && isnotnull(ss_store_sk#1))
:                    :     :     :     +- *(34) FileScan parquet default.store_sales[ss_sold_time_sk#3,ss_hdemo_sk#5,ss_store_sk#1] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_hdemo_sk), IsNotNull(ss_sold_time_sk), IsNotNull(ss_store_sk)], ReadSchema: struct<ss_sold_time_sk:int,ss_hdemo_sk:int,ss_store_sk:int>
:                    :     :     +- ReusedExchange [hd_demo_sk#6], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
:                    :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
:                    :        +- *(32) Project [t_time_sk#4]
:                    :           +- *(32) Filter ((((isnotnull(t_hour#9) && isnotnull(t_minute#10)) && (t_hour#9 = 11)) && (t_minute#10 >= 30)) && isnotnull(t_time_sk#4))
:                    :              +- *(32) FileScan parquet default.time_dim[t_time_sk#4,t_hour#9,t_minute#10] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/time_dim], PartitionFilters: [], PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,11), GreaterThanOrEqual(t_minute,30), IsN..., ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>
:                    +- ReusedExchange [s_store_sk#2], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
+- BroadcastExchange IdentityBroadcastMode
   +- *(40) HashAggregate(keys=[], functions=[count(1)])
      +- Exchange SinglePartition
         +- *(39) HashAggregate(keys=[], functions=[partial_count(1)])
            +- *(39) Project
               +- *(39) BroadcastHashJoin [ss_store_sk#1], [s_store_sk#2], Inner, BuildRight
                  :- *(39) Project [ss_store_sk#1]
                  :  +- *(39) BroadcastHashJoin [ss_sold_time_sk#3], [t_time_sk#4], Inner, BuildRight
                  :     :- *(39) Project [ss_sold_time_sk#3, ss_store_sk#1]
                  :     :  +- *(39) BroadcastHashJoin [ss_hdemo_sk#5], [hd_demo_sk#6], Inner, BuildRight
                  :     :     :- *(39) Project [ss_sold_time_sk#3, ss_hdemo_sk#5, ss_store_sk#1]
                  :     :     :  +- *(39) Filter ((isnotnull(ss_hdemo_sk#5) && isnotnull(ss_sold_time_sk#3)) && isnotnull(ss_store_sk#1))
                  :     :     :     +- *(39) FileScan parquet default.store_sales[ss_sold_time_sk#3,ss_hdemo_sk#5,ss_store_sk#1] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_hdemo_sk), IsNotNull(ss_sold_time_sk), IsNotNull(ss_store_sk)], ReadSchema: struct<ss_sold_time_sk:int,ss_hdemo_sk:int,ss_store_sk:int>
                  :     :     +- ReusedExchange [hd_demo_sk#6], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                  :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                  :        +- *(37) Project [t_time_sk#4]
                  :           +- *(37) Filter ((((isnotnull(t_hour#9) && isnotnull(t_minute#10)) && (t_hour#9 = 12)) && (t_minute#10 < 30)) && isnotnull(t_time_sk#4))
                  :              +- *(37) FileScan parquet default.time_dim[t_time_sk#4,t_hour#9,t_minute#10] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/time_dim], PartitionFilters: [], PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,12), LessThan(t_minute,30), IsNotNull(t_t..., ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>
                  +- ReusedExchange [s_store_sk#2], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))