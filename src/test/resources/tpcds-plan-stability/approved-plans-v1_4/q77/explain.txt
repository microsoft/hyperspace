== Physical Plan ==
TakeOrderedAndProject(limit=100, orderBy=[channel#1 ASC NULLS FIRST,id#2 ASC NULLS FIRST], output=[channel#1,id#2,sales#3,returns#4,profit#5])
+- *(25) HashAggregate(keys=[channel#1, id#2, spark_grouping_id#6], functions=[sum(sales#7), sum(returns#8), sum(profit#9)])
   +- Exchange hashpartitioning(channel#1, id#2, spark_grouping_id#6, 200)
      +- *(24) HashAggregate(keys=[channel#1, id#2, spark_grouping_id#6], functions=[partial_sum(sales#7), partial_sum(returns#8), partial_sum(profit#9)])
         +- *(24) Expand [List(sales#7, returns#8, profit#9, channel#10, id#11, 0), List(sales#7, returns#8, profit#9, channel#10, null, 1), List(sales#7, returns#8, profit#9, null, null, 3)], [sales#7, returns#8, profit#9, channel#1, id#2, spark_grouping_id#6]
            +- Union
               :- *(8) Project [sales#7, coalesce(returns#12, 0.00) AS returns#8, CheckOverflow((promote_precision(cast(profit#13 as decimal(18,2))) - promote_precision(cast(coalesce(profit_loss#14, 0.00) as decimal(18,2)))), DecimalType(18,2)) AS profit#9, store channel AS channel#10, s_store_sk#15 AS id#11]
               :  +- *(8) BroadcastHashJoin [s_store_sk#15], [s_store_sk#16], LeftOuter, BuildRight
               :     :- *(8) HashAggregate(keys=[s_store_sk#15], functions=[sum(UnscaledValue(ss_ext_sales_price#17)), sum(UnscaledValue(ss_net_profit#18))])
               :     :  +- Exchange hashpartitioning(s_store_sk#15, 200)
               :     :     +- *(3) HashAggregate(keys=[s_store_sk#15], functions=[partial_sum(UnscaledValue(ss_ext_sales_price#17)), partial_sum(UnscaledValue(ss_net_profit#18))])
               :     :        +- *(3) Project [ss_ext_sales_price#17, ss_net_profit#18, s_store_sk#15]
               :     :           +- *(3) BroadcastHashJoin [ss_store_sk#19], [s_store_sk#15], Inner, BuildRight
               :     :              :- *(3) Project [ss_store_sk#19, ss_ext_sales_price#17, ss_net_profit#18]
               :     :              :  +- *(3) BroadcastHashJoin [ss_sold_date_sk#20], [d_date_sk#21], Inner, BuildRight
               :     :              :     :- *(3) Project [ss_sold_date_sk#20, ss_store_sk#19, ss_ext_sales_price#17, ss_net_profit#18]
               :     :              :     :  +- *(3) Filter (isnotnull(ss_sold_date_sk#20) && isnotnull(ss_store_sk#19))
               :     :              :     :     +- *(3) FileScan parquet default.store_sales[ss_sold_date_sk#20,ss_store_sk#19,ss_ext_sales_price#17,ss_net_profit#18] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_sold_date_sk), IsNotNull(ss_store_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_store_sk:int,ss_ext_sales_price:decimal(7,2),ss_net_profit:decimal(...
               :     :              :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :     :              :        +- *(1) Project [d_date_sk#21]
               :     :              :           +- *(1) Filter (((isnotnull(d_date#22) && (d_date#22 >= 11172)) && (d_date#22 <= 11202)) && isnotnull(d_date_sk#21))
               :     :              :              +- *(1) FileScan parquet default.date_dim[d_date_sk#21,d_date#22] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_date), GreaterThanOrEqual(d_date,2000-08-03), LessThanOrEqual(d_date,2000-09-02), Is..., ReadSchema: struct<d_date_sk:int,d_date:date>
               :     :              +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :     :                 +- *(2) Project [s_store_sk#15]
               :     :                    +- *(2) Filter isnotnull(s_store_sk#15)
               :     :                       +- *(2) FileScan parquet default.store[s_store_sk#15] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store], PartitionFilters: [], PushedFilters: [IsNotNull(s_store_sk)], ReadSchema: struct<s_store_sk:int>
               :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :        +- *(7) HashAggregate(keys=[s_store_sk#16], functions=[sum(UnscaledValue(sr_return_amt#23)), sum(UnscaledValue(sr_net_loss#24))])
               :           +- Exchange hashpartitioning(s_store_sk#16, 200)
               :              +- *(6) HashAggregate(keys=[s_store_sk#16], functions=[partial_sum(UnscaledValue(sr_return_amt#23)), partial_sum(UnscaledValue(sr_net_loss#24))])
               :                 +- *(6) Project [sr_return_amt#23, sr_net_loss#24, s_store_sk#16]
               :                    +- *(6) BroadcastHashJoin [sr_store_sk#25], [cast(s_store_sk#16 as bigint)], Inner, BuildRight
               :                       :- *(6) Project [sr_store_sk#25, sr_return_amt#23, sr_net_loss#24]
               :                       :  +- *(6) BroadcastHashJoin [sr_returned_date_sk#26], [cast(d_date_sk#21 as bigint)], Inner, BuildRight
               :                       :     :- *(6) Project [sr_returned_date_sk#26, sr_store_sk#25, sr_return_amt#23, sr_net_loss#24]
               :                       :     :  +- *(6) Filter (isnotnull(sr_returned_date_sk#26) && isnotnull(sr_store_sk#25))
               :                       :     :     +- *(6) FileScan parquet default.store_returns[sr_returned_date_sk#26,sr_store_sk#25,sr_return_amt#23,sr_net_loss#24] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store_returns], PartitionFilters: [], PushedFilters: [IsNotNull(sr_returned_date_sk), IsNotNull(sr_store_sk)], ReadSchema: struct<sr_returned_date_sk:bigint,sr_store_sk:bigint,sr_return_amt:decimal(7,2),sr_net_loss:decim...
               :                       :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :                       :        +- *(4) Project [d_date_sk#21]
               :                       :           +- *(4) Filter (((isnotnull(d_date#22) && (d_date#22 >= 11172)) && (d_date#22 <= 11202)) && isnotnull(d_date_sk#21))
               :                       :              +- *(4) FileScan parquet default.date_dim[d_date_sk#21,d_date#22] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_date), GreaterThanOrEqual(d_date,2000-08-03), LessThanOrEqual(d_date,2000-09-02), Is..., ReadSchema: struct<d_date_sk:int,d_date:date>
               :                       +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :                          +- *(5) Project [s_store_sk#16]
               :                             +- *(5) Filter isnotnull(s_store_sk#16)
               :                                +- *(5) FileScan parquet default.store[s_store_sk#16] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/store], PartitionFilters: [], PushedFilters: [IsNotNull(s_store_sk)], ReadSchema: struct<s_store_sk:int>
               :- *(15) Project [sales#27, returns#28, CheckOverflow((promote_precision(cast(profit#29 as decimal(18,2))) - promote_precision(cast(profit_loss#30 as decimal(18,2)))), DecimalType(18,2)) AS profit#31, catalog channel AS channel#32, cs_call_center_sk#33 AS id#34]
               :  +- BroadcastNestedLoopJoin BuildLeft, Inner
               :     :- BroadcastExchange IdentityBroadcastMode
               :     :  +- *(11) HashAggregate(keys=[cs_call_center_sk#33], functions=[sum(UnscaledValue(cs_ext_sales_price#35)), sum(UnscaledValue(cs_net_profit#36))])
               :     :     +- Exchange hashpartitioning(cs_call_center_sk#33, 200)
               :     :        +- *(10) HashAggregate(keys=[cs_call_center_sk#33], functions=[partial_sum(UnscaledValue(cs_ext_sales_price#35)), partial_sum(UnscaledValue(cs_net_profit#36))])
               :     :           +- *(10) Project [cs_call_center_sk#33, cs_ext_sales_price#35, cs_net_profit#36]
               :     :              +- *(10) BroadcastHashJoin [cs_sold_date_sk#37], [d_date_sk#21], Inner, BuildRight
               :     :                 :- *(10) Project [cs_sold_date_sk#37, cs_call_center_sk#33, cs_ext_sales_price#35, cs_net_profit#36]
               :     :                 :  +- *(10) Filter isnotnull(cs_sold_date_sk#37)
               :     :                 :     +- *(10) FileScan parquet default.catalog_sales[cs_sold_date_sk#37,cs_call_center_sk#33,cs_ext_sales_price#35,cs_net_profit#36] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/catalog_sales], PartitionFilters: [], PushedFilters: [IsNotNull(cs_sold_date_sk)], ReadSchema: struct<cs_sold_date_sk:int,cs_call_center_sk:int,cs_ext_sales_price:decimal(7,2),cs_net_profit:de...
               :     :                 +- ReusedExchange [d_date_sk#21], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               :     +- *(14) HashAggregate(keys=[], functions=[sum(UnscaledValue(cr_return_amount#38)), sum(UnscaledValue(cr_net_loss#39))])
               :        +- Exchange SinglePartition
               :           +- *(13) HashAggregate(keys=[], functions=[partial_sum(UnscaledValue(cr_return_amount#38)), partial_sum(UnscaledValue(cr_net_loss#39))])
               :              +- *(13) Project [cr_return_amount#38, cr_net_loss#39]
               :                 +- *(13) BroadcastHashJoin [cr_returned_date_sk#40], [d_date_sk#21], Inner, BuildRight
               :                    :- *(13) Project [cr_returned_date_sk#40, cr_return_amount#38, cr_net_loss#39]
               :                    :  +- *(13) Filter isnotnull(cr_returned_date_sk#40)
               :                    :     +- *(13) FileScan parquet default.catalog_returns[cr_returned_date_sk#40,cr_return_amount#38,cr_net_loss#39] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/catalog_returns], PartitionFilters: [], PushedFilters: [IsNotNull(cr_returned_date_sk)], ReadSchema: struct<cr_returned_date_sk:int,cr_return_amount:decimal(7,2),cr_net_loss:decimal(7,2)>
               :                    +- ReusedExchange [d_date_sk#21], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
               +- *(23) Project [sales#41, coalesce(returns#42, 0.00) AS returns#43, CheckOverflow((promote_precision(cast(profit#44 as decimal(18,2))) - promote_precision(cast(coalesce(profit_loss#45, 0.00) as decimal(18,2)))), DecimalType(18,2)) AS profit#46, web channel AS channel#47, wp_web_page_sk#48 AS id#49]
                  +- *(23) BroadcastHashJoin [wp_web_page_sk#48], [wp_web_page_sk#50], LeftOuter, BuildRight
                     :- *(23) HashAggregate(keys=[wp_web_page_sk#48], functions=[sum(UnscaledValue(ws_ext_sales_price#51)), sum(UnscaledValue(ws_net_profit#52))])
                     :  +- Exchange hashpartitioning(wp_web_page_sk#48, 200)
                     :     +- *(18) HashAggregate(keys=[wp_web_page_sk#48], functions=[partial_sum(UnscaledValue(ws_ext_sales_price#51)), partial_sum(UnscaledValue(ws_net_profit#52))])
                     :        +- *(18) Project [ws_ext_sales_price#51, ws_net_profit#52, wp_web_page_sk#48]
                     :           +- *(18) BroadcastHashJoin [ws_web_page_sk#53], [wp_web_page_sk#48], Inner, BuildRight
                     :              :- *(18) Project [ws_web_page_sk#53, ws_ext_sales_price#51, ws_net_profit#52]
                     :              :  +- *(18) BroadcastHashJoin [ws_sold_date_sk#54], [d_date_sk#21], Inner, BuildRight
                     :              :     :- *(18) Project [ws_sold_date_sk#54, ws_web_page_sk#53, ws_ext_sales_price#51, ws_net_profit#52]
                     :              :     :  +- *(18) Filter (isnotnull(ws_sold_date_sk#54) && isnotnull(ws_web_page_sk#53))
                     :              :     :     +- *(18) FileScan parquet default.web_sales[ws_sold_date_sk#54,ws_web_page_sk#53,ws_ext_sales_price#51,ws_net_profit#52] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/web_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ws_sold_date_sk), IsNotNull(ws_web_page_sk)], ReadSchema: struct<ws_sold_date_sk:int,ws_web_page_sk:int,ws_ext_sales_price:decimal(7,2),ws_net_profit:decim...
                     :              :     +- ReusedExchange [d_date_sk#21], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                     :              +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                     :                 +- *(17) Project [wp_web_page_sk#48]
                     :                    +- *(17) Filter isnotnull(wp_web_page_sk#48)
                     :                       +- *(17) FileScan parquet default.web_page[wp_web_page_sk#48] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/web_page], PartitionFilters: [], PushedFilters: [IsNotNull(wp_web_page_sk)], ReadSchema: struct<wp_web_page_sk:int>
                     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                        +- *(22) HashAggregate(keys=[wp_web_page_sk#50], functions=[sum(UnscaledValue(wr_return_amt#55)), sum(UnscaledValue(wr_net_loss#56))])
                           +- Exchange hashpartitioning(wp_web_page_sk#50, 200)
                              +- *(21) HashAggregate(keys=[wp_web_page_sk#50], functions=[partial_sum(UnscaledValue(wr_return_amt#55)), partial_sum(UnscaledValue(wr_net_loss#56))])
                                 +- *(21) Project [wr_return_amt#55, wr_net_loss#56, wp_web_page_sk#50]
                                    +- *(21) BroadcastHashJoin [wr_web_page_sk#57], [cast(wp_web_page_sk#50 as bigint)], Inner, BuildRight
                                       :- *(21) Project [wr_web_page_sk#57, wr_return_amt#55, wr_net_loss#56]
                                       :  +- *(21) BroadcastHashJoin [wr_returned_date_sk#58], [cast(d_date_sk#21 as bigint)], Inner, BuildRight
                                       :     :- *(21) Project [wr_returned_date_sk#58, wr_web_page_sk#57, wr_return_amt#55, wr_net_loss#56]
                                       :     :  +- *(21) Filter (isnotnull(wr_returned_date_sk#58) && isnotnull(wr_web_page_sk#57))
                                       :     :     +- *(21) FileScan parquet default.web_returns[wr_returned_date_sk#58,wr_web_page_sk#57,wr_return_amt#55,wr_net_loss#56] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/web_returns], PartitionFilters: [], PushedFilters: [IsNotNull(wr_returned_date_sk), IsNotNull(wr_web_page_sk)], ReadSchema: struct<wr_returned_date_sk:bigint,wr_web_page_sk:bigint,wr_return_amt:decimal(7,2),wr_net_loss:de...
                                       :     +- ReusedExchange [d_date_sk#21], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                                       +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
                                          +- *(20) Project [wp_web_page_sk#50]
                                             +- *(20) Filter isnotnull(wp_web_page_sk#50)
                                                +- *(20) FileScan parquet default.web_page[wp_web_page_sk#50] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/apdave/github/hyperspace-1/spark-warehouse/web_page], PartitionFilters: [], PushedFilters: [IsNotNull(wp_web_page_sk)], ReadSchema: struct<wp_web_page_sk:int>